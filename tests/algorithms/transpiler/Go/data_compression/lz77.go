//go:build ignore

// Generated by Mochi v0.10.59 on 2025-08-06 21:47:50 GMT+7
package main

import (
	"encoding/json"
	"fmt"
	"os"
	"runtime"
	"strconv"
	"time"
)

var seededNow bool
var nowSeed int64

func init() {
	if s := os.Getenv("MOCHI_NOW_SEED"); s != "" {
		if v, err := strconv.ParseInt(s, 10, 64); err == nil {
			nowSeed = v
			seededNow = true
		}
	}
}
func _now() int {
	if seededNow {
		nowSeed = (nowSeed*1664525 + 1013904223) % 2147483647
		return int(nowSeed)
	}
	return int(time.Now().UnixNano())
}

func _substr(s string, start, end int) string {
	r := []rune(s)
	if start < 0 {
		start = 0
	}
	if end > len(r) {
		end = len(r)
	}
	if start > len(r) {
		start = len(r)
	}
	if end < start {
		end = start
	}
	return string(r[start:end])
}

type Token struct {
	Offset    int    `json:"offset"`
	Length    int    `json:"length"`
	Indicator string `json:"indicator"`
}

func token_to_string(t Token) string {
	return (((((("(" + fmt.Sprint(t.Offset)) + ", ") + fmt.Sprint(t.Length)) + ", ") + t.Indicator) + ")")
}

func tokens_to_string(ts []Token) string {
	var res string = "["
	_ = res
	var i int = 0
	_ = i
	for i < len(ts) {
		res = (res + token_to_string(ts[i]))
		if i < (len(ts) - 1) {
			res = (res + ", ")
		}
		i = (i + 1)
	}
	return (res + "]")
}

func match_length_from_index(text string, window string, text_index int, window_index int) int {
	if (text_index >= len(text)) || (window_index >= len(window)) {
		return 0
	}
	var tc string = _substr(text, text_index, (text_index + 1))
	_ = tc
	var wc string = _substr(window, window_index, (window_index + 1))
	_ = wc
	if tc != wc {
		return 0
	}
	return (1 + match_length_from_index(text, (window+tc), (text_index+1), (window_index+1)))
}

func find_encoding_token(text string, search_buffer string) Token {
	if len(text) == 0 {
		panic("We need some text to work with.")
	}
	var length int = 0
	_ = length
	var offset int = 0
	_ = offset
	if len(search_buffer) == 0 {
		return Token{
			Offset:    offset,
			Length:    length,
			Indicator: _substr(text, 0, 1),
		}
	}
	var i int = 0
	_ = i
	for i < len(search_buffer) {
		var ch string = _substr(search_buffer, i, (i + 1))
		_ = ch
		var found_offset int = (len(search_buffer) - i)
		_ = found_offset
		if ch == _substr(text, 0, 1) {
			var found_length int = match_length_from_index(text, search_buffer, 0, i)
			_ = found_length
			if found_length >= length {
				offset = found_offset
				length = found_length
			}
		}
		i = (i + 1)
	}
	return Token{
		Offset:    offset,
		Length:    length,
		Indicator: _substr(text, length, (length + 1)),
	}
}

func lz77_compress(text string, window_size int, lookahead int) []Token {
	var search_buffer_size int = (window_size - lookahead)
	_ = search_buffer_size
	var output []Token = []Token{}
	_ = output
	var search_buffer string = ""
	_ = search_buffer
	var remaining string = text
	_ = remaining
	for len(remaining) > 0 {
		var token Token = find_encoding_token(remaining, search_buffer)
		_ = token
		var add_len int = (token.Length + 1)
		_ = add_len
		search_buffer = (search_buffer + _substr(remaining, 0, add_len))
		if len(search_buffer) > search_buffer_size {
			search_buffer = _substr(search_buffer, (len(search_buffer) - search_buffer_size), len(search_buffer))
		}
		remaining = _substr(remaining, add_len, len(remaining))
		output = append(output, token)
	}
	return output
}

func lz77_decompress(tokens []Token) string {
	var output string = ""
	_ = output
	for _, t := range tokens {
		var i int = 0
		_ = i
		for i < t.Length {
			output = (output + _substr(output, (len(output)-t.Offset), ((len(output)-t.Offset)+1)))
			i = (i + 1)
		}
		output = (output + t.Indicator)
	}
	return output
}

var c1 []Token

var c2 []Token

var tokens_example []Token

func main() {
	func() {
		var ms runtime.MemStats
		runtime.ReadMemStats(&ms)
		startMem := ms.Alloc
		benchStart := time.Now().UnixNano()
		c1 = lz77_compress("ababcbababaa", 13, 6)
		fmt.Println(tokens_to_string(func(v any) []Token {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]Token); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []Token{}
				}
				out := make([]Token, len(arr))
				for i, x := range arr {
					out[i] = x.(Token)
				}
				return out
			}
			return v.([]Token)
		}(c1)))
		c2 = lz77_compress("aacaacabcabaaac", 13, 6)
		fmt.Println(tokens_to_string(func(v any) []Token {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]Token); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []Token{}
				}
				out := make([]Token, len(arr))
				for i, x := range arr {
					out[i] = x.(Token)
				}
				return out
			}
			return v.([]Token)
		}(c2)))
		tokens_example = []Token{Token{
			Offset:    0,
			Length:    0,
			Indicator: "c",
		}, Token{
			Offset:    0,
			Length:    0,
			Indicator: "a",
		}, Token{
			Offset:    0,
			Length:    0,
			Indicator: "b",
		}, Token{
			Offset:    0,
			Length:    0,
			Indicator: "r",
		}, Token{
			Offset:    3,
			Length:    1,
			Indicator: "c",
		}, Token{
			Offset:    2,
			Length:    1,
			Indicator: "d",
		}, Token{
			Offset:    7,
			Length:    4,
			Indicator: "r",
		}, Token{
			Offset:    3,
			Length:    5,
			Indicator: "d",
		}}
		fmt.Println(lz77_decompress(func(v any) []Token {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]Token); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []Token{}
				}
				out := make([]Token, len(arr))
				for i, x := range arr {
					out[i] = x.(Token)
				}
				return out
			}
			return v.([]Token)
		}(tokens_example)))
		runtime.ReadMemStats(&ms)
		endMem := ms.Alloc
		benchEnd := time.Now().UnixNano()
		data := map[string]any{"name": "main", "duration_us": (benchEnd - benchStart) / 1000, "memory_bytes": endMem - startMem}
		out, _ := json.MarshalIndent(data, "", "  ")
		fmt.Println(string(out))
	}()
}
