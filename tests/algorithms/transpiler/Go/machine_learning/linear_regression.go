//go:build ignore

// Generated by Mochi v0.10.67 on 2025-08-16 19:48:23 GMT+7
package main

import (
	"encoding/json"
	"fmt"
	"os"
	"runtime"
	"strconv"
	"time"
)

var seededNow bool
var nowSeed int64

func init() {
	if s := os.Getenv("MOCHI_NOW_SEED"); s != "" {
		if v, err := strconv.ParseInt(s, 10, 64); err == nil {
			nowSeed = v
			seededNow = true
		}
	}
}
func _now() int {
	if seededNow {
		nowSeed = (nowSeed*1664525 + 1013904223) % 2147483647
		return int(nowSeed)
	}
	return int(time.Now().UnixNano())
}

func _index[T any](s []T, i any) T {
	idx := func(v any) int {
		switch vv := v.(type) {
		case int:
			return vv
		case int64:
			return int(vv)
		case float64:
			return int(vv)
		case float32:
			return int(vv)
		default:
			return v.(int)
		}
	}(i)
	if idx < 0 {
		idx += len(s)
	}
	if idx < 0 || idx >= len(s) {
		var zero T
		return zero
	}
	return s[idx]
}

func _setIndex[T any](s []T, i any, v T) {
	idx := func(v any) int {
		switch vv := v.(type) {
		case int:
			return vv
		case int64:
			return int(vv)
		case float64:
			return int(vv)
		case float32:
			return int(vv)
		default:
			return v.(int)
		}
	}(i)
	if idx < 0 {
		idx += len(s)
	}
	if idx < 0 || idx >= len(s) {
		return
	}
	s[idx] = v
}

func dot(x []float64, y []float64) float64 {
	var sum float64 = 0.0
	_ = sum
	var i int = 0
	_ = i
	for i < len(x) {
		sum = (sum + (_index(x, i) * _index(y, i)))
		i = (i + 1)
	}
	return sum
}

func run_steep_gradient_descent(data_x [][]float64, data_y []float64, len_data int, alpha float64, theta []float64) []float64 {
	var gradients []float64 = []float64{}
	_ = gradients
	var j int = 0
	_ = j
	for j < len(theta) {
		gradients = append(gradients, 0.0)
		j = (j + 1)
	}
	var i int = 0
	_ = i
	for i < len_data {
		var prediction float64 = dot(func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(theta), func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(_index(data_x, i)))
		_ = prediction
		var error float64 = (prediction - _index(data_y, i))
		_ = error
		var k int = 0
		_ = k
		for k < len(theta) {
			_setIndex(gradients, k, (_index(gradients, k) + (error * _index(_index(data_x, i), k))))
			k = (k + 1)
		}
		i = (i + 1)
	}
	var t []float64 = []float64{}
	_ = t
	var g int = 0
	_ = g
	for g < len(theta) {
		t = append(t, (_index(theta, g) - ((float64(alpha) / float64(len_data)) * _index(gradients, g))))
		g = (g + 1)
	}
	return t
}

func sum_of_square_error(data_x [][]float64, data_y []float64, len_data int, theta []float64) float64 {
	var total float64 = 0.0
	_ = total
	var i int = 0
	_ = i
	for i < len_data {
		var prediction float64 = dot(func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(theta), func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(_index(data_x, i)))
		_ = prediction
		var diff float64 = (prediction - _index(data_y, i))
		_ = diff
		total = (total + (diff * diff))
		i = (i + 1)
	}
	return (float64(total) / float64((2.0 * float64(len_data))))
}

func run_linear_regression(data_x [][]float64, data_y []float64) []float64 {
	var iterations int = 10
	_ = iterations
	var alpha float64 = 0.01
	_ = alpha
	var no_features int = len(_index(data_x, 0))
	_ = no_features
	var len_data int = len(data_x)
	_ = len_data
	var theta []float64 = []float64{}
	_ = theta
	var i int = 0
	_ = i
	for i < no_features {
		theta = append(theta, 0.0)
		i = (i + 1)
	}
	var iter int = 0
	_ = iter
	for iter < iterations {
		theta = run_steep_gradient_descent(func(v any) [][]float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([][]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return [][]float64{}
				}
				out := make([][]float64, len(arr))
				for i, x := range arr {
					out[i] = func(v any) []float64 {
						if v == nil {
							return nil
						}
						if vv, ok := v.([]float64); ok {
							return vv
						}
						if arr, ok := v.([]any); ok {
							if len(arr) == 0 {
								return []float64{}
							}
							out := make([]float64, len(arr))
							for i, x := range arr {
								out[i] = x.(float64)
							}
							return out
						}
						return v.([]float64)
					}(x)
				}
				return out
			}
			return v.([][]float64)
		}(data_x), func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(data_y), len_data, alpha, func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(theta))
		var error float64 = sum_of_square_error(func(v any) [][]float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([][]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return [][]float64{}
				}
				out := make([][]float64, len(arr))
				for i, x := range arr {
					out[i] = func(v any) []float64 {
						if v == nil {
							return nil
						}
						if vv, ok := v.([]float64); ok {
							return vv
						}
						if arr, ok := v.([]any); ok {
							if len(arr) == 0 {
								return []float64{}
							}
							out := make([]float64, len(arr))
							for i, x := range arr {
								out[i] = x.(float64)
							}
							return out
						}
						return v.([]float64)
					}(x)
				}
				return out
			}
			return v.([][]float64)
		}(data_x), func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(data_y), len_data, func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(theta))
		_ = error
		fmt.Println(((("At Iteration " + fmt.Sprint((iter + 1))) + " - Error is ") + fmt.Sprint(error)))
		iter = (iter + 1)
	}
	return theta
}

func absf(x float64) float64 {
	if x < 0.0 {
		return (0 - x)
	} else {
		return x
	}
}

func mean_absolute_error(predicted_y []float64, original_y []float64) float64 {
	var total float64 = 0.0
	_ = total
	var i int = 0
	_ = i
	for i < len(predicted_y) {
		var diff float64 = absf((_index(predicted_y, i) - _index(original_y, i)))
		_ = diff
		total = (total + diff)
		i = (i + 1)
	}
	return (float64(total) / float64(len(predicted_y)))
}

var data_x [][]float64

var data_y []float64

var theta []float64

var i int

var predicted_y []float64

var original_y []float64

var mae float64

func main() {
	func() {
		var ms runtime.MemStats
		runtime.ReadMemStats(&ms)
		startMem := ms.Alloc
		benchStart := time.Now().UnixNano()
		data_x = [][]float64{[]float64{1.0, 1.0}, []float64{1.0, 2.0}, []float64{1.0, 3.0}}
		data_y = []float64{1.0, 2.0, 3.0}
		theta = run_linear_regression(func(v any) [][]float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([][]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return [][]float64{}
				}
				out := make([][]float64, len(arr))
				for i, x := range arr {
					out[i] = func(v any) []float64 {
						if v == nil {
							return nil
						}
						if vv, ok := v.([]float64); ok {
							return vv
						}
						if arr, ok := v.([]any); ok {
							if len(arr) == 0 {
								return []float64{}
							}
							out := make([]float64, len(arr))
							for i, x := range arr {
								out[i] = x.(float64)
							}
							return out
						}
						return v.([]float64)
					}(x)
				}
				return out
			}
			return v.([][]float64)
		}(data_x), func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(data_y))
		fmt.Println("Resultant Feature vector :")
		i = 0
		for i < len(theta) {
			fmt.Println(fmt.Sprint(_index(theta, i)))
			i = (i + 1)
		}
		predicted_y = []float64{3.0, (0 - 0.5), 2.0, 7.0}
		original_y = []float64{2.5, 0.0, 2.0, 8.0}
		mae = mean_absolute_error(func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(predicted_y), func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(original_y))
		fmt.Println(("Mean Absolute Error : " + fmt.Sprint(mae)))
		runtime.ReadMemStats(&ms)
		endMem := ms.Alloc
		benchEnd := time.Now().UnixNano()
		data := map[string]any{"name": "main", "duration_us": (benchEnd - benchStart) / 1000, "memory_bytes": endMem - startMem}
		out, _ := json.MarshalIndent(data, "", "  ")
		fmt.Println(string(out))
	}()
}
