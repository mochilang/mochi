//go:build ignore

// Generated by Mochi v0.10.66 on 2025-08-15 10:31:12 GMT+7
package main

import (
	"encoding/json"
	"fmt"
	"os"
	"runtime"
	"strconv"
	"time"
)

var seededNow bool
var nowSeed int64

func init() {
	if s := os.Getenv("MOCHI_NOW_SEED"); s != "" {
		if v, err := strconv.ParseInt(s, 10, 64); err == nil {
			nowSeed = v
			seededNow = true
		}
	}
}
func _now() int {
	if seededNow {
		nowSeed = (nowSeed*1664525 + 1013904223) % 2147483647
		return int(nowSeed)
	}
	return int(time.Now().UnixNano())
}

func _index[T any](s []T, i any) T {
	idx := func(v any) int {
		switch vv := v.(type) {
		case int:
			return vv
		case int64:
			return int(vv)
		case float64:
			return int(vv)
		case float32:
			return int(vv)
		default:
			return v.(int)
		}
	}(i)
	if idx < 0 {
		idx += len(s)
	}
	if idx < 0 || idx >= len(s) {
		var zero T
		return zero
	}
	return s[idx]
}

type DataPoint struct {
	X []float64 `json:"x"`
	Y float64   `json:"y"`
}

func absf(x float64) float64 {
	if x < 0.0 {
		return (0 - x)
	}
	return x
}

func hypothesis_value(input []float64, params []float64) float64 {
	var value float64 = _index(params, 0)
	_ = value
	var i int = 0
	_ = i
	for i < len(input) {
		value = (value + (_index(input, i) * _index(params, (i+1))))
		i = (i + 1)
	}
	return value
}

func calc_error(dp DataPoint, params []float64) float64 {
	return (hypothesis_value(func(v any) []float64 {
		if v == nil {
			return nil
		}
		if vv, ok := v.([]float64); ok {
			return vv
		}
		if arr, ok := v.([]any); ok {
			if len(arr) == 0 {
				return []float64{}
			}
			out := make([]float64, len(arr))
			for i, x := range arr {
				out[i] = x.(float64)
			}
			return out
		}
		return v.([]float64)
	}(dp.X), func(v any) []float64 {
		if v == nil {
			return nil
		}
		if vv, ok := v.([]float64); ok {
			return vv
		}
		if arr, ok := v.([]any); ok {
			if len(arr) == 0 {
				return []float64{}
			}
			out := make([]float64, len(arr))
			for i, x := range arr {
				out[i] = x.(float64)
			}
			return out
		}
		return v.([]float64)
	}(params)) - dp.Y)
}

func summation_of_cost_derivative(index int, params []float64, data []DataPoint) float64 {
	var sum float64 = 0.0
	_ = sum
	var i int = 0
	_ = i
	for i < len(data) {
		var dp DataPoint = _index(data, i)
		_ = dp
		var e float64 = calc_error(dp, func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(params))
		_ = e
		if index == (0 - 1) {
			sum = (sum + e)
		} else {
			sum = (sum + (e * _index(dp.X, index)))
		}
		i = (i + 1)
	}
	return sum
}

func get_cost_derivative(index int, params []float64, data []DataPoint) float64 {
	return (float64(summation_of_cost_derivative(index, func(v any) []float64 {
		if v == nil {
			return nil
		}
		if vv, ok := v.([]float64); ok {
			return vv
		}
		if arr, ok := v.([]any); ok {
			if len(arr) == 0 {
				return []float64{}
			}
			out := make([]float64, len(arr))
			for i, x := range arr {
				out[i] = x.(float64)
			}
			return out
		}
		return v.([]float64)
	}(params), func(v any) []DataPoint {
		if v == nil {
			return nil
		}
		if vv, ok := v.([]DataPoint); ok {
			return vv
		}
		if arr, ok := v.([]any); ok {
			if len(arr) == 0 {
				return []DataPoint{}
			}
			out := make([]DataPoint, len(arr))
			for i, x := range arr {
				out[i] = x.(DataPoint)
			}
			return out
		}
		return v.([]DataPoint)
	}(data))) / float64(float64(len(data))))
}

func allclose(a []float64, b []float64, atol float64, rtol float64) bool {
	var i int = 0
	_ = i
	for i < len(a) {
		var diff float64 = absf((_index(a, i) - _index(b, i)))
		_ = diff
		var limit float64 = (atol + (rtol * absf(_index(b, i))))
		_ = limit
		if diff > limit {
			return false
		}
		i = (i + 1)
	}
	return true
}

func run_gradient_descent(train_data []DataPoint, initial_params []float64) []float64 {
	var learning_rate float64 = 0.009
	_ = learning_rate
	var absolute_error_limit float64 = 2e-06
	_ = absolute_error_limit
	var relative_error_limit float64 = 0.0
	_ = relative_error_limit
	var j int = 0
	_ = j
	var params []float64 = initial_params
	_ = params
	for {
		j = (j + 1)
		var temp []float64 = []float64{}
		_ = temp
		var i int = 0
		_ = i
		for i < len(params) {
			var deriv float64 = get_cost_derivative((i - 1), func(v any) []float64 {
				if v == nil {
					return nil
				}
				if vv, ok := v.([]float64); ok {
					return vv
				}
				if arr, ok := v.([]any); ok {
					if len(arr) == 0 {
						return []float64{}
					}
					out := make([]float64, len(arr))
					for i, x := range arr {
						out[i] = x.(float64)
					}
					return out
				}
				return v.([]float64)
			}(params), func(v any) []DataPoint {
				if v == nil {
					return nil
				}
				if vv, ok := v.([]DataPoint); ok {
					return vv
				}
				if arr, ok := v.([]any); ok {
					if len(arr) == 0 {
						return []DataPoint{}
					}
					out := make([]DataPoint, len(arr))
					for i, x := range arr {
						out[i] = x.(DataPoint)
					}
					return out
				}
				return v.([]DataPoint)
			}(train_data))
			_ = deriv
			temp = append(temp, (_index(params, i) - (learning_rate * deriv)))
			i = (i + 1)
		}
		if allclose(func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(params), func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(temp), absolute_error_limit, relative_error_limit) {
			fmt.Println(("Number of iterations:" + fmt.Sprint(j)))
			break
		}
		params = temp
	}
	return params
}

func test_gradient_descent(test_data []DataPoint, params []float64) {
	var i int = 0
	_ = i
	for i < len(test_data) {
		var dp DataPoint = _index(test_data, i)
		_ = dp
		fmt.Println(("Actual output value:" + fmt.Sprint(dp.Y)))
		fmt.Println(("Hypothesis output:" + fmt.Sprint(hypothesis_value(func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(dp.X), func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(params)))))
		i = (i + 1)
	}
}

var train_data []DataPoint

var test_data []DataPoint

var parameter_vector []float64

func main() {
	func() {
		var ms runtime.MemStats
		runtime.ReadMemStats(&ms)
		startMem := ms.Alloc
		benchStart := time.Now().UnixNano()
		train_data = []DataPoint{DataPoint{
			X: []float64{5.0, 2.0, 3.0},
			Y: 15.0,
		}, DataPoint{
			X: []float64{6.0, 5.0, 9.0},
			Y: 25.0,
		}, DataPoint{
			X: []float64{11.0, 12.0, 13.0},
			Y: 41.0,
		}, DataPoint{
			X: []float64{1.0, 1.0, 1.0},
			Y: 8.0,
		}, DataPoint{
			X: []float64{11.0, 12.0, 13.0},
			Y: 41.0,
		}}
		test_data = []DataPoint{DataPoint{
			X: []float64{515.0, 22.0, 13.0},
			Y: 555.0,
		}, DataPoint{
			X: []float64{61.0, 35.0, 49.0},
			Y: 150.0,
		}}
		parameter_vector = []float64{2.0, 4.0, 1.0, 5.0}
		parameter_vector = run_gradient_descent(func(v any) []DataPoint {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]DataPoint); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []DataPoint{}
				}
				out := make([]DataPoint, len(arr))
				for i, x := range arr {
					out[i] = x.(DataPoint)
				}
				return out
			}
			return v.([]DataPoint)
		}(train_data), func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(parameter_vector))
		fmt.Println("\nTesting gradient descent for a linear hypothesis function.\n")
		test_gradient_descent(func(v any) []DataPoint {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]DataPoint); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []DataPoint{}
				}
				out := make([]DataPoint, len(arr))
				for i, x := range arr {
					out[i] = x.(DataPoint)
				}
				return out
			}
			return v.([]DataPoint)
		}(test_data), func(v any) []float64 {
			if v == nil {
				return nil
			}
			if vv, ok := v.([]float64); ok {
				return vv
			}
			if arr, ok := v.([]any); ok {
				if len(arr) == 0 {
					return []float64{}
				}
				out := make([]float64, len(arr))
				for i, x := range arr {
					out[i] = x.(float64)
				}
				return out
			}
			return v.([]float64)
		}(parameter_vector))
		runtime.ReadMemStats(&ms)
		endMem := ms.Alloc
		benchEnd := time.Now().UnixNano()
		data := map[string]any{"name": "main", "duration_us": (benchEnd - benchStart) / 1000, "memory_bytes": endMem - startMem}
		out, _ := json.MarshalIndent(data, "", "  ")
		fmt.Println(string(out))
	}()
}
