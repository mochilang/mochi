// Generated by Mochi Zig transpiler on 2025-08-16 21:23 +0700
const std = @import("std");

fn handleError(err: anyerror) noreturn {
    std.debug.panic("{any}", .{err});
}

fn absf(x: f64) f64 {
    if (x < 0.0) {
        return @as(f64, @floatFromInt(0)) - x;
    }
    return x;
}

fn maxf(a: f64, b: f64) f64 {
    if (a > b) {
        return a;
    }
    return b;
}

fn minf(a_1: f64, b_1: f64) f64 {
    if (a_1 < b_1) {
        return a_1;
    }
    return b_1;
}

fn clip(x_1: f64, lo: f64, hi: f64) f64 {
    return maxf(lo, minf(x_1, hi));
}

fn to_float(x_2: i64) f64 {
    return @as(f64, @floatFromInt(x_2)) * 1.0;
}

fn powf(base: f64, exp_param: f64) f64 {
    var result: f64 = 1.0;
    result = result;
    var i: i64 = 0;
    i = i;
    const n: i64 = @as(i64, @intFromFloat(exp_param));
    while (i < n) {
        result = result * base;
        i = i +% 1;
    }
    return result;
}

fn ln(x_3: f64) f64 {
    if (x_3 <= 0.0) {
        @panic("ln domain error");
    }
    const y: f64 = (x_3 - 1.0) / (x_3 + 1.0);
    const y2: f64 = y * y;
    var term: f64 = y;
    term = term;
    var sum: f64 = 0.0;
    sum = sum;
    var k: i64 = 0;
    k = k;
    while (k < 10) {
        const denom: f64 = @as(f64, @floatFromInt(2 *% k +% 1));
        sum = sum + term / denom;
        term = term * y2;
        k = k +% 1;
    }
    return 2.0 * sum;
}

fn exp(x_4: f64) f64 {
    var term_1: f64 = 1.0;
    term_1 = term_1;
    var sum_1: f64 = 1.0;
    sum_1 = sum_1;
    var n_1: i64 = 1;
    n_1 = n_1;
    while (n_1 < 20) {
        term_1 = term_1 * x_4 / @as(f64, @floatFromInt(n_1));
        sum_1 = sum_1 + term_1;
        n_1 = n_1 +% 1;
    }
    return sum_1;
}

fn mean(v: []f64) f64 {
    var total: f64 = 0.0;
    total = total;
    var i_1: i64 = 0;
    i_1 = i_1;
    while (i_1 < @as(i64, @intCast(v.len))) {
        total = total + v[_idx(v.len, i_1)];
        i_1 = i_1 +% 1;
    }
    return total / @as(f64, @floatFromInt(@as(i64, @intCast(v.len))));
}

fn binary_cross_entropy(y_true: []f64, y_pred: []f64, epsilon: f64) f64 {
    if (@as(i64, @intCast(y_true.len)) != @as(i64, @intCast(y_pred.len))) {
        @panic("Input arrays must have the same length.");
    }
    var losses: []f64 = std.heap.page_allocator.alloc(f64, 0) catch unreachable;
    losses = losses;
    var i_2: i64 = 0;
    i_2 = i_2;
    while (i_2 < @as(i64, @intCast(y_true.len))) {
        const yt: f64 = y_true[_idx(y_true.len, i_2)];
        const yp: f64 = clip(y_pred[_idx(y_pred.len, i_2)], epsilon, 1.0 - epsilon);
        const loss: f64 = @as(f64, @floatFromInt(0)) - yt * ln(yp) + (1.0 - yt) * ln(1.0 - yp);
        losses = blk0: { var _tmp = std.ArrayList(f64).init(std.heap.page_allocator); _tmp.appendSlice(@as([]const f64, losses)) catch |err| handleError(err); _tmp.append(loss) catch |err| handleError(err); break :blk0 (_tmp.toOwnedSlice() catch |err| handleError(err)); };
        i_2 = i_2 +% 1;
    }
    return mean(losses);
}

fn binary_focal_cross_entropy(y_true_1: []f64, y_pred_1: []f64, gamma: f64, alpha: f64, epsilon_1: f64) f64 {
    if (@as(i64, @intCast(y_true_1.len)) != @as(i64, @intCast(y_pred_1.len))) {
        @panic("Input arrays must have the same length.");
    }
    var losses_1: []f64 = std.heap.page_allocator.alloc(f64, 0) catch unreachable;
    losses_1 = losses_1;
    var i_3: i64 = 0;
    i_3 = i_3;
    while (i_3 < @as(i64, @intCast(y_true_1.len))) {
        const yt_1: f64 = y_true_1[_idx(y_true_1.len, i_3)];
        const yp_1: f64 = clip(y_pred_1[_idx(y_pred_1.len, i_3)], epsilon_1, 1.0 - epsilon_1);
        const term1: f64 = alpha * powf(1.0 - yp_1, gamma) * yt_1 * ln(yp_1);
        const term2: f64 = (1.0 - alpha) * powf(yp_1, gamma) * (1.0 - yt_1) * ln(1.0 - yp_1);
        losses_1 = blk1: { var _tmp_1 = std.ArrayList(f64).init(std.heap.page_allocator); _tmp_1.appendSlice(@as([]const f64, losses_1)) catch |err| handleError(err); _tmp_1.append(@as(f64, @floatFromInt(0)) - term1 + term2) catch |err| handleError(err); break :blk1 (_tmp_1.toOwnedSlice() catch |err| handleError(err)); };
        i_3 = i_3 +% 1;
    }
    return mean(losses_1);
}

fn categorical_cross_entropy(y_true_2: [][]f64, y_pred_2: [][]f64, epsilon_2: f64) f64 {
    if (@as(i64, @intCast(y_true_2.len)) != @as(i64, @intCast(y_pred_2.len))) {
        @panic("Input arrays must have the same shape.");
    }
    const rows: i64 = @as(i64, @intCast(y_true_2.len));
    var total_1: f64 = 0.0;
    total_1 = total_1;
    var i_4: i64 = 0;
    i_4 = i_4;
    while (i_4 < rows) {
        if (@as(i64, @intCast(y_true_2[_idx(y_true_2.len, i_4)].len)) != @as(i64, @intCast(y_pred_2[_idx(y_pred_2.len, i_4)].len))) {
            @panic("Input arrays must have the same shape.");
        }
        var sum_true: f64 = 0.0;
        sum_true = sum_true;
        var sum_pred: f64 = 0.0;
        sum_pred = sum_pred;
        var j: i64 = 0;
        j = j;
        while (j < @as(i64, @intCast(y_true_2[_idx(y_true_2.len, i_4)].len))) {
            const yt_2: f64 = y_true_2[_idx(y_true_2.len, i_4)][_idx(y_true_2[_idx(y_true_2.len, i_4)].len, j)];
            const yp_2: f64 = y_pred_2[_idx(y_pred_2.len, i_4)][_idx(y_pred_2[_idx(y_pred_2.len, i_4)].len, j)];
            if (yt_2 != 0.0 and yt_2 != 1.0) {
                @panic("y_true must be one-hot encoded.");
            }
            sum_true = sum_true + yt_2;
            sum_pred = sum_pred + yp_2;
            j = j +% 1;
        }
        if (sum_true != 1.0) {
            @panic("y_true must be one-hot encoded.");
        }
        if (absf(sum_pred - 1.0) > epsilon_2) {
            @panic("Predicted probabilities must sum to approximately 1.");
        }
        j = 0;
        while (j < @as(i64, @intCast(y_true_2[_idx(y_true_2.len, i_4)].len))) {
            const yp_3: f64 = clip(y_pred_2[_idx(y_pred_2.len, i_4)][_idx(y_pred_2[_idx(y_pred_2.len, i_4)].len, j)], epsilon_2, 1.0);
            total_1 = total_1 - y_true_2[_idx(y_true_2.len, i_4)][_idx(y_true_2[_idx(y_true_2.len, i_4)].len, j)] * ln(yp_3);
            j = j +% 1;
        }
        i_4 = i_4 +% 1;
    }
    return total_1;
}

fn categorical_focal_cross_entropy(y_true_3: [][]f64, y_pred_3: [][]f64, alpha_1: []f64, gamma_1: f64, epsilon_3: f64) f64 {
    if (@as(i64, @intCast(y_true_3.len)) != @as(i64, @intCast(y_pred_3.len))) {
        @panic("Shape of y_true and y_pred must be the same.");
    }
    const rows_1: i64 = @as(i64, @intCast(y_true_3.len));
    const cols: i64 = @as(i64, @intCast(y_true_3[_idx(y_true_3.len, 0)].len));
    var a_2: []f64 = blk2: { const tmp = std.heap.page_allocator.alloc(f64, alpha_1.len) catch unreachable; @memcpy(tmp, alpha_1); break :blk2 tmp; };
    a_2 = a_2;
    if (@as(i64, @intCast(a_2.len)) == 0) {
        var tmp: []f64 = std.heap.page_allocator.alloc(f64, 0) catch unreachable;
        tmp = tmp;
        var j_1: i64 = 0;
        j_1 = j_1;
        while (j_1 < cols) {
            tmp = blk3: { var _tmp_2 = std.ArrayList(f64).init(std.heap.page_allocator); _tmp_2.appendSlice(@as([]const f64, tmp)) catch |err| handleError(err); _tmp_2.append(1.0) catch |err| handleError(err); break :blk3 (_tmp_2.toOwnedSlice() catch |err| handleError(err)); };
            j_1 = j_1 +% 1;
        }
        a_2 = tmp;
    }
    if (@as(i64, @intCast(a_2.len)) != cols) {
        @panic("Length of alpha must match the number of classes.");
    }
    var total_2: f64 = 0.0;
    total_2 = total_2;
    var i_5: i64 = 0;
    i_5 = i_5;
    while (i_5 < rows_1) {
        if (@as(i64, @intCast(y_true_3[_idx(y_true_3.len, i_5)].len)) != cols or @as(i64, @intCast(y_pred_3[_idx(y_pred_3.len, i_5)].len)) != cols) {
            @panic("Shape of y_true and y_pred must be the same.");
        }
        var sum_true_1: f64 = 0.0;
        sum_true_1 = sum_true_1;
        var sum_pred_1: f64 = 0.0;
        sum_pred_1 = sum_pred_1;
        var j_2: i64 = 0;
        j_2 = j_2;
        while (j_2 < cols) {
            const yt_3: f64 = y_true_3[_idx(y_true_3.len, i_5)][_idx(y_true_3[_idx(y_true_3.len, i_5)].len, j_2)];
            const yp_4: f64 = y_pred_3[_idx(y_pred_3.len, i_5)][_idx(y_pred_3[_idx(y_pred_3.len, i_5)].len, j_2)];
            if (yt_3 != 0.0 and yt_3 != 1.0) {
                @panic("y_true must be one-hot encoded.");
            }
            sum_true_1 = sum_true_1 + yt_3;
            sum_pred_1 = sum_pred_1 + yp_4;
            j_2 = j_2 +% 1;
        }
        if (sum_true_1 != 1.0) {
            @panic("y_true must be one-hot encoded.");
        }
        if (absf(sum_pred_1 - 1.0) > epsilon_3) {
            @panic("Predicted probabilities must sum to approximately 1.");
        }
        var row_loss: f64 = 0.0;
        row_loss = row_loss;
        j_2 = 0;
        while (j_2 < cols) {
            const yp_5: f64 = clip(y_pred_3[_idx(y_pred_3.len, i_5)][_idx(y_pred_3[_idx(y_pred_3.len, i_5)].len, j_2)], epsilon_3, 1.0);
            row_loss = row_loss + a_2[_idx(a_2.len, j_2)] * powf(1.0 - yp_5, gamma_1) * y_true_3[_idx(y_true_3.len, i_5)][_idx(y_true_3[_idx(y_true_3.len, i_5)].len, j_2)] * ln(yp_5);
            j_2 = j_2 +% 1;
        }
        total_2 = total_2 - row_loss;
        i_5 = i_5 +% 1;
    }
    return total_2 / @as(f64, @floatFromInt(rows_1));
}

fn hinge_loss(y_true_4: []f64, y_pred_4: []f64) f64 {
    if (@as(i64, @intCast(y_true_4.len)) != @as(i64, @intCast(y_pred_4.len))) {
        @panic("Length of predicted and actual array must be same.");
    }
    var losses_2: []f64 = std.heap.page_allocator.alloc(f64, 0) catch unreachable;
    losses_2 = losses_2;
    var i_6: i64 = 0;
    i_6 = i_6;
    while (i_6 < @as(i64, @intCast(y_true_4.len))) {
        const yt_4: f64 = y_true_4[_idx(y_true_4.len, i_6)];
        if (yt_4 != @as(f64, @floatFromInt(0)) - 1.0 and yt_4 != 1.0) {
            @panic("y_true can have values -1 or 1 only.");
        }
        const pred: f64 = y_pred_4[_idx(y_pred_4.len, i_6)];
        const l: f64 = maxf(0.0, 1.0 - yt_4 * pred);
        losses_2 = blk4: { var _tmp_3 = std.ArrayList(f64).init(std.heap.page_allocator); _tmp_3.appendSlice(@as([]const f64, losses_2)) catch |err| handleError(err); _tmp_3.append(l) catch |err| handleError(err); break :blk4 (_tmp_3.toOwnedSlice() catch |err| handleError(err)); };
        i_6 = i_6 +% 1;
    }
    return mean(losses_2);
}

fn huber_loss(y_true_5: []f64, y_pred_5: []f64, delta: f64) f64 {
    if (@as(i64, @intCast(y_true_5.len)) != @as(i64, @intCast(y_pred_5.len))) {
        @panic("Input arrays must have the same length.");
    }
    var total_3: f64 = 0.0;
    total_3 = total_3;
    var i_7: i64 = 0;
    i_7 = i_7;
    while (i_7 < @as(i64, @intCast(y_true_5.len))) {
        const diff: f64 = y_true_5[_idx(y_true_5.len, i_7)] - y_pred_5[_idx(y_pred_5.len, i_7)];
        const adiff: f64 = absf(diff);
        if (adiff <= delta) {
            total_3 = total_3 + 0.5 * diff * diff;
        } else {
            total_3 = total_3 + delta * (adiff - 0.5 * delta);
        }
        i_7 = i_7 +% 1;
    }
    return total_3 / @as(f64, @floatFromInt(@as(i64, @intCast(y_true_5.len))));
}

fn mean_squared_error(y_true_6: []f64, y_pred_6: []f64) f64 {
    if (@as(i64, @intCast(y_true_6.len)) != @as(i64, @intCast(y_pred_6.len))) {
        @panic("Input arrays must have the same length.");
    }
    var losses_3: []f64 = std.heap.page_allocator.alloc(f64, 0) catch unreachable;
    losses_3 = losses_3;
    var i_8: i64 = 0;
    i_8 = i_8;
    while (i_8 < @as(i64, @intCast(y_true_6.len))) {
        const diff_1: f64 = y_true_6[_idx(y_true_6.len, i_8)] - y_pred_6[_idx(y_pred_6.len, i_8)];
        losses_3 = blk5: { var _tmp_4 = std.ArrayList(f64).init(std.heap.page_allocator); _tmp_4.appendSlice(@as([]const f64, losses_3)) catch |err| handleError(err); _tmp_4.append(diff_1 * diff_1) catch |err| handleError(err); break :blk5 (_tmp_4.toOwnedSlice() catch |err| handleError(err)); };
        i_8 = i_8 +% 1;
    }
    return mean(losses_3);
}

fn mean_absolute_error(y_true_7: []f64, y_pred_7: []f64) f64 {
    if (@as(i64, @intCast(y_true_7.len)) != @as(i64, @intCast(y_pred_7.len))) {
        @panic("Input arrays must have the same length.");
    }
    var total_4: f64 = 0.0;
    total_4 = total_4;
    var i_9: i64 = 0;
    i_9 = i_9;
    while (i_9 < @as(i64, @intCast(y_true_7.len))) {
        total_4 = total_4 + absf(y_true_7[_idx(y_true_7.len, i_9)] - y_pred_7[_idx(y_pred_7.len, i_9)]);
        i_9 = i_9 +% 1;
    }
    return total_4 / @as(f64, @floatFromInt(@as(i64, @intCast(y_true_7.len))));
}

fn mean_squared_logarithmic_error(y_true_8: []f64, y_pred_8: []f64) f64 {
    if (@as(i64, @intCast(y_true_8.len)) != @as(i64, @intCast(y_pred_8.len))) {
        @panic("Input arrays must have the same length.");
    }
    var total_5: f64 = 0.0;
    total_5 = total_5;
    var i_10: i64 = 0;
    i_10 = i_10;
    while (i_10 < @as(i64, @intCast(y_true_8.len))) {
        const a_3: f64 = ln(1.0 + y_true_8[_idx(y_true_8.len, i_10)]);
        const b_2: f64 = ln(1.0 + y_pred_8[_idx(y_pred_8.len, i_10)]);
        const diff_2: f64 = a_3 - b_2;
        total_5 = total_5 + diff_2 * diff_2;
        i_10 = i_10 +% 1;
    }
    return total_5 / @as(f64, @floatFromInt(@as(i64, @intCast(y_true_8.len))));
}

fn mean_absolute_percentage_error(y_true_9: []f64, y_pred_9: []f64, epsilon_4: f64) f64 {
    if (@as(i64, @intCast(y_true_9.len)) != @as(i64, @intCast(y_pred_9.len))) {
        @panic("The length of the two arrays should be the same.");
    }
    var total_6: f64 = 0.0;
    total_6 = total_6;
    var i_11: i64 = 0;
    i_11 = i_11;
    while (i_11 < @as(i64, @intCast(y_true_9.len))) {
        var yt_5: f64 = y_true_9[_idx(y_true_9.len, i_11)];
        yt_5 = yt_5;
        if (yt_5 == 0.0) {
            yt_5 = epsilon_4;
        }
        total_6 = total_6 + absf((yt_5 - y_pred_9[_idx(y_pred_9.len, i_11)]) / yt_5);
        i_11 = i_11 +% 1;
    }
    return total_6 / @as(f64, @floatFromInt(@as(i64, @intCast(y_true_9.len))));
}

fn perplexity_loss(y_true_10: [][]i64, y_pred_10: [][][]f64, epsilon_5: f64) f64 {
    const batch: i64 = @as(i64, @intCast(y_true_10.len));
    if (batch != @as(i64, @intCast(y_pred_10.len))) {
        @panic("Batch size of y_true and y_pred must be equal.");
    }
    const sentence_len: i64 = @as(i64, @intCast(y_true_10[_idx(y_true_10.len, 0)].len));
    if (sentence_len != @as(i64, @intCast(y_pred_10[_idx(y_pred_10.len, 0)].len))) {
        @panic("Sentence length of y_true and y_pred must be equal.");
    }
    const vocab_size: i64 = @as(i64, @intCast(y_pred_10[_idx(y_pred_10.len, 0)][_idx(y_pred_10[_idx(y_pred_10.len, 0)].len, 0)].len));
    var b_3: i64 = 0;
    b_3 = b_3;
    var total_perp: f64 = 0.0;
    total_perp = total_perp;
    while (b_3 < batch) {
        if (@as(i64, @intCast(y_true_10[_idx(y_true_10.len, b_3)].len)) != sentence_len or @as(i64, @intCast(y_pred_10[_idx(y_pred_10.len, b_3)].len)) != sentence_len) {
            @panic("Sentence length of y_true and y_pred must be equal.");
        }
        var sum_log: f64 = 0.0;
        sum_log = sum_log;
        var j_3: i64 = 0;
        j_3 = j_3;
        while (j_3 < sentence_len) {
            const label: i64 = y_true_10[_idx(y_true_10.len, b_3)][_idx(y_true_10[_idx(y_true_10.len, b_3)].len, j_3)];
            if (label >= vocab_size) {
                @panic("Label value must not be greater than vocabulary size.");
            }
            const prob: f64 = clip(y_pred_10[_idx(y_pred_10.len, b_3)][_idx(y_pred_10[_idx(y_pred_10.len, b_3)].len, j_3)][_idx(y_pred_10[_idx(y_pred_10.len, b_3)][_idx(y_pred_10[_idx(y_pred_10.len, b_3)].len, j_3)].len, label)], epsilon_5, 1.0);
            sum_log = sum_log + ln(prob);
            j_3 = j_3 +% 1;
        }
        const mean_log: f64 = sum_log / @as(f64, @floatFromInt(sentence_len));
        const perp: f64 = exp(@as(f64, @floatFromInt(0)) - mean_log);
        total_perp = total_perp + perp;
        b_3 = b_3 +% 1;
    }
    return total_perp / @as(f64, @floatFromInt(batch));
}

fn smooth_l1_loss(y_true_11: []f64, y_pred_11: []f64, beta: f64) f64 {
    if (@as(i64, @intCast(y_true_11.len)) != @as(i64, @intCast(y_pred_11.len))) {
        @panic("The length of the two arrays should be the same.");
    }
    var total_7: f64 = 0.0;
    total_7 = total_7;
    var i_12: i64 = 0;
    i_12 = i_12;
    while (i_12 < @as(i64, @intCast(y_true_11.len))) {
        const diff_3: f64 = absf(y_true_11[_idx(y_true_11.len, i_12)] - y_pred_11[_idx(y_pred_11.len, i_12)]);
        if (diff_3 < beta) {
            total_7 = total_7 + 0.5 * diff_3 * diff_3 / beta;
        } else {
            total_7 = total_7 + diff_3 - 0.5 * beta;
        }
        i_12 = i_12 +% 1;
    }
    return total_7 / @as(f64, @floatFromInt(@as(i64, @intCast(y_true_11.len))));
}

fn kullback_leibler_divergence(y_true_12: []f64, y_pred_12: []f64) f64 {
    if (@as(i64, @intCast(y_true_12.len)) != @as(i64, @intCast(y_pred_12.len))) {
        @panic("Input arrays must have the same length.");
    }
    var total_8: f64 = 0.0;
    total_8 = total_8;
    var i_13: i64 = 0;
    i_13 = i_13;
    while (i_13 < @as(i64, @intCast(y_true_12.len))) {
        total_8 = total_8 + y_true_12[_idx(y_true_12.len, i_13)] * ln(y_true_12[_idx(y_true_12.len, i_13)] / y_pred_12[_idx(y_pred_12.len, i_13)]);
        i_13 = i_13 +% 1;
    }
    return total_8;
}

fn mochi_main() void {
    const y_true_bc: []f64 = @constCast(([5]f64{0.0, 1.0, 1.0, 0.0, 1.0})[0..5]);
    const y_pred_bc: []f64 = @constCast(([5]f64{0.2, 0.7, 0.9, 0.3, 0.8})[0..5]);
    std.debug.print("{d}\n", .{binary_cross_entropy(y_true_bc, y_pred_bc, 0.000000000000001)});
    std.debug.print("{d}\n", .{binary_focal_cross_entropy(y_true_bc, y_pred_bc, 2.0, 0.25, 0.000000000000001)});
    const y_true_cce: [][]f64 = @constCast(([3][]f64{@constCast(([3]f64{1.0, 0.0, 0.0})[0..3]), @constCast(([3]f64{0.0, 1.0, 0.0})[0..3]), @constCast(([3]f64{0.0, 0.0, 1.0})[0..3])})[0..3]);
    const y_pred_cce: [][]f64 = @constCast(([3][]f64{@constCast(([3]f64{0.9, 0.1, 0.0})[0..3]), @constCast(([3]f64{0.2, 0.7, 0.1})[0..3]), @constCast(([3]f64{0.0, 0.1, 0.9})[0..3])})[0..3]);
    std.debug.print("{d}\n", .{categorical_cross_entropy(y_true_cce, y_pred_cce, 0.000000000000001)});
    const alpha_2: []f64 = @constCast(([3]f64{0.6, 0.2, 0.7})[0..3]);
    std.debug.print("{d}\n", .{categorical_focal_cross_entropy(y_true_cce, y_pred_cce, alpha_2, 2.0, 0.000000000000001)});
    const y_true_hinge: []f64 = blk6: { var _tmp_5 = std.ArrayList(f64).init(std.heap.page_allocator); _tmp_5.append(@as(f64, @floatFromInt(0)) - 1.0) catch unreachable; _tmp_5.append(1.0) catch unreachable; _tmp_5.append(1.0) catch unreachable; _tmp_5.append(@as(f64, @floatFromInt(0)) - 1.0) catch unreachable; _tmp_5.append(1.0) catch unreachable; break :blk6 (_tmp_5.toOwnedSlice() catch unreachable); };
    const y_pred_hinge: []f64 = blk7: { var _tmp_6 = std.ArrayList(f64).init(std.heap.page_allocator); _tmp_6.append(@as(f64, @floatFromInt(0)) - 4.0) catch unreachable; _tmp_6.append(@as(f64, @floatFromInt(0)) - 0.3) catch unreachable; _tmp_6.append(0.7) catch unreachable; _tmp_6.append(5.0) catch unreachable; _tmp_6.append(10.0) catch unreachable; break :blk7 (_tmp_6.toOwnedSlice() catch unreachable); };
    std.debug.print("{d}\n", .{hinge_loss(y_true_hinge, y_pred_hinge)});
    const y_true_huber: []f64 = @constCast(([5]f64{0.9, 10.0, 2.0, 1.0, 5.2})[0..5]);
    const y_pred_huber: []f64 = @constCast(([5]f64{0.8, 2.1, 2.9, 4.2, 5.2})[0..5]);
    std.debug.print("{d}\n", .{huber_loss(y_true_huber, y_pred_huber, 1.0)});
    std.debug.print("{d}\n", .{mean_squared_error(y_true_huber, y_pred_huber)});
    std.debug.print("{d}\n", .{mean_absolute_error(y_true_huber, y_pred_huber)});
    std.debug.print("{d}\n", .{mean_squared_logarithmic_error(y_true_huber, y_pred_huber)});
    const y_true_mape: []f64 = @constCast(([4]f64{10.0, 20.0, 30.0, 40.0})[0..4]);
    const y_pred_mape: []f64 = @constCast(([4]f64{12.0, 18.0, 33.0, 45.0})[0..4]);
    std.debug.print("{d}\n", .{mean_absolute_percentage_error(y_true_mape, y_pred_mape, 0.000000000000001)});
    const y_true_perp: [][]i64 = @constCast(([2][]i64{@constCast(([2]i64{1, 4})[0..2]), @constCast(([2]i64{2, 3})[0..2])})[0..2]);
    const y_pred_perp: [][][]f64 = @constCast(([2][][]f64{@constCast(([2][]f64{@constCast(([5]f64{0.28, 0.19, 0.21, 0.15, 0.17})[0..5]), @constCast(([5]f64{0.24, 0.19, 0.09, 0.18, 0.3})[0..5])})[0..2]), @constCast(([2][]f64{@constCast(([5]f64{0.03, 0.26, 0.21, 0.18, 0.32})[0..5]), @constCast(([5]f64{0.28, 0.1, 0.33, 0.15, 0.14})[0..5])})[0..2])})[0..2]);
    std.debug.print("{d}\n", .{perplexity_loss(y_true_perp, y_pred_perp, 0.0000001)});
    const y_true_smooth: []f64 = @constCast(([4]f64{3.0, 5.0, 2.0, 7.0})[0..4]);
    const y_pred_smooth: []f64 = @constCast(([4]f64{2.9, 4.8, 2.1, 7.2})[0..4]);
    std.debug.print("{d}\n", .{smooth_l1_loss(y_true_smooth, y_pred_smooth, 1.0)});
    const y_true_kl: []f64 = @constCast(([3]f64{0.2, 0.3, 0.5})[0..3]);
    const y_pred_kl: []f64 = @constCast(([3]f64{0.3, 0.3, 0.4})[0..3]);
    std.debug.print("{d}\n", .{kullback_leibler_divergence(y_true_kl, y_pred_kl)});
}

pub fn main() void {
    {
        const __start = _now();
        const __start_mem: i64 = _mem();
        mochi_main();
        const __end = _now();
        const __end_mem: i64 = _mem();
        const __duration_us: i64 = @divTrunc(@as(i64, @intCast(__end - __start)), 1000);
        const __mem_diff: i64 = __end_mem - __start_mem;
        const __memory_bytes: i64 = if (__mem_diff < 0) -__mem_diff else __mem_diff;
        std.debug.print("{{\"duration_us\":{d},\"memory_bytes\":{d},\"name\":\"main\"}}\n", .{__duration_us, __memory_bytes});
    }
}

var _now_seed: i64 = 0;
var _now_seeded: bool = false;
fn _now() i64 {
    if (_now_seeded) {
        _now_seed = @mod(_now_seed * 1664525 + 1013904223, 2147483647);
        return _now_seed;
    }
    if (! _now_seeded) {
        if (std.process.getEnvVarOwned(std.heap.page_allocator, "MOCHI_NOW_SEED")) |env_seed| {
            defer std.heap.page_allocator.free(env_seed);
            if (std.fmt.parseInt(i64, env_seed, 10)) |v| {
                _now_seed = v;
                _now_seeded = true;
                _now_seed = @mod(_now_seed * 1664525 + 1013904223, 2147483647);
                return _now_seed;
            } else |_| {}
        } else |_| {}
    }
    return @as(i64, @intCast(std.time.nanoTimestamp()));
}

fn _idx(len: usize, i: i64) usize {
    return if (i < 0 or i >= @as(i64, @intCast(len))) 0 else @as(usize, @intCast(i));
}

fn _mem() i64 {
    const path = "/proc/self/statm";
    var file = std.fs.openFileAbsolute(path, .{}) catch return 0;
    defer file.close();
    var buf: [64]u8 = undefined;
    const n = file.readAll(&buf) catch return 0;
    var it = std.mem.tokenizeScalar(u8, buf[0..n], ' ');
    _ = it.next(); // total program size
    if (it.next()) |tok| {
        const pages = std.fmt.parseInt(i64, tok, 10) catch return 0;
        return pages * @as(i64, @intCast(std.mem.page_size));
    }
    return 0;
}

fn _print(v: []const u8) void {
    std.debug.print("{s}\n", .{v});
}
