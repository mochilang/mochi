// Generated by Mochi transpiler v0.10.61 on 2025-08-08 16:01:40 GMT+7
import Foundation
import Dispatch
#if canImport(FoundationNetworking)
import FoundationNetworking
#endif

let stdout = FileHandle.standardOutput
extension FileHandle {
    func write(_ string: String) {
        if let data = string.data(using: .utf8) {
            self.write(data)
        }
    }
}

func _p(_ v: Any?) -> String {
    if let val = v {
        if let d = val as? Double {
            if d.rounded(.towardZero) == d {
                return String(Int64(d))
            }
        }
        return String(describing: val)
    }
    return "<nil>"
}

extension Double { init(_ v: Any) { if let d = v as? Double { self = d } else if let i = v as? Int { self = Double(i) } else if let i = v as? Int64 { self = Double(i) } else if let s = v as? String { self = Double(s) ?? 0 } else { self = 0 } } }
var _nowSeed = 0
var _nowSeeded = false
func _now() -> Int {
    if !_nowSeeded {
        if let s = ProcessInfo.processInfo.environment["MOCHI_NOW_SEED"], let v = Int(s) {
            _nowSeed = v
            _nowSeeded = true
        }
    }
    if _nowSeeded {
        _nowSeed = (_nowSeed * 1664525 + 1013904223) % 2147483647
        return _nowSeed
    }
    return Int(DispatchTime.now().uptimeNanoseconds)
}
func _int(_ v: Any) -> Int {
    if let s = v as? String { return Int(s) ?? 0 }
    if let i = v as? Int { return i }
    if let i = v as? Int64 { return Int(i) }
    if let d = v as? Double { return Int(d) }
    return 0
}
func _mem() -> Int {
    if let status = try? String(contentsOfFile: "/proc/self/status") {
        for line in status.split(separator: "\n") {
            if line.hasPrefix("VmRSS:") {
                let parts = line.split(whereSeparator: { $0 == " " || $0 == "\t" })
                if parts.count >= 2, let kb = Int(parts[1]) {
                    return kb * 1024
                }
            }
        }
    }
    return 0
}
func _idx<T>(_ xs: [T], _ i: Int) -> T? {
    var idx = i
    if idx < 0 { idx += xs.count }
    if idx >= 0 && idx < xs.count { return xs[idx] }
    return nil
}
func _append<T>(_ xs: [T], _ v: T) -> [T] {
    var out = xs
    out.append(v)
    return out
}
struct DataPoint: Codable {
    var x: [Double]
    var y: Double
    init() {
        self.x = []
        self.y = 0
    }
    init(x: [Double], y: Double) {
        self.x = x
        self.y = y
    }
}
do {
    let _benchMemStart = _mem()
    let _benchStart = _now()
    func absf(_ x: Double) -> Double {
        if (x < 0.0) {
            return -x
        }
        return x
    }
    func hypothesis_value(_ input: [Double], _ params: [Double]) -> Double {
        var value: Double = Double(_idx(params, 0))
        var i: Int = 0
        while (i < _int(((input).count))) {
            value = Double((value + (Double(_idx(input, i)) * Double(_idx(params, (i &+ 1))))))
            i = _int((i &+ 1))
        }
        return value
    }
    func calc_error(_ dp: DataPoint, _ params: [Double]) -> Double {
        return (Double(hypothesis_value((dp.x as! [Double]), (params as! [Double]))) - dp.y)
    }
    func summation_of_cost_derivative(_ index: Int, _ params: [Double], _ data: [DataPoint]) -> Double {
        var sum: Double = 0.0
        var i: Int = 0
        while (i < _int(((data).count))) {
            let dp = (_idx(data, i) as! DataPoint)
            let e = Double(calc_error((dp as! DataPoint), (params as! [Double])))
            if (index == _int(-1)) {
                sum = Double((sum + e))
            } else {
                sum = Double((sum + (e * Double(_idx(dp.x, index)))))
            }
            i = _int((i &+ 1))
        }
        return sum
    }
    func get_cost_derivative(_ index: Int, _ params: [Double], _ data: [DataPoint]) -> Double {
        return (Double(summation_of_cost_derivative(index, (params as! [Double]), (data as! [DataPoint]))) / Double(((data).count)))
    }
    func allclose(_ a: [Double], _ b: [Double], _ atol: Double, _ rtol: Double) -> Bool {
        var i: Int = 0
        while (i < _int(((a).count))) {
            let diff = Double(absf(Double((Double(_idx(a, i)) - Double(_idx(b, i))))))
            let limit = (atol + (rtol * Double(absf(Double(_idx(b, i))))))
            if (diff > limit) {
                return false
            }
            i = _int((i &+ 1))
        }
        return true
    }
    func run_gradient_descent(_ train_data: [DataPoint], _ initial_params: [Double]) -> [Double] {
        let learning_rate = 0.009
        let absolute_error_limit = 0.000002
        let relative_error_limit = 0.0
        var j: Int = 0
        var params: [Double] = initial_params
        while true {
            j = _int((j &+ 1))
            var temp: [Double] = ([] as! [Double])
            var i: Int = 0
            while (i < _int(((params).count))) {
                let deriv = Double(get_cost_derivative((i &- 1), (params as! [Double]), (train_data as! [DataPoint])))
                temp = (_append(temp, (Double(_idx(params, i)) - (learning_rate * deriv))) as! [Double])
                i = _int((i &+ 1))
            }
            if allclose((params as! [Double]), (temp as! [Double]), Double(absolute_error_limit), Double(relative_error_limit)) {
                print(_p(("Number of iterations:" + _p(j))))
                break
            }
            params = (temp as! [Double])
        }
        return params
    }
    func test_gradient_descent(_ test_data: [DataPoint], _ params: [Double]) {
        var i: Int = 0
        while (i < _int(((test_data).count))) {
            let dp = (_idx(test_data, i) as! DataPoint)
            print(_p(("Actual output value:" + _p(dp.y))))
            print(_p(("Hypothesis output:" + _p(Double(hypothesis_value((dp.x as! [Double]), (params as! [Double])))))))
            i = _int((i &+ 1))
        }
    }
    let train_data: [DataPoint] = ([DataPoint(x: ([5.0, 2.0, 3.0] as! [Double]), y: 15.0), DataPoint(x: ([6.0, 5.0, 9.0] as! [Double]), y: 25.0), DataPoint(x: ([11.0, 12.0, 13.0] as! [Double]), y: 41.0), DataPoint(x: ([1.0, 1.0, 1.0] as! [Double]), y: 8.0), DataPoint(x: ([11.0, 12.0, 13.0] as! [Double]), y: 41.0)] as! [DataPoint])
    let test_data: [DataPoint] = ([DataPoint(x: ([515.0, 22.0, 13.0] as! [Double]), y: 555.0), DataPoint(x: ([61.0, 35.0, 49.0] as! [Double]), y: 150.0)] as! [DataPoint])
    var parameter_vector: [Double] = ([2.0, 4.0, 1.0, 5.0] as! [Double])
    parameter_vector = (run_gradient_descent((train_data as! [DataPoint]), (parameter_vector as! [Double])) as! [Double])
    print(_p("\nTesting gradient descent for a linear hypothesis function.\n"))
    _ = test_gradient_descent((test_data as! [DataPoint]), (parameter_vector as! [Double]))
    let _benchEnd = _now()
    let _benchMemEnd = _mem()
    print("{\n  \"duration_us\": \((_benchEnd - _benchStart) / 1000),\n  \"memory_bytes\": \(_benchMemEnd - _benchMemStart),\n  \"name\": \"main\"\n}")
}
