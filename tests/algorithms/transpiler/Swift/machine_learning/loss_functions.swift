// Generated by Mochi transpiler v0.10.62 on 2025-08-09 16:38:25 GMT+7
import Foundation
import Dispatch
#if canImport(FoundationNetworking)
import FoundationNetworking
#endif

let stdout = FileHandle.standardOutput
extension FileHandle {
    func write(_ string: String) {
        if let data = string.data(using: .utf8) {
            self.write(data)
        }
    }
}

func _p(_ v: Any?) -> String {
    if let val = v {
        if let d = val as? Double {
            if d.rounded(.towardZero) == d {
                return String(Int64(d))
            }
        }
        return String(describing: val)
    }
    return "<nil>"
}

extension Double { init(_ v: Any) { if let d = v as? Double { self = d } else if let i = v as? Int { self = Double(i) } else if let i = v as? Int64 { self = Double(i) } else if let s = v as? String { self = Double(s) ?? 0 } else { self = 0 } } }
var _nowSeed = 0
var _nowSeeded = false
func _now() -> Int {
    if !_nowSeeded {
        if let s = ProcessInfo.processInfo.environment["MOCHI_NOW_SEED"], let v = Int(s) {
            _nowSeed = v
            _nowSeeded = true
        }
    }
    if _nowSeeded {
        _nowSeed = (_nowSeed * 1664525 + 1013904223) % 2147483647
        return _nowSeed
    }
    return Int(DispatchTime.now().uptimeNanoseconds)
}
func _int(_ v: Any) -> Int {
    if let s = v as? String { return Int(s) ?? 0 }
    if let i = v as? Int { return i }
    if let i = v as? Int64 { return Int(i) }
    if let d = v as? Double { return Int(d) }
    return 0
}
func _mem() -> Int {
    if let status = try? String(contentsOfFile: "/proc/self/status") {
        for line in status.split(separator: "\n") {
            if line.hasPrefix("VmRSS:") {
                let parts = line.split(whereSeparator: { $0 == " " || $0 == "\t" })
                if parts.count >= 2, let kb = Int(parts[1]) {
                    return kb * 1024
                }
            }
        }
    }
    return 0
}
func _idx<T>(_ xs: [T], _ i: Int) -> T? {
    var idx = i
    if idx < 0 { idx += xs.count }
    if idx >= 0 && idx < xs.count { return xs[idx] }
    return xs.first
}
func _append<T>(_ xs: [T], _ v: T) -> [T] {
    var out = xs
    out.append(v)
    return out
}
do {
    let _benchMemStart = _mem()
    let _benchStart = _now()
    func absf(_ x: Double) -> Double {
        if (x < 0.0) {
            return -x
        }
        return x
    }
    func maxf(_ a: Double, _ b: Double) -> Double {
        if (a > b) {
            return a
        }
        return b
    }
    func minf(_ a: Double, _ b: Double) -> Double {
        if (a < b) {
            return a
        }
        return b
    }
    func clip(_ x: Double, _ lo: Double, _ hi: Double) -> Double {
        return Double(maxf(Double(lo), Double(minf(Double(x), Double(hi)))))
    }
    func to_float(_ x: Int) -> Double {
        return (Double(x) * 1.0)
    }
    func powf(_ base: Double, _ exp: Double) -> Double {
        var result = 1.0
        var i = 0
        let n = _int(_int(exp))
        while (i < n) {
            result = Double((result * base))
            i = _int((i &+ 1))
        }
        return result
    }
    func ln(_ x: Double) -> Double {
        if (x <= 0.0) {
            _ = fatalError("ln domain error")
        }
        let y = (Double((x - 1.0)) / Double((x + 1.0)))
        let y2 = (y * y)
        var term = y
        var sum = 0.0
        var k = 0
        while (k < 10) {
            let denom = Double(to_float(((2 &* k) &+ 1)))
            sum = Double((sum + (term / denom)))
            term = Double((term * y2))
            k = _int((k &+ 1))
        }
        return (2.0 * sum)
    }
    func exp(_ x: Double) -> Double {
        var term = 1.0
        var sum = 1.0
        var n = 1
        while (n < 20) {
            term = Double(((term * x) / Double(to_float(n))))
            sum = Double((sum + term))
            n = _int((n &+ 1))
        }
        return sum
    }
    func mean(_ v: [Double]) -> Double {
        var total = 0.0
        var i = 0
        while (i < _int(((v).count))) {
            total = Double((total + Double(_idx(v, i))))
            i = _int((i &+ 1))
        }
        return (total / Double(to_float(_int(((v).count)))))
    }
    func binary_cross_entropy(_ y_true: [Double], _ y_pred: [Double], _ epsilon: Double) -> Double {
        if (_int(((y_true).count)) != _int(((y_pred).count))) {
            _ = fatalError("Input arrays must have the same length.")
        }
        var losses: [Double] = ([] as! [Double])
        var i = 0
        while (i < _int(((y_true).count))) {
            let yt = Double(_idx(y_true, i))
            let yp = Double(clip(Double(_idx(y_pred, i)), Double(epsilon), Double((1.0 - epsilon))))
            let loss = -Double(((yt * Double(ln(Double(yp)))) + (Double((1.0 - yt)) * Double(ln(Double((1.0 - yp)))))))
            losses = (_append(losses, loss) as! [Double])
            i = _int((i &+ 1))
        }
        return Double(mean((losses as! [Double])))
    }
    func binary_focal_cross_entropy(_ y_true: [Double], _ y_pred: [Double], _ gamma: Double, _ alpha: Double, _ epsilon: Double) -> Double {
        if (_int(((y_true).count)) != _int(((y_pred).count))) {
            _ = fatalError("Input arrays must have the same length.")
        }
        var losses: [Double] = ([] as! [Double])
        var i = 0
        while (i < _int(((y_true).count))) {
            let yt = Double(_idx(y_true, i))
            let yp = Double(clip(Double(_idx(y_pred, i)), Double(epsilon), Double((1.0 - epsilon))))
            let term1 = (((alpha * Double(powf(Double((1.0 - yp)), Double(gamma)))) * yt) * Double(ln(Double(yp))))
            let term2 = (((Double((1.0 - alpha)) * Double(powf(Double(yp), Double(gamma)))) * Double((1.0 - yt))) * Double(ln(Double((1.0 - yp)))))
            losses = (_append(losses, -Double((term1 + term2))) as! [Double])
            i = _int((i &+ 1))
        }
        return Double(mean((losses as! [Double])))
    }
    func categorical_cross_entropy(_ y_true: [[Double]], _ y_pred: [[Double]], _ epsilon: Double) -> Double {
        if (_int(((y_true).count)) != _int(((y_pred).count))) {
            _ = fatalError("Input arrays must have the same shape.")
        }
        let rows = _int(((y_true).count))
        var total = 0.0
        var i = 0
        while (i < rows) {
            if (_int((((_idx(y_true, i) as! [Double])).count)) != _int((((_idx(y_pred, i) as! [Double])).count))) {
                _ = fatalError("Input arrays must have the same shape.")
            }
            var sum_true = 0.0
            var sum_pred = 0.0
            var j = 0
            while (j < _int((((_idx(y_true, i) as! [Double])).count))) {
                let yt = Double(_idx(_idx(y_true, i)!, j))
                let yp = Double(_idx(_idx(y_pred, i)!, j))
                if ((yt != 0.0) && (yt != 1.0)) {
                    _ = fatalError("y_true must be one-hot encoded.")
                }
                sum_true = Double((sum_true + yt))
                sum_pred = Double((sum_pred + yp))
                j = _int((j &+ 1))
            }
            if (sum_true != 1.0) {
                _ = fatalError("y_true must be one-hot encoded.")
            }
            if (Double(absf(Double((sum_pred - 1.0)))) > epsilon) {
                _ = fatalError("Predicted probabilities must sum to approximately 1.")
            }
            j = 0
            while (j < _int((((_idx(y_true, i) as! [Double])).count))) {
                let yp = Double(clip(Double(_idx(_idx(y_pred, i)!, j)), Double(epsilon), 1.0))
                total = Double((total - Double((Double(_idx(_idx(y_true, i)!, j)) * Double(ln(Double(yp)))))))
                j = _int((j &+ 1))
            }
            i = _int((i &+ 1))
        }
        return total
    }
    func categorical_focal_cross_entropy(_ y_true: [[Double]], _ y_pred: [[Double]], _ alpha: [Double], _ gamma: Double, _ epsilon: Double) -> Double {
        if (_int(((y_true).count)) != _int(((y_pred).count))) {
            _ = fatalError("Shape of y_true and y_pred must be the same.")
        }
        let rows = _int(((y_true).count))
        let cols = _int((((_idx(y_true, 0) as! [Double])).count))
        var a = alpha
        if (_int(((a).count)) == 0) {
            var tmp: [Double] = ([] as! [Double])
            var j = 0
            while (j < cols) {
                tmp = (_append(tmp, 1.0) as! [Double])
                j = _int((j &+ 1))
            }
            a = (tmp as! [Double])
        }
        if (_int(((a).count)) != cols) {
            _ = fatalError("Length of alpha must match the number of classes.")
        }
        var total = 0.0
        var i = 0
        while (i < rows) {
            if ((_int((((_idx(y_true, i) as! [Double])).count)) != cols) || (_int((((_idx(y_pred, i) as! [Double])).count)) != cols)) {
                _ = fatalError("Shape of y_true and y_pred must be the same.")
            }
            var sum_true = 0.0
            var sum_pred = 0.0
            var j = 0
            while (j < cols) {
                let yt = Double(_idx(_idx(y_true, i)!, j))
                let yp = Double(_idx(_idx(y_pred, i)!, j))
                if ((yt != 0.0) && (yt != 1.0)) {
                    _ = fatalError("y_true must be one-hot encoded.")
                }
                sum_true = Double((sum_true + yt))
                sum_pred = Double((sum_pred + yp))
                j = _int((j &+ 1))
            }
            if (sum_true != 1.0) {
                _ = fatalError("y_true must be one-hot encoded.")
            }
            if (Double(absf(Double((sum_pred - 1.0)))) > epsilon) {
                _ = fatalError("Predicted probabilities must sum to approximately 1.")
            }
            var row_loss = 0.0
            j = 0
            while (j < cols) {
                let yp = Double(clip(Double(_idx(_idx(y_pred, i)!, j)), Double(epsilon), 1.0))
                row_loss = Double((row_loss + (((Double(_idx(a, j)) * Double(powf(Double((1.0 - yp)), Double(gamma)))) * Double(_idx(_idx(y_true, i)!, j))) * Double(ln(Double(yp))))))
                j = _int((j &+ 1))
            }
            total = Double((total - row_loss))
            i = _int((i &+ 1))
        }
        return (total / Double(to_float(rows)))
    }
    func hinge_loss(_ y_true: [Double], _ y_pred: [Double]) -> Double {
        if (_int(((y_true).count)) != _int(((y_pred).count))) {
            _ = fatalError("Length of predicted and actual array must be same.")
        }
        var losses: [Double] = ([] as! [Double])
        var i = 0
        while (i < _int(((y_true).count))) {
            let yt = Double(_idx(y_true, i))
            if ((yt != Double(-1.0)) && (yt != 1.0)) {
                _ = fatalError("y_true can have values -1 or 1 only.")
            }
            let pred = Double(_idx(y_pred, i))
            let l = Double(maxf(0.0, Double((1.0 - (yt * pred)))))
            losses = (_append(losses, l) as! [Double])
            i = _int((i &+ 1))
        }
        return Double(mean((losses as! [Double])))
    }
    func huber_loss(_ y_true: [Double], _ y_pred: [Double], _ delta: Double) -> Double {
        if (_int(((y_true).count)) != _int(((y_pred).count))) {
            _ = fatalError("Input arrays must have the same length.")
        }
        var total = 0.0
        var i = 0
        while (i < _int(((y_true).count))) {
            let diff = (Double(_idx(y_true, i)) - Double(_idx(y_pred, i)))
            let adiff = Double(absf(Double(diff)))
            if (adiff <= delta) {
                total = Double((total + ((0.5 * diff) * diff)))
            } else {
                total = Double((total + (delta * Double((adiff - (0.5 * delta))))))
            }
            i = _int((i &+ 1))
        }
        return (total / Double(to_float(_int(((y_true).count)))))
    }
    func mean_squared_error(_ y_true: [Double], _ y_pred: [Double]) -> Double {
        if (_int(((y_true).count)) != _int(((y_pred).count))) {
            _ = fatalError("Input arrays must have the same length.")
        }
        var losses: [Double] = ([] as! [Double])
        var i = 0
        while (i < _int(((y_true).count))) {
            let diff = (Double(_idx(y_true, i)) - Double(_idx(y_pred, i)))
            losses = (_append(losses, (diff * diff)) as! [Double])
            i = _int((i &+ 1))
        }
        return Double(mean((losses as! [Double])))
    }
    func mean_absolute_error(_ y_true: [Double], _ y_pred: [Double]) -> Double {
        if (_int(((y_true).count)) != _int(((y_pred).count))) {
            _ = fatalError("Input arrays must have the same length.")
        }
        var total = 0.0
        var i = 0
        while (i < _int(((y_true).count))) {
            total = Double((total + Double(absf(Double((Double(_idx(y_true, i)) - Double(_idx(y_pred, i))))))))
            i = _int((i &+ 1))
        }
        return (total / Double(to_float(_int(((y_true).count)))))
    }
    func mean_squared_logarithmic_error(_ y_true: [Double], _ y_pred: [Double]) -> Double {
        if (_int(((y_true).count)) != _int(((y_pred).count))) {
            _ = fatalError("Input arrays must have the same length.")
        }
        var total = 0.0
        var i = 0
        while (i < _int(((y_true).count))) {
            let a = Double(ln(Double((1.0 + Double(_idx(y_true, i))))))
            let b = Double(ln(Double((1.0 + Double(_idx(y_pred, i))))))
            let diff = (a - b)
            total = Double((total + (diff * diff)))
            i = _int((i &+ 1))
        }
        return (total / Double(to_float(_int(((y_true).count)))))
    }
    func mean_absolute_percentage_error(_ y_true: [Double], _ y_pred: [Double], _ epsilon: Double) -> Double {
        if (_int(((y_true).count)) != _int(((y_pred).count))) {
            _ = fatalError("The length of the two arrays should be the same.")
        }
        var total = 0.0
        var i = 0
        while (i < _int(((y_true).count))) {
            var yt = Double(_idx(y_true, i))
            if (yt == 0.0) {
                yt = Double(epsilon)
            }
            total = Double((total + Double(absf(Double((Double((yt - Double(_idx(y_pred, i)))) / yt))))))
            i = _int((i &+ 1))
        }
        return (total / Double(to_float(_int(((y_true).count)))))
    }
    func perplexity_loss(_ y_true: [[Int]], _ y_pred: [[[Double]]], _ epsilon: Double) -> Double {
        let batch = _int(((y_true).count))
        if (batch != _int(((y_pred).count))) {
            _ = fatalError("Batch size of y_true and y_pred must be equal.")
        }
        let sentence_len = _int((((_idx(y_true, 0) as! [Int])).count))
        if (sentence_len != _int((((_idx(y_pred, 0) as! [[Double]])).count))) {
            _ = fatalError("Sentence length of y_true and y_pred must be equal.")
        }
        let vocab_size = _int((((_idx(_idx(y_pred, 0)!, 0) as! [Double])).count))
        var b = 0
        var total_perp = 0.0
        while (b < batch) {
            if ((_int((((_idx(y_true, b) as! [Int])).count)) != sentence_len) || (_int((((_idx(y_pred, b) as! [[Double]])).count)) != sentence_len)) {
                _ = fatalError("Sentence length of y_true and y_pred must be equal.")
            }
            var sum_log = 0.0
            var j = 0
            while (j < sentence_len) {
                let label = (_idx(_idx(y_true, b)!, j) as? Int ?? 0)
                if (label >= vocab_size) {
                    _ = fatalError("Label value must not be greater than vocabulary size.")
                }
                let prob = Double(clip(Double(_idx(_idx(_idx(y_pred, b)!, j)!, label)), Double(epsilon), 1.0))
                sum_log = Double((sum_log + Double(ln(Double(prob)))))
                j = _int((j &+ 1))
            }
            let mean_log = (sum_log / Double(to_float(sentence_len)))
            let perp = Double(exp(Double(-mean_log)))
            total_perp = Double((total_perp + perp))
            b = _int((b &+ 1))
        }
        return (total_perp / Double(to_float(batch)))
    }
    func smooth_l1_loss(_ y_true: [Double], _ y_pred: [Double], _ beta: Double) -> Double {
        if (_int(((y_true).count)) != _int(((y_pred).count))) {
            _ = fatalError("The length of the two arrays should be the same.")
        }
        var total = 0.0
        var i = 0
        while (i < _int(((y_true).count))) {
            let diff = Double(absf(Double((Double(_idx(y_true, i)) - Double(_idx(y_pred, i))))))
            if (diff < beta) {
                total = Double((total + (((0.5 * diff) * diff) / beta)))
            } else {
                total = Double(((total + diff) - (0.5 * beta)))
            }
            i = _int((i &+ 1))
        }
        return (total / Double(to_float(_int(((y_true).count)))))
    }
    func kullback_leibler_divergence(_ y_true: [Double], _ y_pred: [Double]) -> Double {
        if (_int(((y_true).count)) != _int(((y_pred).count))) {
            _ = fatalError("Input arrays must have the same length.")
        }
        var total = 0.0
        var i = 0
        while (i < _int(((y_true).count))) {
            total = Double((total + (Double(_idx(y_true, i)) * Double(ln(Double((Double(_idx(y_true, i)) / Double(_idx(y_pred, i)))))))))
            i = _int((i &+ 1))
        }
        return total
    }
    func main() -> Void {
        let y_true_bc: [Double] = ({
            var _arr: [Double] = []
            _arr = _append(_arr, 0.0)
            _arr = _append(_arr, 1.0)
            _arr = _append(_arr, 1.0)
            _arr = _append(_arr, 0.0)
            _arr = _append(_arr, 1.0)
            return _arr
        }() as! [Double])
        let y_pred_bc: [Double] = ({
            var _arr: [Double] = []
            _arr = _append(_arr, 0.2)
            _arr = _append(_arr, 0.7)
            _arr = _append(_arr, 0.9)
            _arr = _append(_arr, 0.3)
            _arr = _append(_arr, 0.8)
            return _arr
        }() as! [Double])
        print(_p(Double(binary_cross_entropy((y_true_bc as! [Double]), (y_pred_bc as! [Double]), 0.000000000000001))))
        print(_p(Double(binary_focal_cross_entropy((y_true_bc as! [Double]), (y_pred_bc as! [Double]), 2.0, 0.25, 0.000000000000001))))
        let y_true_cce: [[Double]] = ({
            var _arr: [[Double]] = []
            _arr = _append(_arr, ({
                var _arr: [Double] = []
                _arr = _append(_arr, 1.0)
                _arr = _append(_arr, 0.0)
                _arr = _append(_arr, 0.0)
                return _arr
            }() as! [Double]))
            _arr = _append(_arr, ({
                var _arr: [Double] = []
                _arr = _append(_arr, 0.0)
                _arr = _append(_arr, 1.0)
                _arr = _append(_arr, 0.0)
                return _arr
            }() as! [Double]))
            _arr = _append(_arr, ({
                var _arr: [Double] = []
                _arr = _append(_arr, 0.0)
                _arr = _append(_arr, 0.0)
                _arr = _append(_arr, 1.0)
                return _arr
            }() as! [Double]))
            return _arr
        }() as! [[Double]])
        let y_pred_cce: [[Double]] = ({
            var _arr: [[Double]] = []
            _arr = _append(_arr, ({
                var _arr: [Double] = []
                _arr = _append(_arr, 0.9)
                _arr = _append(_arr, 0.1)
                _arr = _append(_arr, 0.0)
                return _arr
            }() as! [Double]))
            _arr = _append(_arr, ({
                var _arr: [Double] = []
                _arr = _append(_arr, 0.2)
                _arr = _append(_arr, 0.7)
                _arr = _append(_arr, 0.1)
                return _arr
            }() as! [Double]))
            _arr = _append(_arr, ({
                var _arr: [Double] = []
                _arr = _append(_arr, 0.0)
                _arr = _append(_arr, 0.1)
                _arr = _append(_arr, 0.9)
                return _arr
            }() as! [Double]))
            return _arr
        }() as! [[Double]])
        print(_p(Double(categorical_cross_entropy((y_true_cce as! [[Double]]), (y_pred_cce as! [[Double]]), 0.000000000000001))))
        let alpha: [Double] = ({
            var _arr: [Double] = []
            _arr = _append(_arr, 0.6)
            _arr = _append(_arr, 0.2)
            _arr = _append(_arr, 0.7)
            return _arr
        }() as! [Double])
        print(_p(Double(categorical_focal_cross_entropy((y_true_cce as! [[Double]]), (y_pred_cce as! [[Double]]), (alpha as! [Double]), 2.0, 0.000000000000001))))
        let y_true_hinge: [Double] = ({
            var _arr: [Double] = []
            _arr = _append(_arr, -1.0)
            _arr = _append(_arr, 1.0)
            _arr = _append(_arr, 1.0)
            _arr = _append(_arr, -1.0)
            _arr = _append(_arr, 1.0)
            return _arr
        }() as! [Double])
        let y_pred_hinge: [Double] = ({
            var _arr: [Double] = []
            _arr = _append(_arr, -4.0)
            _arr = _append(_arr, -0.3)
            _arr = _append(_arr, 0.7)
            _arr = _append(_arr, 5.0)
            _arr = _append(_arr, 10.0)
            return _arr
        }() as! [Double])
        print(_p(Double(hinge_loss((y_true_hinge as! [Double]), (y_pred_hinge as! [Double])))))
        let y_true_huber: [Double] = ({
            var _arr: [Double] = []
            _arr = _append(_arr, 0.9)
            _arr = _append(_arr, 10.0)
            _arr = _append(_arr, 2.0)
            _arr = _append(_arr, 1.0)
            _arr = _append(_arr, 5.2)
            return _arr
        }() as! [Double])
        let y_pred_huber: [Double] = ({
            var _arr: [Double] = []
            _arr = _append(_arr, 0.8)
            _arr = _append(_arr, 2.1)
            _arr = _append(_arr, 2.9)
            _arr = _append(_arr, 4.2)
            _arr = _append(_arr, 5.2)
            return _arr
        }() as! [Double])
        print(_p(Double(huber_loss((y_true_huber as! [Double]), (y_pred_huber as! [Double]), 1.0))))
        print(_p(Double(mean_squared_error((y_true_huber as! [Double]), (y_pred_huber as! [Double])))))
        print(_p(Double(mean_absolute_error((y_true_huber as! [Double]), (y_pred_huber as! [Double])))))
        print(_p(Double(mean_squared_logarithmic_error((y_true_huber as! [Double]), (y_pred_huber as! [Double])))))
        let y_true_mape: [Double] = ({
            var _arr: [Double] = []
            _arr = _append(_arr, 10.0)
            _arr = _append(_arr, 20.0)
            _arr = _append(_arr, 30.0)
            _arr = _append(_arr, 40.0)
            return _arr
        }() as! [Double])
        let y_pred_mape: [Double] = ({
            var _arr: [Double] = []
            _arr = _append(_arr, 12.0)
            _arr = _append(_arr, 18.0)
            _arr = _append(_arr, 33.0)
            _arr = _append(_arr, 45.0)
            return _arr
        }() as! [Double])
        print(_p(Double(mean_absolute_percentage_error((y_true_mape as! [Double]), (y_pred_mape as! [Double]), 0.000000000000001))))
        let y_true_perp: [[Int]] = ({
            var _arr: [[Int]] = []
            _arr = _append(_arr, ({
                var _arr: [Int] = []
                _arr = _append(_arr, 1)
                _arr = _append(_arr, 4)
                return _arr
            }() as! [Int]))
            _arr = _append(_arr, ({
                var _arr: [Int] = []
                _arr = _append(_arr, 2)
                _arr = _append(_arr, 3)
                return _arr
            }() as! [Int]))
            return _arr
        }() as! [[Int]])
        let y_pred_perp: [[[Double]]] = ({
            var _arr: [[[Double]]] = []
            _arr = _append(_arr, ({
                var _arr: [[Double]] = []
                _arr = _append(_arr, ({
                    var _arr: [Double] = []
                    _arr = _append(_arr, 0.28)
                    _arr = _append(_arr, 0.19)
                    _arr = _append(_arr, 0.21)
                    _arr = _append(_arr, 0.15)
                    _arr = _append(_arr, 0.17)
                    return _arr
                }() as! [Double]))
                _arr = _append(_arr, ({
                    var _arr: [Double] = []
                    _arr = _append(_arr, 0.24)
                    _arr = _append(_arr, 0.19)
                    _arr = _append(_arr, 0.09)
                    _arr = _append(_arr, 0.18)
                    _arr = _append(_arr, 0.3)
                    return _arr
                }() as! [Double]))
                return _arr
            }() as! [[Double]]))
            _arr = _append(_arr, ({
                var _arr: [[Double]] = []
                _arr = _append(_arr, ({
                    var _arr: [Double] = []
                    _arr = _append(_arr, 0.03)
                    _arr = _append(_arr, 0.26)
                    _arr = _append(_arr, 0.21)
                    _arr = _append(_arr, 0.18)
                    _arr = _append(_arr, 0.32)
                    return _arr
                }() as! [Double]))
                _arr = _append(_arr, ({
                    var _arr: [Double] = []
                    _arr = _append(_arr, 0.28)
                    _arr = _append(_arr, 0.1)
                    _arr = _append(_arr, 0.33)
                    _arr = _append(_arr, 0.15)
                    _arr = _append(_arr, 0.14)
                    return _arr
                }() as! [Double]))
                return _arr
            }() as! [[Double]]))
            return _arr
        }() as! [[[Double]]])
        print(_p(Double(perplexity_loss((y_true_perp as! [[Int]]), (y_pred_perp as! [[[Double]]]), 0.0000001))))
        let y_true_smooth: [Double] = ({
            var _arr: [Double] = []
            _arr = _append(_arr, 3.0)
            _arr = _append(_arr, 5.0)
            _arr = _append(_arr, 2.0)
            _arr = _append(_arr, 7.0)
            return _arr
        }() as! [Double])
        let y_pred_smooth: [Double] = ({
            var _arr: [Double] = []
            _arr = _append(_arr, 2.9)
            _arr = _append(_arr, 4.8)
            _arr = _append(_arr, 2.1)
            _arr = _append(_arr, 7.2)
            return _arr
        }() as! [Double])
        print(_p(Double(smooth_l1_loss((y_true_smooth as! [Double]), (y_pred_smooth as! [Double]), 1.0))))
        let y_true_kl: [Double] = ({
            var _arr: [Double] = []
            _arr = _append(_arr, 0.2)
            _arr = _append(_arr, 0.3)
            _arr = _append(_arr, 0.5)
            return _arr
        }() as! [Double])
        let y_pred_kl: [Double] = ({
            var _arr: [Double] = []
            _arr = _append(_arr, 0.3)
            _arr = _append(_arr, 0.3)
            _arr = _append(_arr, 0.4)
            return _arr
        }() as! [Double])
        print(_p(Double(kullback_leibler_divergence((y_true_kl as! [Double]), (y_pred_kl as! [Double])))))
    }
    _ = main()
    let _benchEnd = _now()
    let _benchMemEnd = _mem()
    print("{\n  \"duration_us\": \((_benchEnd - _benchStart) / 1000),\n  \"memory_bytes\": \(_benchMemEnd - _benchMemStart),\n  \"name\": \"main\"\n}")
}
