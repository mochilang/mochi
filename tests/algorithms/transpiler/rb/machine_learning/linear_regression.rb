# Generated by Mochi transpiler v0.10.59 on 2025-08-06 22:14 +0700
require 'json'

$now_seed = 0
$now_seeded = false
s = ENV['MOCHI_NOW_SEED']
if (!s || s == '') && ENV['MOCHI_BENCHMARK']
  s = '1'
end
if s && s != ''
  begin
    $now_seed = Integer(s)
    $now_seeded = true
  rescue StandardError
  end
end
def _now()
  if $now_seeded
    $now_seed += 1_000_000
    $now_seed
  else
    Process.clock_gettime(Process::CLOCK_MONOTONIC, :nanosecond)
  end
end


require 'objspace'
def _mem()
  ObjectSpace.memsize_of_all
end


def _add(a, b)
  if a.is_a?(Array) && b.is_a?(String)
    a.join + b
  elsif a.is_a?(String) && b.is_a?(Array)
    a + b.join
  else
    a + b
  end
end


def _padStart(s, len, ch)
  s.to_s.rjust(len, ch)
end


def _str(x)
  if x.is_a?(Float) && x == x.to_i
    x.to_i.to_s
  else
    x.to_s
  end
end


class String
  alias each each_char
end

start_mem = _mem()
start = _now()
  def dot(x, y)
    sum = 0.clone
    i = 0.clone
    while i < x.length
      sum = _add(sum, x[i] * y[i]).clone
      i = _add(i, 1).clone
    end
    return sum
  end
  def run_steep_gradient_descent(data_x, data_y, len_data, alpha, theta)
    gradients = [].clone
    j = 0.clone
    while j < theta.length
      gradients = (gradients << (0)).clone
      j = _add(j, 1).clone
    end
    i = 0.clone
    while i < len_data
      prediction = dot(theta, data_x[i])
      error = prediction - data_y[i]
      k = 0.clone
      while k < theta.length
        gradients[k] = _add(gradients[k], error * data_x[i][k])
        k = _add(k, 1).clone
      end
      i = _add(i, 1).clone
    end
    t = [].clone
    g = 0.clone
    while g < theta.length
      t = (t << (theta[g] - (alpha / len_data) * gradients[g])).clone
      g = _add(g, 1).clone
    end
    return t
  end
  def sum_of_square_error(data_x, data_y, len_data, theta)
    total = 0.clone
    i = 0.clone
    while i < len_data
      prediction = dot(theta, data_x[i])
      diff = prediction - data_y[i]
      total = _add(total, diff * diff).clone
      i = _add(i, 1).clone
    end
    return total / (2 * len_data)
  end
  def run_linear_regression(data_x, data_y)
    iterations = 10
    alpha = 0.01
    no_features = data_x[0].length
    len_data = data_x.length
    theta = [].clone
    i = 0.clone
    while i < no_features
      theta = (theta << (0)).clone
      i = _add(i, 1).clone
    end
    iter = 0.clone
    while iter < iterations
      theta = run_steep_gradient_descent(data_x, data_y, len_data, alpha, theta).clone
      error = sum_of_square_error(data_x, data_y, len_data, theta)
      puts(_add(_add(_add("At Iteration ", _str(_add(iter, 1))), " - Error is "), _str(error)))
      iter = _add(iter, 1).clone
    end
    return theta
  end
  def absf(x)
    if x < 0
      return -x
    else
      return x
    end
  end
  def mean_absolute_error(predicted_y, original_y)
    total = 0.clone
    i = 0.clone
    while i < predicted_y.length
      diff = absf(predicted_y[i] - original_y[i])
      total = _add(total, diff).clone
      i = _add(i, 1).clone
    end
    return total / predicted_y.length
  end
  $data_x = [[1, 1], [1, 2], [1, 3]]
  $data_y = [1, 2, 3]
  $theta = run_linear_regression($data_x, $data_y)
  puts("Resultant Feature vector :")
  $i = 0.clone
  while $i < $theta.length
    puts(_str($theta[$i]))
    $i = _add($i, 1).clone
  end
  $predicted_y = [3, -0.5, 2, 7]
  $original_y = [2.5, 0, 2, 8]
  $mae = mean_absolute_error($predicted_y, $original_y)
  puts(_add("Mean Absolute Error : ", _str($mae)))
end_time = _now()
end_mem = _mem()
result = {"duration_us" => ((end_time - start) / 1000), "memory_bytes" => (end_mem - start_mem), "name" => "main"}
puts(JSON.pretty_generate(result))
