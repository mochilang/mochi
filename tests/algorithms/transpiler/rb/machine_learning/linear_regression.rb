# Generated by Mochi transpiler v0.10.63 on 2025-08-11 18:28 +0700
$VERBOSE = nil
require 'json'

$now_seed = 0
$now_seeded = false
s = ENV['MOCHI_NOW_SEED']
if s && s != ''
  begin
    $now_seed = Integer(s)
    $now_seeded = true
  rescue StandardError
  end
end
if !$now_seeded && ENV['MOCHI_BENCHMARK']
  $now_seeded = true
end
def _now()
  if $now_seeded
    $now_seed = ($now_seed * 1_664_525 + 1_013_904_223) % 2_147_483_647
    $now_seed
  else
    Process.clock_gettime(Process::CLOCK_MONOTONIC, :nanosecond)
  end
end


require 'objspace'
def _mem()
  ObjectSpace.memsize_of_all
end


def _add(a, b)
  if a.is_a?(Array) && b.is_a?(String)
    a.join + b
  elsif a.is_a?(String) && b.is_a?(Array)
    a + b.join
  elsif a.is_a?(Array) && !b.is_a?(Array)
    a + [b]
  elsif !a.is_a?(Array) && b.is_a?(Array)
    [a] + b
  elsif a.is_a?(String) || b.is_a?(String)
    a.to_s + b.to_s
  else
    a + b
  end
end


def _eq(a, b)
  if a.is_a?(Float) || b.is_a?(Float)
    (a.to_f - b.to_f).abs < 1e-6
  else
    a == b
  end
end


def _padStart(s, len, ch)
  s.to_s.rjust(len, ch)
end


def _padEnd(s, len, ch)
  s.to_s.ljust(len, ch)
end


def _str(x)
  if x.is_a?(Array)
    x.map { |e| _str(e) }.join(' ')
  elsif x.is_a?(Float) && x == x.to_i
    x.to_i.to_s
  else
    x.to_s
  end
end


class String
  alias each each_char
end


def panic(msg)
  raise RuntimeError, msg
end

__name__ = '__main__'
start_mem = _mem()
start = _now()
  def dot(x, y)
    sum = 0.0
    i = 0
    while i < x.length
      sum = _add(sum, x[i] * y[i])
      i = _add(i, 1)
    end
    return sum
  end
  def run_steep_gradient_descent(data_x, data_y, len_data, alpha, theta)
    gradients = []
    j = 0
    while j < theta.length
      gradients = (gradients + [0.0])
      j = _add(j, 1)
    end
    i = 0
    while i < len_data
      prediction = dot(theta, data_x[i])
      error = prediction - data_y[i]
      k = 0
      while k < theta.length
        gradients[k] = _add(gradients[k], error * data_x[i][k])
        k = _add(k, 1)
      end
      i = _add(i, 1)
    end
    t = []
    g = 0
    while g < theta.length
      t = (t + [theta[g] - (alpha / len_data) * gradients[g]])
      g = _add(g, 1)
    end
    return t
  end
  def sum_of_square_error(data_x, data_y, len_data, theta)
    total = 0.0
    i = 0
    while i < len_data
      prediction = dot(theta, data_x[i])
      diff = prediction - data_y[i]
      total = _add(total, diff * diff)
      i = _add(i, 1)
    end
    return total / (2.0 * len_data)
  end
  def run_linear_regression(data_x, data_y)
    iterations = 10
    alpha = 0.01
    no_features = data_x[0].length
    len_data = data_x.length
    theta = []
    i = 0
    while i < no_features
      theta = (theta + [0.0])
      i = _add(i, 1)
    end
    iter = 0
    while iter < iterations
      theta = run_steep_gradient_descent(data_x, data_y, len_data, alpha, theta)
      error = sum_of_square_error(data_x, data_y, len_data, theta)
      puts(_add(_add(_add("At Iteration ", _str(_add(iter, 1))), " - Error is "), _str(error)))
      iter = _add(iter, 1)
    end
    return theta
  end
  def absf(x)
    if x < 0.0
      return -x
    else
      return x
    end
  end
  def mean_absolute_error(predicted_y, original_y)
    total = 0.0
    i = 0
    while i < predicted_y.length
      diff = absf(predicted_y[i] - original_y[i])
      total = _add(total, diff)
      i = _add(i, 1)
    end
    return total / predicted_y.length
  end
  $data_x = [[1.0, 1.0], [1.0, 2.0], [1.0, 3.0]]
  $data_y = [1.0, 2.0, 3.0]
  $theta = run_linear_regression($data_x, $data_y)
  puts("Resultant Feature vector :")
  $i = 0
  while $i < $theta.length
    puts(_str($theta[$i]))
    $i = _add($i, 1)
  end
  $predicted_y = [3.0, -0.5, 2.0, 7.0]
  $original_y = [2.5, 0.0, 2.0, 8.0]
  $mae = mean_absolute_error($predicted_y, $original_y)
  puts(_add("Mean Absolute Error : ", _str($mae)))
end_time = _now()
end_mem = _mem()
result = {"duration_us" => ((end_time - start) / 1000), "memory_bytes" => (end_mem - start_mem), "name" => "main"}
puts(JSON.pretty_generate(result))
