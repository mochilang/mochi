# Generated by Mochi transpiler v0.10.59 on 2025-08-06 22:14 +0700
require 'json'

$now_seed = 0
$now_seeded = false
s = ENV['MOCHI_NOW_SEED']
if (!s || s == '') && ENV['MOCHI_BENCHMARK']
  s = '1'
end
if s && s != ''
  begin
    $now_seed = Integer(s)
    $now_seeded = true
  rescue StandardError
  end
end
def _now()
  if $now_seeded
    $now_seed += 1_000_000
    $now_seed
  else
    Process.clock_gettime(Process::CLOCK_MONOTONIC, :nanosecond)
  end
end


require 'objspace'
def _mem()
  ObjectSpace.memsize_of_all
end


def _add(a, b)
  if a.is_a?(Array) && b.is_a?(String)
    a.join + b
  elsif a.is_a?(String) && b.is_a?(Array)
    a + b.join
  else
    a + b
  end
end


def _padStart(s, len, ch)
  s.to_s.rjust(len, ch)
end


def _str(x)
  if x.is_a?(Float) && x == x.to_i
    x.to_i.to_s
  else
    x.to_s
  end
end


class String
  alias each each_char
end

start_mem = _mem()
start = _now()
  def absf(x)
    if x < 0.0
      return -x
    end
    return x
  end
  def hypothesis_value(input, params)
    value = params[0].clone
    i = 0.clone
    while i < input.length
      value = _add(value, input[i] * params[_add(i, 1)]).clone
      i = _add(i, 1).clone
    end
    return value
  end
  def calc_error(dp, params)
    return hypothesis_value(dp.x, params) - dp.y
  end
  def summation_of_cost_derivative(index, params, data)
    sum = 0.0.clone
    i = 0.clone
    while i < data.length
      dp = data[i]
      e = calc_error(dp, params)
      if index == (-1)
        sum = _add(sum, e).clone
      else
        sum = _add(sum, e * dp.x[index]).clone
      end
      i = _add(i, 1).clone
    end
    return sum
  end
  def get_cost_derivative(index, params, data)
    return summation_of_cost_derivative(index, params, data) / ((data.length).to_f)
  end
  def allclose(a, b, atol, rtol)
    i = 0.clone
    while i < a.length
      diff = absf(a[i] - b[i])
      limit = _add(atol, rtol * absf(b[i]))
      if diff > limit
        return false
      end
      i = _add(i, 1).clone
    end
    return true
  end
  def run_gradient_descent(train_data, initial_params)
    learning_rate = 0.009
    absolute_error_limit = 2e-06
    relative_error_limit = 0.0
    j = 0.clone
    params = initial_params.clone
    while true
      j = _add(j, 1).clone
      temp = [].clone
      i = 0.clone
      while i < params.length
        deriv = get_cost_derivative(i - 1, params, train_data)
        temp = (temp << (params[i] - learning_rate * deriv)).clone
        i = _add(i, 1).clone
      end
      if allclose(params, temp, absolute_error_limit, relative_error_limit)
        puts(_add("Number of iterations:", _str(j)))
        break
      end
      params = temp.clone
    end
    return params
  end
  def test_gradient_descent(test_data, params)
    i = 0.clone
    while i < test_data.length
      dp = test_data[i]
      puts(_add("Actual output value:", _str(dp.y)))
      puts(_add("Hypothesis output:", _str(hypothesis_value(dp.x, params))))
      i = _add(i, 1).clone
    end
  end
  DataPoint = Struct.new(:x, :y, keyword_init: true)
  $train_data = [DataPoint.new(x: [5.0, 2.0, 3.0], y: 15.0), DataPoint.new(x: [6.0, 5.0, 9.0], y: 25.0), DataPoint.new(x: [11.0, 12.0, 13.0], y: 41.0), DataPoint.new(x: [1.0, 1.0, 1.0], y: 8.0), DataPoint.new(x: [11.0, 12.0, 13.0], y: 41.0)]
  $test_data = [DataPoint.new(x: [515.0, 22.0, 13.0], y: 555.0), DataPoint.new(x: [61.0, 35.0, 49.0], y: 150.0)]
  $parameter_vector = [2.0, 4.0, 1.0, 5.0].clone
  $parameter_vector = run_gradient_descent($train_data, $parameter_vector).clone
  puts("\nTesting gradient descent for a linear hypothesis function.\n")
  test_gradient_descent($test_data, $parameter_vector)
end_time = _now()
end_mem = _mem()
result = {"duration_us" => ((end_time - start) / 1000), "memory_bytes" => (end_mem - start_mem), "name" => "main"}
puts(JSON.pretty_generate(result))
