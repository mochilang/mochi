# Generated by Mochi transpiler v0.10.67 on 2025-08-16 19:42 +0700
$VERBOSE = nil
require 'json'

$now_seed = 0
$now_seeded = false
s = ENV['MOCHI_NOW_SEED']
if s && s != ''
  begin
    $now_seed = Integer(s)
    $now_seeded = true
  rescue StandardError
  end
end
if !$now_seeded && ENV['MOCHI_BENCHMARK']
  $now_seeded = true
end
def _now()
  if $now_seeded
    $now_seed = ($now_seed * 1_664_525 + 1_013_904_223) % 2_147_483_647
    $now_seed
  else
    Process.clock_gettime(Process::CLOCK_MONOTONIC, :nanosecond)
  end
end


require 'objspace'
def _mem()
  ObjectSpace.memsize_of_all
end


def _idx(arr, idx)
  return nil if arr.nil?
  if arr.is_a?(Array) && idx.is_a?(Numeric)
    return nil if idx < 0 || idx >= arr.length
  end
  arr[idx]
end


def _pow(a, b)
  res = (a.nil? ? 0 : a) ** (b.nil? ? 0 : b)
  res.is_a?(Float) && res == res.to_i ? res.to_i : res
end


def _len(x)
  x.respond_to?(:length) ? x.length : 0
end


def _has(obj, key)
  if obj.is_a?(Hash)
    obj.key?(key)
  elsif obj.respond_to?(:include?)
    obj.include?(key)
  elsif obj.respond_to?(:to_h)
    k = key.respond_to?(:to_sym) ? key.to_sym : key
    obj.to_h.key?(k)
  else
    false
  end
end


def _add(a, b)
  if a.is_a?(Array) && b.is_a?(String)
    a.join + b
  elsif a.is_a?(String) && b.is_a?(Array)
    a + b.join
  elsif a.is_a?(Array) && !b.is_a?(Array)
    a + [b]
  elsif !a.is_a?(Array) && b.is_a?(Array)
    [a] + b
  elsif a.is_a?(String) || b.is_a?(String)
    a.to_s + b.to_s
  else
    (a.nil? ? 0 : a) + (b.nil? ? 0 : b)
  end
end


def _append(arr, x)
  x = x.clone if x.is_a?(Array)
  (arr || []) + [x]
end


def _eq(a, b)
  if a.is_a?(Float) || b.is_a?(Float)
    diff = (a.to_f - b.to_f).abs
    scale = [a.to_f.abs, b.to_f.abs].max
    scale = 1.0 if scale == 0.0
    diff <= 1e-6 * scale
  else
    a == b
  end
end


def _padStart(s, len, ch)
  s.to_s.rjust(len, ch)
end


def _padEnd(s, len, ch)
  s.to_s.ljust(len, ch)
end


def _str(x)
  if x.is_a?(Array)
    x.map { |e| _str(e) }.join(' ')
  elsif x.is_a?(Float)
    s = x.to_s
    if s.include?('e') || s.include?('E')
      s
    elsif x == x.to_i
      x.to_i.to_s
    else
      s
    end
  else
    x.to_s
  end
end


class String
  alias each each_char
end


def panic(msg)
  raise RuntimeError, msg
end

__name__ = '__main__'
start_mem = _mem()
start = Process.clock_gettime(Process::CLOCK_MONOTONIC, :nanosecond)
  def absf(x)
    if x < 0.0
      return -x
    end
    return x
  end
  def hypothesis_value(input, params)
    value = (__tmp1 = params; __tmp1.is_a?(Hash) ? __tmp1[0] : _idx(__tmp1, 0))
    i = 0
    while i < _len(input)
      value = _add(value, (__tmp2 = input; __tmp2.is_a?(Hash) ? __tmp2[i] : _idx(__tmp2, i)) * (__tmp3 = params; __tmp3.is_a?(Hash) ? __tmp3[_add(i, 1)] : _idx(__tmp3, _add(i, 1))))
      i = _add(i, 1)
    end
    return value
  end
  def calc_error(dp, params)
    return hypothesis_value(dp.x, params) - dp.y
  end
  def summation_of_cost_derivative(index, params, data)
    sum = 0.0
    i = 0
    while i < _len(data)
      dp = (__tmp4 = data; __tmp4.is_a?(Hash) ? __tmp4[i] : _idx(__tmp4, i))
      e = calc_error(dp, params)
      if _eq(index, (-1))
        sum = _add(sum, e)
      else
        sum = _add(sum, e * (__tmp5 = dp.x; __tmp5.is_a?(Hash) ? __tmp5[index] : _idx(__tmp5, index)))
      end
      i = _add(i, 1)
    end
    return sum
  end
  def get_cost_derivative(index, params, data)
    return summation_of_cost_derivative(index, params, data) / ((_len(data)).to_f)
  end
  def allclose(a, b, atol, rtol)
    i = 0
    while i < _len(a)
      diff = absf((__tmp6 = a; __tmp6.is_a?(Hash) ? __tmp6[i] : _idx(__tmp6, i)) - (__tmp7 = b; __tmp7.is_a?(Hash) ? __tmp7[i] : _idx(__tmp7, i)))
      limit = _add(atol, rtol * absf((__tmp8 = b; __tmp8.is_a?(Hash) ? __tmp8[i] : _idx(__tmp8, i))))
      if diff > limit
        return false
      end
      i = _add(i, 1)
    end
    return true
  end
  def run_gradient_descent(train_data, initial_params)
    learning_rate = 0.009
    absolute_error_limit = 2e-06
    relative_error_limit = 0.0
    j = 0
    params = initial_params
    while true
      j = _add(j, 1)
      temp = []
      i = 0
      while i < _len(params)
        deriv = get_cost_derivative(i - 1, params, train_data)
        temp = _append(temp, (__tmp9 = params; __tmp9.is_a?(Hash) ? __tmp9[i] : _idx(__tmp9, i)) - learning_rate * deriv)
        i = _add(i, 1)
      end
      if allclose(params, temp, absolute_error_limit, relative_error_limit)
        puts(_add("Number of iterations:", _str(j)))
        break
      end
      params = temp
    end
    return params
  end
  def test_gradient_descent(test_data, params)
    i = 0
    while i < _len(test_data)
      dp = (__tmp10 = test_data; __tmp10.is_a?(Hash) ? __tmp10[i] : _idx(__tmp10, i))
      puts(_add("Actual output value:", _str(dp.y)))
      puts(_add("Hypothesis output:", _str(hypothesis_value(dp.x, params))))
      i = _add(i, 1)
    end
  end
  Object.send(:remove_const, :DataPoint) if Object.const_defined?(:DataPoint)
  Object.const_set(:DataPoint, Struct.new(:x, :y, keyword_init: true))
  $train_data = [DataPoint.new(x: [5.0, 2.0, 3.0], y: 15.0), DataPoint.new(x: [6.0, 5.0, 9.0], y: 25.0), DataPoint.new(x: [11.0, 12.0, 13.0], y: 41.0), DataPoint.new(x: [1.0, 1.0, 1.0], y: 8.0), DataPoint.new(x: [11.0, 12.0, 13.0], y: 41.0)]
  $test_data = [DataPoint.new(x: [515.0, 22.0, 13.0], y: 555.0), DataPoint.new(x: [61.0, 35.0, 49.0], y: 150.0)]
  $parameter_vector = [2.0, 4.0, 1.0, 5.0]
  $parameter_vector = run_gradient_descent($train_data, $parameter_vector)
  puts("\nTesting gradient descent for a linear hypothesis function.\n")
  test_gradient_descent($test_data, $parameter_vector)
end_time = Process.clock_gettime(Process::CLOCK_MONOTONIC, :nanosecond)
end_mem = _mem()
result = {"duration_us" => ((end_time - start) / 1000), "memory_bytes" => (end_mem - start_mem), "name" => "main"}
puts(JSON.pretty_generate(result))
