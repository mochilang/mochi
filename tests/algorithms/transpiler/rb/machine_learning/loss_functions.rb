# Generated by Mochi transpiler v0.10.65 on 2025-08-14 16:48 +0700
$VERBOSE = nil
require 'json'

$now_seed = 0
$now_seeded = false
s = ENV['MOCHI_NOW_SEED']
if s && s != ''
  begin
    $now_seed = Integer(s)
    $now_seeded = true
  rescue StandardError
  end
end
if !$now_seeded && ENV['MOCHI_BENCHMARK']
  $now_seeded = true
end
def _now()
  if $now_seeded
    $now_seed = ($now_seed * 1_664_525 + 1_013_904_223) % 2_147_483_647
    $now_seed
  else
    Process.clock_gettime(Process::CLOCK_MONOTONIC, :nanosecond)
  end
end


require 'objspace'
def _mem()
  ObjectSpace.memsize_of_all
end


def _idx(arr, idx)
  return nil if arr.nil?
  if arr.is_a?(Array) && idx.is_a?(Numeric)
    return nil if idx < 0 || idx >= arr.length
  end
  arr[idx]
end


def _len(x)
  x.respond_to?(:length) ? x.length : 0
end


def _has(obj, key)
  if obj.is_a?(Hash)
    obj.key?(key)
  elsif obj.respond_to?(:include?)
    obj.include?(key)
  elsif obj.respond_to?(:to_h)
    k = key.respond_to?(:to_sym) ? key.to_sym : key
    obj.to_h.key?(k)
  else
    false
  end
end


def _add(a, b)
  if a.is_a?(Array) && b.is_a?(String)
    a.join + b
  elsif a.is_a?(String) && b.is_a?(Array)
    a + b.join
  elsif a.is_a?(Array) && !b.is_a?(Array)
    a + [b]
  elsif !a.is_a?(Array) && b.is_a?(Array)
    [a] + b
  elsif a.is_a?(String) || b.is_a?(String)
    a.to_s + b.to_s
  else
    (a.nil? ? 0 : a) + (b.nil? ? 0 : b)
  end
end


def _append(arr, x)
  x = x.clone if x.is_a?(Array)
  (arr || []) + [x]
end


def _eq(a, b)
  if a.is_a?(Float) || b.is_a?(Float)
    diff = (a.to_f - b.to_f).abs
    scale = [a.to_f.abs, b.to_f.abs].max
    scale = 1.0 if scale == 0.0
    diff <= 1e-6 * scale
  else
    a == b
  end
end


def _padStart(s, len, ch)
  s.to_s.rjust(len, ch)
end


def _padEnd(s, len, ch)
  s.to_s.ljust(len, ch)
end


def _str(x)
  if x.is_a?(Array)
    x.map { |e| _str(e) }.join(' ')
  elsif x.is_a?(Float)
    s = x.to_s
    if s.include?('e') || s.include?('E')
      s
    elsif x == x.to_i
      x.to_i.to_s
    else
      s
    end
  else
    x.to_s
  end
end


class String
  alias each each_char
end


def panic(msg)
  raise RuntimeError, msg
end

__name__ = '__main__'
start_mem = _mem()
start = Process.clock_gettime(Process::CLOCK_MONOTONIC, :nanosecond)
  def absf(x)
    if x < 0.0
      return -x
    end
    return x
  end
  def maxf(a, b)
    if a > b
      return a
    end
    return b
  end
  def minf(a, b)
    if a < b
      return a
    end
    return b
  end
  def clip(x, lo, hi)
    return maxf(lo, minf(x, hi))
  end
  def to_float(x)
    return x * 1.0
  end
  def powf(base, exp)
    result = 1.0
    i = 0
    n = (exp).to_i
    while i < n
      result = result * base
      i = _add(i, 1)
    end
    return result
  end
  def ln(x)
    if x <= 0.0
      panic("ln domain error")
    end
    y = (x - 1.0) / (_add(x, 1.0))
    y2 = y * y
    term = y
    sum = 0.0
    k = 0
    while k < 10
      denom = (_add(2 * k, 1)).to_f
      sum = _add(sum, term / denom)
      term = term * y2
      k = _add(k, 1)
    end
    return 2.0 * sum
  end
  def exp(x)
    term = 1.0
    sum = 1.0
    n = 1
    while n < 20
      term = term * x / (n).to_f
      sum = _add(sum, term)
      n = _add(n, 1)
    end
    return sum
  end
  def mean(v)
    total = 0.0
    i = 0
    while i < _len(v)
      total = _add(total, (__tmp1 = v; __tmp1.is_a?(Hash) ? __tmp1[i] : _idx(__tmp1, i)))
      i = _add(i, 1)
    end
    return total / (_len(v)).to_f
  end
  def binary_cross_entropy(y_true, y_pred, epsilon)
    if !_eq(_len(y_true), _len(y_pred))
      panic("Input arrays must have the same length.")
    end
    losses = []
    i = 0
    while i < _len(y_true)
      yt = (__tmp2 = y_true; __tmp2.is_a?(Hash) ? __tmp2[i] : _idx(__tmp2, i))
      yp = clip((__tmp3 = y_pred; __tmp3.is_a?(Hash) ? __tmp3[i] : _idx(__tmp3, i)), epsilon, 1.0 - epsilon)
      loss = -(_add(yt * ln(yp), (1.0 - yt) * ln(1.0 - yp)))
      losses = _append(losses, loss)
      i = _add(i, 1)
    end
    return mean(losses)
  end
  def binary_focal_cross_entropy(y_true, y_pred, gamma, alpha, epsilon)
    if !_eq(_len(y_true), _len(y_pred))
      panic("Input arrays must have the same length.")
    end
    losses = []
    i = 0
    while i < _len(y_true)
      yt = (__tmp4 = y_true; __tmp4.is_a?(Hash) ? __tmp4[i] : _idx(__tmp4, i))
      yp = clip((__tmp5 = y_pred; __tmp5.is_a?(Hash) ? __tmp5[i] : _idx(__tmp5, i)), epsilon, 1.0 - epsilon)
      term1 = alpha * powf(1.0 - yp, gamma) * yt * ln(yp)
      term2 = (1.0 - alpha) * powf(yp, gamma) * (1.0 - yt) * ln(1.0 - yp)
      losses = _append(losses, -(_add(term1, term2)))
      i = _add(i, 1)
    end
    return mean(losses)
  end
  def categorical_cross_entropy(y_true, y_pred, epsilon)
    if !_eq(_len(y_true), _len(y_pred))
      panic("Input arrays must have the same shape.")
    end
    rows = _len(y_true)
    total = 0.0
    i = 0
    while i < rows
      if !_eq(_len((__tmp6 = y_true; __tmp6.is_a?(Hash) ? __tmp6[i] : _idx(__tmp6, i))), _len((__tmp7 = y_pred; __tmp7.is_a?(Hash) ? __tmp7[i] : _idx(__tmp7, i))))
        panic("Input arrays must have the same shape.")
      end
      sum_true = 0.0
      sum_pred = 0.0
      j = 0
      while j < _len((__tmp8 = y_true; __tmp8.is_a?(Hash) ? __tmp8[i] : _idx(__tmp8, i)))
        yt = (__tmp9 = (__tmp10 = y_true; __tmp10.is_a?(Hash) ? __tmp10[i] : _idx(__tmp10, i)); __tmp9.is_a?(Hash) ? __tmp9[j] : _idx(__tmp9, j))
        yp = (__tmp11 = (__tmp12 = y_pred; __tmp12.is_a?(Hash) ? __tmp12[i] : _idx(__tmp12, i)); __tmp11.is_a?(Hash) ? __tmp11[j] : _idx(__tmp11, j))
        if (!_eq(yt, 0.0) && !_eq(yt, 1.0))
          panic("y_true must be one-hot encoded.")
        end
        sum_true = _add(sum_true, yt)
        sum_pred = _add(sum_pred, yp)
        j = _add(j, 1)
      end
      if !_eq(sum_true, 1.0)
        panic("y_true must be one-hot encoded.")
      end
      if absf(sum_pred - 1.0) > epsilon
        panic("Predicted probabilities must sum to approximately 1.")
      end
      j = 0
      while j < _len((__tmp13 = y_true; __tmp13.is_a?(Hash) ? __tmp13[i] : _idx(__tmp13, i)))
        yp = clip((__tmp14 = (__tmp15 = y_pred; __tmp15.is_a?(Hash) ? __tmp15[i] : _idx(__tmp15, i)); __tmp14.is_a?(Hash) ? __tmp14[j] : _idx(__tmp14, j)), epsilon, 1.0)
        total = total - ((__tmp16 = (__tmp17 = y_true; __tmp17.is_a?(Hash) ? __tmp17[i] : _idx(__tmp17, i)); __tmp16.is_a?(Hash) ? __tmp16[j] : _idx(__tmp16, j)) * ln(yp))
        j = _add(j, 1)
      end
      i = _add(i, 1)
    end
    return total
  end
  def categorical_focal_cross_entropy(y_true, y_pred, alpha, gamma, epsilon)
    if !_eq(_len(y_true), _len(y_pred))
      panic("Shape of y_true and y_pred must be the same.")
    end
    rows = _len(y_true)
    cols = _len((__tmp18 = y_true; __tmp18.is_a?(Hash) ? __tmp18[0] : _idx(__tmp18, 0)))
    a = alpha
    if _eq(_len(a), 0)
      tmp = []
      j = 0
      while j < cols
        tmp = _append(tmp, 1.0)
        j = _add(j, 1)
      end
      a = tmp
    end
    if !_eq(_len(a), cols)
      panic("Length of alpha must match the number of classes.")
    end
    total = 0.0
    i = 0
    while i < rows
      if !_eq(_len((__tmp19 = y_true; __tmp19.is_a?(Hash) ? __tmp19[i] : _idx(__tmp19, i))), cols) || !_eq(_len((__tmp20 = y_pred; __tmp20.is_a?(Hash) ? __tmp20[i] : _idx(__tmp20, i))), cols)
        panic("Shape of y_true and y_pred must be the same.")
      end
      sum_true = 0.0
      sum_pred = 0.0
      j = 0
      while j < cols
        yt = (__tmp21 = (__tmp22 = y_true; __tmp22.is_a?(Hash) ? __tmp22[i] : _idx(__tmp22, i)); __tmp21.is_a?(Hash) ? __tmp21[j] : _idx(__tmp21, j))
        yp = (__tmp23 = (__tmp24 = y_pred; __tmp24.is_a?(Hash) ? __tmp24[i] : _idx(__tmp24, i)); __tmp23.is_a?(Hash) ? __tmp23[j] : _idx(__tmp23, j))
        if (!_eq(yt, 0.0) && !_eq(yt, 1.0))
          panic("y_true must be one-hot encoded.")
        end
        sum_true = _add(sum_true, yt)
        sum_pred = _add(sum_pred, yp)
        j = _add(j, 1)
      end
      if !_eq(sum_true, 1.0)
        panic("y_true must be one-hot encoded.")
      end
      if absf(sum_pred - 1.0) > epsilon
        panic("Predicted probabilities must sum to approximately 1.")
      end
      row_loss = 0.0
      j = 0
      while j < cols
        yp = clip((__tmp25 = (__tmp26 = y_pred; __tmp26.is_a?(Hash) ? __tmp26[i] : _idx(__tmp26, i)); __tmp25.is_a?(Hash) ? __tmp25[j] : _idx(__tmp25, j)), epsilon, 1.0)
        row_loss = _add(row_loss, (__tmp27 = a; __tmp27.is_a?(Hash) ? __tmp27[j] : _idx(__tmp27, j)) * powf(1.0 - yp, gamma) * (__tmp28 = (__tmp29 = y_true; __tmp29.is_a?(Hash) ? __tmp29[i] : _idx(__tmp29, i)); __tmp28.is_a?(Hash) ? __tmp28[j] : _idx(__tmp28, j)) * ln(yp))
        j = _add(j, 1)
      end
      total = total - row_loss
      i = _add(i, 1)
    end
    return total / (rows).to_f
  end
  def hinge_loss(y_true, y_pred)
    if !_eq(_len(y_true), _len(y_pred))
      panic("Length of predicted and actual array must be same.")
    end
    losses = []
    i = 0
    while i < _len(y_true)
      yt = (__tmp30 = y_true; __tmp30.is_a?(Hash) ? __tmp30[i] : _idx(__tmp30, i))
      if (!_eq(yt, (-1.0)) && !_eq(yt, 1.0))
        panic("y_true can have values -1 or 1 only.")
      end
      pred = (__tmp31 = y_pred; __tmp31.is_a?(Hash) ? __tmp31[i] : _idx(__tmp31, i))
      l = maxf(0.0, 1.0 - yt * pred)
      losses = _append(losses, l)
      i = _add(i, 1)
    end
    return mean(losses)
  end
  def huber_loss(y_true, y_pred, delta)
    if !_eq(_len(y_true), _len(y_pred))
      panic("Input arrays must have the same length.")
    end
    total = 0.0
    i = 0
    while i < _len(y_true)
      diff = (__tmp32 = y_true; __tmp32.is_a?(Hash) ? __tmp32[i] : _idx(__tmp32, i)) - (__tmp33 = y_pred; __tmp33.is_a?(Hash) ? __tmp33[i] : _idx(__tmp33, i))
      adiff = absf(diff)
      if adiff <= delta
        total = _add(total, 0.5 * diff * diff)
      else
        total = _add(total, delta * (adiff - 0.5 * delta))
      end
      i = _add(i, 1)
    end
    return total / (_len(y_true)).to_f
  end
  def mean_squared_error(y_true, y_pred)
    if !_eq(_len(y_true), _len(y_pred))
      panic("Input arrays must have the same length.")
    end
    losses = []
    i = 0
    while i < _len(y_true)
      diff = (__tmp34 = y_true; __tmp34.is_a?(Hash) ? __tmp34[i] : _idx(__tmp34, i)) - (__tmp35 = y_pred; __tmp35.is_a?(Hash) ? __tmp35[i] : _idx(__tmp35, i))
      losses = _append(losses, diff * diff)
      i = _add(i, 1)
    end
    return mean(losses)
  end
  def mean_absolute_error(y_true, y_pred)
    if !_eq(_len(y_true), _len(y_pred))
      panic("Input arrays must have the same length.")
    end
    total = 0.0
    i = 0
    while i < _len(y_true)
      total = _add(total, absf((__tmp36 = y_true; __tmp36.is_a?(Hash) ? __tmp36[i] : _idx(__tmp36, i)) - (__tmp37 = y_pred; __tmp37.is_a?(Hash) ? __tmp37[i] : _idx(__tmp37, i))))
      i = _add(i, 1)
    end
    return total / (_len(y_true)).to_f
  end
  def mean_squared_logarithmic_error(y_true, y_pred)
    if !_eq(_len(y_true), _len(y_pred))
      panic("Input arrays must have the same length.")
    end
    total = 0.0
    i = 0
    while i < _len(y_true)
      a = ln(_add(1.0, (__tmp38 = y_true; __tmp38.is_a?(Hash) ? __tmp38[i] : _idx(__tmp38, i))))
      b = ln(_add(1.0, (__tmp39 = y_pred; __tmp39.is_a?(Hash) ? __tmp39[i] : _idx(__tmp39, i))))
      diff = a - b
      total = _add(total, diff * diff)
      i = _add(i, 1)
    end
    return total / (_len(y_true)).to_f
  end
  def mean_absolute_percentage_error(y_true, y_pred, epsilon)
    if !_eq(_len(y_true), _len(y_pred))
      panic("The length of the two arrays should be the same.")
    end
    total = 0.0
    i = 0
    while i < _len(y_true)
      yt = (__tmp40 = y_true; __tmp40.is_a?(Hash) ? __tmp40[i] : _idx(__tmp40, i))
      if _eq(yt, 0.0)
        yt = epsilon
      end
      total = _add(total, absf((yt - (__tmp41 = y_pred; __tmp41.is_a?(Hash) ? __tmp41[i] : _idx(__tmp41, i))) / yt))
      i = _add(i, 1)
    end
    return total / (_len(y_true)).to_f
  end
  def perplexity_loss(y_true, y_pred, epsilon)
    batch = _len(y_true)
    if !_eq(batch, _len(y_pred))
      panic("Batch size of y_true and y_pred must be equal.")
    end
    sentence_len = _len((__tmp42 = y_true; __tmp42.is_a?(Hash) ? __tmp42[0] : _idx(__tmp42, 0)))
    if !_eq(sentence_len, _len((__tmp43 = y_pred; __tmp43.is_a?(Hash) ? __tmp43[0] : _idx(__tmp43, 0))))
      panic("Sentence length of y_true and y_pred must be equal.")
    end
    vocab_size = _len((__tmp44 = (__tmp45 = y_pred; __tmp45.is_a?(Hash) ? __tmp45[0] : _idx(__tmp45, 0)); __tmp44.is_a?(Hash) ? __tmp44[0] : _idx(__tmp44, 0)))
    b = 0
    total_perp = 0.0
    while b < batch
      if !_eq(_len((__tmp46 = y_true; __tmp46.is_a?(Hash) ? __tmp46[b] : _idx(__tmp46, b))), sentence_len) || !_eq(_len((__tmp47 = y_pred; __tmp47.is_a?(Hash) ? __tmp47[b] : _idx(__tmp47, b))), sentence_len)
        panic("Sentence length of y_true and y_pred must be equal.")
      end
      sum_log = 0.0
      j = 0
      while j < sentence_len
        label = (__tmp48 = (__tmp49 = y_true; __tmp49.is_a?(Hash) ? __tmp49[b] : _idx(__tmp49, b)); __tmp48.is_a?(Hash) ? __tmp48[j] : _idx(__tmp48, j))
        if label >= vocab_size
          panic("Label value must not be greater than vocabulary size.")
        end
        prob = clip((__tmp50 = (__tmp51 = (__tmp52 = y_pred; __tmp52.is_a?(Hash) ? __tmp52[b] : _idx(__tmp52, b)); __tmp51.is_a?(Hash) ? __tmp51[j] : _idx(__tmp51, j)); __tmp50.is_a?(Hash) ? __tmp50[label] : _idx(__tmp50, label)), epsilon, 1.0)
        sum_log = _add(sum_log, ln(prob))
        j = _add(j, 1)
      end
      mean_log = sum_log / (sentence_len).to_f
      perp = exp(-mean_log)
      total_perp = _add(total_perp, perp)
      b = _add(b, 1)
    end
    return total_perp / (batch).to_f
  end
  def smooth_l1_loss(y_true, y_pred, beta)
    if !_eq(_len(y_true), _len(y_pred))
      panic("The length of the two arrays should be the same.")
    end
    total = 0.0
    i = 0
    while i < _len(y_true)
      diff = absf((__tmp53 = y_true; __tmp53.is_a?(Hash) ? __tmp53[i] : _idx(__tmp53, i)) - (__tmp54 = y_pred; __tmp54.is_a?(Hash) ? __tmp54[i] : _idx(__tmp54, i)))
      if diff < beta
        total = _add(total, 0.5 * diff * diff / beta)
      else
        total = _add(total, diff) - 0.5 * beta
      end
      i = _add(i, 1)
    end
    return total / (_len(y_true)).to_f
  end
  def kullback_leibler_divergence(y_true, y_pred)
    if !_eq(_len(y_true), _len(y_pred))
      panic("Input arrays must have the same length.")
    end
    total = 0.0
    i = 0
    while i < _len(y_true)
      total = _add(total, (__tmp55 = y_true; __tmp55.is_a?(Hash) ? __tmp55[i] : _idx(__tmp55, i)) * ln((__tmp56 = y_true; __tmp56.is_a?(Hash) ? __tmp56[i] : _idx(__tmp56, i)) / (__tmp57 = y_pred; __tmp57.is_a?(Hash) ? __tmp57[i] : _idx(__tmp57, i))))
      i = _add(i, 1)
    end
    return total
  end
  def main()
    y_true_bc = [0.0, 1.0, 1.0, 0.0, 1.0]
    y_pred_bc = [0.2, 0.7, 0.9, 0.3, 0.8]
    puts(binary_cross_entropy(y_true_bc, y_pred_bc, 1e-15))
    puts(binary_focal_cross_entropy(y_true_bc, y_pred_bc, 2.0, 0.25, 1e-15))
    y_true_cce = [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]
    y_pred_cce = [[0.9, 0.1, 0.0], [0.2, 0.7, 0.1], [0.0, 0.1, 0.9]]
    puts(categorical_cross_entropy(y_true_cce, y_pred_cce, 1e-15))
    alpha = [0.6, 0.2, 0.7]
    puts(categorical_focal_cross_entropy(y_true_cce, y_pred_cce, alpha, 2.0, 1e-15))
    y_true_hinge = [-1.0, 1.0, 1.0, -1.0, 1.0]
    y_pred_hinge = [-4.0, -0.3, 0.7, 5.0, 10.0]
    puts(hinge_loss(y_true_hinge, y_pred_hinge))
    y_true_huber = [0.9, 10.0, 2.0, 1.0, 5.2]
    y_pred_huber = [0.8, 2.1, 2.9, 4.2, 5.2]
    puts(huber_loss(y_true_huber, y_pred_huber, 1.0))
    puts(mean_squared_error(y_true_huber, y_pred_huber))
    puts(mean_absolute_error(y_true_huber, y_pred_huber))
    puts(mean_squared_logarithmic_error(y_true_huber, y_pred_huber))
    y_true_mape = [10.0, 20.0, 30.0, 40.0]
    y_pred_mape = [12.0, 18.0, 33.0, 45.0]
    puts(mean_absolute_percentage_error(y_true_mape, y_pred_mape, 1e-15))
    y_true_perp = [[1, 4], [2, 3]]
    y_pred_perp = [[[0.28, 0.19, 0.21, 0.15, 0.17], [0.24, 0.19, 0.09, 0.18, 0.3]], [[0.03, 0.26, 0.21, 0.18, 0.32], [0.28, 0.1, 0.33, 0.15, 0.14]]]
    puts(perplexity_loss(y_true_perp, y_pred_perp, 1e-07))
    y_true_smooth = [3.0, 5.0, 2.0, 7.0]
    y_pred_smooth = [2.9, 4.8, 2.1, 7.2]
    puts(smooth_l1_loss(y_true_smooth, y_pred_smooth, 1.0))
    y_true_kl = [0.2, 0.3, 0.5]
    y_pred_kl = [0.3, 0.3, 0.4]
    puts(kullback_leibler_divergence(y_true_kl, y_pred_kl))
  end
  main()
end_time = Process.clock_gettime(Process::CLOCK_MONOTONIC, :nanosecond)
end_mem = _mem()
result = {"duration_us" => ((end_time - start) / 1000), "memory_bytes" => (end_mem - start_mem), "name" => "main"}
puts(JSON.pretty_generate(result))
