# Generated by Mochi transpiler v0.10.59 on 2025-08-06 22:14 +0700
require 'json'

$now_seed = 0
$now_seeded = false
s = ENV['MOCHI_NOW_SEED']
if (!s || s == '') && ENV['MOCHI_BENCHMARK']
  s = '1'
end
if s && s != ''
  begin
    $now_seed = Integer(s)
    $now_seeded = true
  rescue StandardError
  end
end
def _now()
  if $now_seeded
    $now_seed += 1_000_000
    $now_seed
  else
    Process.clock_gettime(Process::CLOCK_MONOTONIC, :nanosecond)
  end
end


require 'objspace'
def _mem()
  ObjectSpace.memsize_of_all
end


def _add(a, b)
  if a.is_a?(Array) && b.is_a?(String)
    a.join + b
  elsif a.is_a?(String) && b.is_a?(Array)
    a + b.join
  else
    a + b
  end
end


def _padStart(s, len, ch)
  s.to_s.rjust(len, ch)
end


def _str(x)
  if x.is_a?(Float) && x == x.to_i
    x.to_i.to_s
  else
    x.to_s
  end
end


class String
  alias each each_char
end

start_mem = _mem()
start = _now()
  def absf(x)
    if x < 0
      return -x
    end
    return x
  end
  def maxf(a, b)
    if a > b
      return a
    end
    return b
  end
  def minf(a, b)
    if a < b
      return a
    end
    return b
  end
  def clip(x, lo, hi)
    return maxf(lo, minf(x, hi))
  end
  def to_float(x)
    return x * 1
  end
  def powf(base, exp)
    result = 1.clone
    i = 0.clone
    n = (exp).to_i
    while i < n
      result = result * base.clone
      i = _add(i, 1).clone
    end
    return result
  end
  def ln(x)
    if x <= 0
      panic("ln domain error")
    end
    y = (x - 1) / (_add(x, 1))
    y2 = y * y
    term = y.clone
    sum = 0.clone
    k = 0.clone
    while k < 10
      denom = to_float(_add(2 * k, 1))
      sum = _add(sum, term / denom).clone
      term = term * y2.clone
      k = _add(k, 1).clone
    end
    return 2 * sum
  end
  def exp(x)
    term = 1.clone
    sum = 1.clone
    n = 1.clone
    while n < 20
      term = term * x / to_float(n).clone
      sum = _add(sum, term).clone
      n = _add(n, 1).clone
    end
    return sum
  end
  def mean(v)
    total = 0.clone
    i = 0.clone
    while i < v.length
      total = _add(total, v[i]).clone
      i = _add(i, 1).clone
    end
    return total / to_float(v.length)
  end
  def binary_cross_entropy(y_true, y_pred, epsilon)
    if y_true.length != y_pred.length
      panic("Input arrays must have the same length.")
    end
    losses = [].clone
    i = 0.clone
    while i < y_true.length
      yt = y_true[i]
      yp = clip(y_pred[i], epsilon, 1 - epsilon)
      loss = -(_add(yt * ln(yp), (1 - yt) * ln(1 - yp)))
      losses = (losses << (loss)).clone
      i = _add(i, 1).clone
    end
    return mean(losses)
  end
  def binary_focal_cross_entropy(y_true, y_pred, gamma, alpha, epsilon)
    if y_true.length != y_pred.length
      panic("Input arrays must have the same length.")
    end
    losses = [].clone
    i = 0.clone
    while i < y_true.length
      yt = y_true[i]
      yp = clip(y_pred[i], epsilon, 1 - epsilon)
      term1 = alpha * powf(1 - yp, gamma) * yt * ln(yp)
      term2 = (1 - alpha) * powf(yp, gamma) * (1 - yt) * ln(1 - yp)
      losses = (losses << (-(_add(term1, term2)))).clone
      i = _add(i, 1).clone
    end
    return mean(losses)
  end
  def categorical_cross_entropy(y_true, y_pred, epsilon)
    if y_true.length != y_pred.length
      panic("Input arrays must have the same shape.")
    end
    rows = y_true.length
    total = 0.clone
    i = 0.clone
    while i < rows
      if y_true[i].length != y_pred[i].length
        panic("Input arrays must have the same shape.")
      end
      sum_true = 0.clone
      sum_pred = 0.clone
      j = 0.clone
      while j < y_true[i].length
        yt = y_true[i][j]
        yp = y_pred[i][j]
        if (yt != 0 && yt != 1)
          panic("y_true must be one-hot encoded.")
        end
        sum_true = _add(sum_true, yt).clone
        sum_pred = _add(sum_pred, yp).clone
        j = _add(j, 1).clone
      end
      if sum_true != 1
        panic("y_true must be one-hot encoded.")
      end
      if absf(sum_pred - 1) > epsilon
        panic("Predicted probabilities must sum to approximately 1.")
      end
      j = 0.clone
      while j < y_true[i].length
        yp = clip(y_pred[i][j], epsilon, 1)
        total = total - (y_true[i][j] * ln(yp)).clone
        j = _add(j, 1).clone
      end
      i = _add(i, 1).clone
    end
    return total
  end
  def categorical_focal_cross_entropy(y_true, y_pred, alpha, gamma, epsilon)
    if y_true.length != y_pred.length
      panic("Shape of y_true and y_pred must be the same.")
    end
    rows = y_true.length
    cols = y_true[0].length
    a = alpha.clone
    if a.length == 0
      tmp = [].clone
      j = 0.clone
      while j < cols
        tmp = (tmp << (1)).clone
        j = _add(j, 1).clone
      end
      a = tmp.clone
    end
    if a.length != cols
      panic("Length of alpha must match the number of classes.")
    end
    total = 0.clone
    i = 0.clone
    while i < rows
      if y_true[i].length != cols || y_pred[i].length != cols
        panic("Shape of y_true and y_pred must be the same.")
      end
      sum_true = 0.clone
      sum_pred = 0.clone
      j = 0.clone
      while j < cols
        yt = y_true[i][j]
        yp = y_pred[i][j]
        if (yt != 0 && yt != 1)
          panic("y_true must be one-hot encoded.")
        end
        sum_true = _add(sum_true, yt).clone
        sum_pred = _add(sum_pred, yp).clone
        j = _add(j, 1).clone
      end
      if sum_true != 1
        panic("y_true must be one-hot encoded.")
      end
      if absf(sum_pred - 1) > epsilon
        panic("Predicted probabilities must sum to approximately 1.")
      end
      row_loss = 0.clone
      j = 0.clone
      while j < cols
        yp = clip(y_pred[i][j], epsilon, 1)
        row_loss = _add(row_loss, a[j] * powf(1 - yp, gamma) * y_true[i][j] * ln(yp)).clone
        j = _add(j, 1).clone
      end
      total = total - row_loss.clone
      i = _add(i, 1).clone
    end
    return total / to_float(rows)
  end
  def hinge_loss(y_true, y_pred)
    if y_true.length != y_pred.length
      panic("Length of predicted and actual array must be same.")
    end
    losses = [].clone
    i = 0.clone
    while i < y_true.length
      yt = y_true[i]
      if (yt != (-1) && yt != 1)
        panic("y_true can have values -1 or 1 only.")
      end
      pred = y_pred[i]
      l = maxf(0, 1 - yt * pred)
      losses = (losses << (l)).clone
      i = _add(i, 1).clone
    end
    return mean(losses)
  end
  def huber_loss(y_true, y_pred, delta)
    if y_true.length != y_pred.length
      panic("Input arrays must have the same length.")
    end
    total = 0.clone
    i = 0.clone
    while i < y_true.length
      diff = y_true[i] - y_pred[i]
      adiff = absf(diff)
      if adiff <= delta
        total = _add(total, 0.5 * diff * diff).clone
      else
        total = _add(total, delta * (adiff - 0.5 * delta)).clone
      end
      i = _add(i, 1).clone
    end
    return total / to_float(y_true.length)
  end
  def mean_squared_error(y_true, y_pred)
    if y_true.length != y_pred.length
      panic("Input arrays must have the same length.")
    end
    losses = [].clone
    i = 0.clone
    while i < y_true.length
      diff = y_true[i] - y_pred[i]
      losses = (losses << (diff * diff)).clone
      i = _add(i, 1).clone
    end
    return mean(losses)
  end
  def mean_absolute_error(y_true, y_pred)
    if y_true.length != y_pred.length
      panic("Input arrays must have the same length.")
    end
    total = 0.clone
    i = 0.clone
    while i < y_true.length
      total = _add(total, absf(y_true[i] - y_pred[i])).clone
      i = _add(i, 1).clone
    end
    return total / to_float(y_true.length)
  end
  def mean_squared_logarithmic_error(y_true, y_pred)
    if y_true.length != y_pred.length
      panic("Input arrays must have the same length.")
    end
    total = 0.clone
    i = 0.clone
    while i < y_true.length
      a = ln(_add(1, y_true[i]))
      b = ln(_add(1, y_pred[i]))
      diff = a - b
      total = _add(total, diff * diff).clone
      i = _add(i, 1).clone
    end
    return total / to_float(y_true.length)
  end
  def mean_absolute_percentage_error(y_true, y_pred, epsilon)
    if y_true.length != y_pred.length
      panic("The length of the two arrays should be the same.")
    end
    total = 0.clone
    i = 0.clone
    while i < y_true.length
      yt = y_true[i].clone
      if yt == 0
        yt = epsilon.clone
      end
      total = _add(total, absf((yt - y_pred[i]) / yt)).clone
      i = _add(i, 1).clone
    end
    return total / to_float(y_true.length)
  end
  def perplexity_loss(y_true, y_pred, epsilon)
    batch = y_true.length
    if batch != y_pred.length
      panic("Batch size of y_true and y_pred must be equal.")
    end
    sentence_len = y_true[0].length
    if sentence_len != y_pred[0].length
      panic("Sentence length of y_true and y_pred must be equal.")
    end
    vocab_size = y_pred[0][0].length
    b = 0.clone
    total_perp = 0.clone
    while b < batch
      if y_true[b].length != sentence_len || y_pred[b].length != sentence_len
        panic("Sentence length of y_true and y_pred must be equal.")
      end
      sum_log = 0.clone
      j = 0.clone
      while j < sentence_len
        label = y_true[b][j]
        if label >= vocab_size
          panic("Label value must not be greater than vocabulary size.")
        end
        prob = clip(y_pred[b][j][label], epsilon, 1)
        sum_log = _add(sum_log, ln(prob)).clone
        j = _add(j, 1).clone
      end
      mean_log = sum_log / to_float(sentence_len)
      perp = exp(-mean_log)
      total_perp = _add(total_perp, perp).clone
      b = _add(b, 1).clone
    end
    return total_perp / to_float(batch)
  end
  def smooth_l1_loss(y_true, y_pred, beta)
    if y_true.length != y_pred.length
      panic("The length of the two arrays should be the same.")
    end
    total = 0.clone
    i = 0.clone
    while i < y_true.length
      diff = absf(y_true[i] - y_pred[i])
      if diff < beta
        total = _add(total, 0.5 * diff * diff / beta).clone
      else
        total = _add(total, diff) - 0.5 * beta.clone
      end
      i = _add(i, 1).clone
    end
    return total / to_float(y_true.length)
  end
  def kullback_leibler_divergence(y_true, y_pred)
    if y_true.length != y_pred.length
      panic("Input arrays must have the same length.")
    end
    total = 0.clone
    i = 0.clone
    while i < y_true.length
      total = _add(total, y_true[i] * ln(y_true[i] / y_pred[i])).clone
      i = _add(i, 1).clone
    end
    return total
  end
  def main()
    y_true_bc = [0, 1, 1, 0, 1]
    y_pred_bc = [0.2, 0.7, 0.9, 0.3, 0.8]
    puts(binary_cross_entropy(y_true_bc, y_pred_bc, 1e-15))
    puts(binary_focal_cross_entropy(y_true_bc, y_pred_bc, 2, 0.25, 1e-15))
    y_true_cce = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]
    y_pred_cce = [[0.9, 0.1, 0], [0.2, 0.7, 0.1], [0, 0.1, 0.9]]
    puts(categorical_cross_entropy(y_true_cce, y_pred_cce, 1e-15))
    alpha = [0.6, 0.2, 0.7]
    puts(categorical_focal_cross_entropy(y_true_cce, y_pred_cce, alpha, 2, 1e-15))
    y_true_hinge = [-1, 1, 1, -1, 1]
    y_pred_hinge = [-4, -0.3, 0.7, 5, 10]
    puts(hinge_loss(y_true_hinge, y_pred_hinge))
    y_true_huber = [0.9, 10, 2, 1, 5.2]
    y_pred_huber = [0.8, 2.1, 2.9, 4.2, 5.2]
    puts(huber_loss(y_true_huber, y_pred_huber, 1))
    puts(mean_squared_error(y_true_huber, y_pred_huber))
    puts(mean_absolute_error(y_true_huber, y_pred_huber))
    puts(mean_squared_logarithmic_error(y_true_huber, y_pred_huber))
    y_true_mape = [10, 20, 30, 40]
    y_pred_mape = [12, 18, 33, 45]
    puts(mean_absolute_percentage_error(y_true_mape, y_pred_mape, 1e-15))
    y_true_perp = [[1, 4], [2, 3]]
    y_pred_perp = [[[0.28, 0.19, 0.21, 0.15, 0.17], [0.24, 0.19, 0.09, 0.18, 0.3]], [[0.03, 0.26, 0.21, 0.18, 0.32], [0.28, 0.1, 0.33, 0.15, 0.14]]]
    puts(perplexity_loss(y_true_perp, y_pred_perp, 1e-07))
    y_true_smooth = [3, 5, 2, 7]
    y_pred_smooth = [2.9, 4.8, 2.1, 7.2]
    puts(smooth_l1_loss(y_true_smooth, y_pred_smooth, 1))
    y_true_kl = [0.2, 0.3, 0.5]
    y_pred_kl = [0.3, 0.3, 0.4]
    puts(kullback_leibler_divergence(y_true_kl, y_pred_kl))
  end
  main()
end_time = _now()
end_mem = _mem()
result = {"duration_us" => ((end_time - start) / 1000), "memory_bytes" => (end_mem - start_mem), "name" => "main"}
puts(JSON.pretty_generate(result))
