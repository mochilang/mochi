# Generated by Mochi transpiler v0.10.63 on 2025-08-11 18:28 +0700
$VERBOSE = nil
require 'json'

$now_seed = 0
$now_seeded = false
s = ENV['MOCHI_NOW_SEED']
if s && s != ''
  begin
    $now_seed = Integer(s)
    $now_seeded = true
  rescue StandardError
  end
end
if !$now_seeded && ENV['MOCHI_BENCHMARK']
  $now_seeded = true
end
def _now()
  if $now_seeded
    $now_seed = ($now_seed * 1_664_525 + 1_013_904_223) % 2_147_483_647
    $now_seed
  else
    Process.clock_gettime(Process::CLOCK_MONOTONIC, :nanosecond)
  end
end


require 'objspace'
def _mem()
  ObjectSpace.memsize_of_all
end


def _add(a, b)
  if a.is_a?(Array) && b.is_a?(String)
    a.join + b
  elsif a.is_a?(String) && b.is_a?(Array)
    a + b.join
  elsif a.is_a?(Array) && !b.is_a?(Array)
    a + [b]
  elsif !a.is_a?(Array) && b.is_a?(Array)
    [a] + b
  elsif a.is_a?(String) || b.is_a?(String)
    a.to_s + b.to_s
  else
    a + b
  end
end


def _eq(a, b)
  if a.is_a?(Float) || b.is_a?(Float)
    (a.to_f - b.to_f).abs < 1e-6
  else
    a == b
  end
end


def _padStart(s, len, ch)
  s.to_s.rjust(len, ch)
end


def _padEnd(s, len, ch)
  s.to_s.ljust(len, ch)
end


def _str(x)
  if x.is_a?(Array)
    x.map { |e| _str(e) }.join(' ')
  elsif x.is_a?(Float) && x == x.to_i
    x.to_i.to_s
  else
    x.to_s
  end
end


class String
  alias each each_char
end


def panic(msg)
  raise RuntimeError, msg
end

__name__ = '__main__'
start_mem = _mem()
start = _now()
  def absf(x)
    if x < 0.0
      return -x
    end
    return x
  end
  def maxf(a, b)
    if a > b
      return a
    end
    return b
  end
  def minf(a, b)
    if a < b
      return a
    end
    return b
  end
  def clip(x, lo, hi)
    return maxf(lo, minf(x, hi))
  end
  def to_float(x)
    return x * 1.0
  end
  def powf(base, exp)
    result = 1.0
    i = 0
    n = (exp).to_i
    while i < n
      result = result * base
      i = _add(i, 1)
    end
    return result
  end
  def ln(x)
    if x <= 0.0
      panic("ln domain error")
    end
    y = (x - 1.0) / (_add(x, 1.0))
    y2 = y * y
    term = y
    sum = 0.0
    k = 0
    while k < 10
      denom = (_add(2 * k, 1)).to_f
      sum = _add(sum, term / denom)
      term = term * y2
      k = _add(k, 1)
    end
    return 2.0 * sum
  end
  def exp(x)
    term = 1.0
    sum = 1.0
    n = 1
    while n < 20
      term = term * x / (n).to_f
      sum = _add(sum, term)
      n = _add(n, 1)
    end
    return sum
  end
  def mean(v)
    total = 0.0
    i = 0
    while i < v.length
      total = _add(total, v[i])
      i = _add(i, 1)
    end
    return total / (v.length).to_f
  end
  def binary_cross_entropy(y_true, y_pred, epsilon)
    if !_eq(y_true.length, y_pred.length)
      panic("Input arrays must have the same length.")
    end
    losses = []
    i = 0
    while i < y_true.length
      yt = y_true[i]
      yp = clip(y_pred[i], epsilon, 1.0 - epsilon)
      loss = -(_add(yt * ln(yp), (1.0 - yt) * ln(1.0 - yp)))
      losses = (losses + [loss])
      i = _add(i, 1)
    end
    return mean(losses)
  end
  def binary_focal_cross_entropy(y_true, y_pred, gamma, alpha, epsilon)
    if !_eq(y_true.length, y_pred.length)
      panic("Input arrays must have the same length.")
    end
    losses = []
    i = 0
    while i < y_true.length
      yt = y_true[i]
      yp = clip(y_pred[i], epsilon, 1.0 - epsilon)
      term1 = alpha * powf(1.0 - yp, gamma) * yt * ln(yp)
      term2 = (1.0 - alpha) * powf(yp, gamma) * (1.0 - yt) * ln(1.0 - yp)
      losses = (losses + [-(_add(term1, term2))])
      i = _add(i, 1)
    end
    return mean(losses)
  end
  def categorical_cross_entropy(y_true, y_pred, epsilon)
    if !_eq(y_true.length, y_pred.length)
      panic("Input arrays must have the same shape.")
    end
    rows = y_true.length
    total = 0.0
    i = 0
    while i < rows
      if !_eq(y_true[i].length, y_pred[i].length)
        panic("Input arrays must have the same shape.")
      end
      sum_true = 0.0
      sum_pred = 0.0
      j = 0
      while j < y_true[i].length
        yt = y_true[i][j]
        yp = y_pred[i][j]
        if (!_eq(yt, 0.0) && !_eq(yt, 1.0))
          panic("y_true must be one-hot encoded.")
        end
        sum_true = _add(sum_true, yt)
        sum_pred = _add(sum_pred, yp)
        j = _add(j, 1)
      end
      if !_eq(sum_true, 1.0)
        panic("y_true must be one-hot encoded.")
      end
      if absf(sum_pred - 1.0) > epsilon
        panic("Predicted probabilities must sum to approximately 1.")
      end
      j = 0
      while j < y_true[i].length
        yp = clip(y_pred[i][j], epsilon, 1.0)
        total = total - (y_true[i][j] * ln(yp))
        j = _add(j, 1)
      end
      i = _add(i, 1)
    end
    return total
  end
  def categorical_focal_cross_entropy(y_true, y_pred, alpha, gamma, epsilon)
    if !_eq(y_true.length, y_pred.length)
      panic("Shape of y_true and y_pred must be the same.")
    end
    rows = y_true.length
    cols = y_true[0].length
    a = alpha
    if _eq(a.length, 0)
      tmp = []
      j = 0
      while j < cols
        tmp = (tmp + [1.0])
        j = _add(j, 1)
      end
      a = tmp
    end
    if !_eq(a.length, cols)
      panic("Length of alpha must match the number of classes.")
    end
    total = 0.0
    i = 0
    while i < rows
      if !_eq(y_true[i].length, cols) || !_eq(y_pred[i].length, cols)
        panic("Shape of y_true and y_pred must be the same.")
      end
      sum_true = 0.0
      sum_pred = 0.0
      j = 0
      while j < cols
        yt = y_true[i][j]
        yp = y_pred[i][j]
        if (!_eq(yt, 0.0) && !_eq(yt, 1.0))
          panic("y_true must be one-hot encoded.")
        end
        sum_true = _add(sum_true, yt)
        sum_pred = _add(sum_pred, yp)
        j = _add(j, 1)
      end
      if !_eq(sum_true, 1.0)
        panic("y_true must be one-hot encoded.")
      end
      if absf(sum_pred - 1.0) > epsilon
        panic("Predicted probabilities must sum to approximately 1.")
      end
      row_loss = 0.0
      j = 0
      while j < cols
        yp = clip(y_pred[i][j], epsilon, 1.0)
        row_loss = _add(row_loss, a[j] * powf(1.0 - yp, gamma) * y_true[i][j] * ln(yp))
        j = _add(j, 1)
      end
      total = total - row_loss
      i = _add(i, 1)
    end
    return total / (rows).to_f
  end
  def hinge_loss(y_true, y_pred)
    if !_eq(y_true.length, y_pred.length)
      panic("Length of predicted and actual array must be same.")
    end
    losses = []
    i = 0
    while i < y_true.length
      yt = y_true[i]
      if (!_eq(yt, (-1.0)) && !_eq(yt, 1.0))
        panic("y_true can have values -1 or 1 only.")
      end
      pred = y_pred[i]
      l = maxf(0.0, 1.0 - yt * pred)
      losses = (losses + [l])
      i = _add(i, 1)
    end
    return mean(losses)
  end
  def huber_loss(y_true, y_pred, delta)
    if !_eq(y_true.length, y_pred.length)
      panic("Input arrays must have the same length.")
    end
    total = 0.0
    i = 0
    while i < y_true.length
      diff = y_true[i] - y_pred[i]
      adiff = absf(diff)
      if adiff <= delta
        total = _add(total, 0.5 * diff * diff)
      else
        total = _add(total, delta * (adiff - 0.5 * delta))
      end
      i = _add(i, 1)
    end
    return total / (y_true.length).to_f
  end
  def mean_squared_error(y_true, y_pred)
    if !_eq(y_true.length, y_pred.length)
      panic("Input arrays must have the same length.")
    end
    losses = []
    i = 0
    while i < y_true.length
      diff = y_true[i] - y_pred[i]
      losses = (losses + [diff * diff])
      i = _add(i, 1)
    end
    return mean(losses)
  end
  def mean_absolute_error(y_true, y_pred)
    if !_eq(y_true.length, y_pred.length)
      panic("Input arrays must have the same length.")
    end
    total = 0.0
    i = 0
    while i < y_true.length
      total = _add(total, absf(y_true[i] - y_pred[i]))
      i = _add(i, 1)
    end
    return total / (y_true.length).to_f
  end
  def mean_squared_logarithmic_error(y_true, y_pred)
    if !_eq(y_true.length, y_pred.length)
      panic("Input arrays must have the same length.")
    end
    total = 0.0
    i = 0
    while i < y_true.length
      a = ln(_add(1.0, y_true[i]))
      b = ln(_add(1.0, y_pred[i]))
      diff = a - b
      total = _add(total, diff * diff)
      i = _add(i, 1)
    end
    return total / (y_true.length).to_f
  end
  def mean_absolute_percentage_error(y_true, y_pred, epsilon)
    if !_eq(y_true.length, y_pred.length)
      panic("The length of the two arrays should be the same.")
    end
    total = 0.0
    i = 0
    while i < y_true.length
      yt = y_true[i]
      if _eq(yt, 0.0)
        yt = epsilon
      end
      total = _add(total, absf((yt - y_pred[i]) / yt))
      i = _add(i, 1)
    end
    return total / (y_true.length).to_f
  end
  def perplexity_loss(y_true, y_pred, epsilon)
    batch = y_true.length
    if !_eq(batch, y_pred.length)
      panic("Batch size of y_true and y_pred must be equal.")
    end
    sentence_len = y_true[0].length
    if !_eq(sentence_len, y_pred[0].length)
      panic("Sentence length of y_true and y_pred must be equal.")
    end
    vocab_size = y_pred[0][0].length
    b = 0
    total_perp = 0.0
    while b < batch
      if !_eq(y_true[b].length, sentence_len) || !_eq(y_pred[b].length, sentence_len)
        panic("Sentence length of y_true and y_pred must be equal.")
      end
      sum_log = 0.0
      j = 0
      while j < sentence_len
        label = y_true[b][j]
        if label >= vocab_size
          panic("Label value must not be greater than vocabulary size.")
        end
        prob = clip(y_pred[b][j][label], epsilon, 1.0)
        sum_log = _add(sum_log, ln(prob))
        j = _add(j, 1)
      end
      mean_log = sum_log / (sentence_len).to_f
      perp = exp(-mean_log)
      total_perp = _add(total_perp, perp)
      b = _add(b, 1)
    end
    return total_perp / (batch).to_f
  end
  def smooth_l1_loss(y_true, y_pred, beta)
    if !_eq(y_true.length, y_pred.length)
      panic("The length of the two arrays should be the same.")
    end
    total = 0.0
    i = 0
    while i < y_true.length
      diff = absf(y_true[i] - y_pred[i])
      if diff < beta
        total = _add(total, 0.5 * diff * diff / beta)
      else
        total = _add(total, diff) - 0.5 * beta
      end
      i = _add(i, 1)
    end
    return total / (y_true.length).to_f
  end
  def kullback_leibler_divergence(y_true, y_pred)
    if !_eq(y_true.length, y_pred.length)
      panic("Input arrays must have the same length.")
    end
    total = 0.0
    i = 0
    while i < y_true.length
      total = _add(total, y_true[i] * ln(y_true[i] / y_pred[i]))
      i = _add(i, 1)
    end
    return total
  end
  def main()
    y_true_bc = [0.0, 1.0, 1.0, 0.0, 1.0]
    y_pred_bc = [0.2, 0.7, 0.9, 0.3, 0.8]
    puts(binary_cross_entropy(y_true_bc, y_pred_bc, 1e-15))
    puts(binary_focal_cross_entropy(y_true_bc, y_pred_bc, 2.0, 0.25, 1e-15))
    y_true_cce = [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]
    y_pred_cce = [[0.9, 0.1, 0.0], [0.2, 0.7, 0.1], [0.0, 0.1, 0.9]]
    puts(categorical_cross_entropy(y_true_cce, y_pred_cce, 1e-15))
    alpha = [0.6, 0.2, 0.7]
    puts(categorical_focal_cross_entropy(y_true_cce, y_pred_cce, alpha, 2.0, 1e-15))
    y_true_hinge = [-1.0, 1.0, 1.0, -1.0, 1.0]
    y_pred_hinge = [-4.0, -0.3, 0.7, 5.0, 10.0]
    puts(hinge_loss(y_true_hinge, y_pred_hinge))
    y_true_huber = [0.9, 10.0, 2.0, 1.0, 5.2]
    y_pred_huber = [0.8, 2.1, 2.9, 4.2, 5.2]
    puts(huber_loss(y_true_huber, y_pred_huber, 1.0))
    puts(mean_squared_error(y_true_huber, y_pred_huber))
    puts(mean_absolute_error(y_true_huber, y_pred_huber))
    puts(mean_squared_logarithmic_error(y_true_huber, y_pred_huber))
    y_true_mape = [10.0, 20.0, 30.0, 40.0]
    y_pred_mape = [12.0, 18.0, 33.0, 45.0]
    puts(mean_absolute_percentage_error(y_true_mape, y_pred_mape, 1e-15))
    y_true_perp = [[1, 4], [2, 3]]
    y_pred_perp = [[[0.28, 0.19, 0.21, 0.15, 0.17], [0.24, 0.19, 0.09, 0.18, 0.3]], [[0.03, 0.26, 0.21, 0.18, 0.32], [0.28, 0.1, 0.33, 0.15, 0.14]]]
    puts(perplexity_loss(y_true_perp, y_pred_perp, 1e-07))
    y_true_smooth = [3.0, 5.0, 2.0, 7.0]
    y_pred_smooth = [2.9, 4.8, 2.1, 7.2]
    puts(smooth_l1_loss(y_true_smooth, y_pred_smooth, 1.0))
    y_true_kl = [0.2, 0.3, 0.5]
    y_pred_kl = [0.3, 0.3, 0.4]
    puts(kullback_leibler_divergence(y_true_kl, y_pred_kl))
  end
  main()
end_time = _now()
end_mem = _mem()
result = {"duration_us" => ((end_time - start) / 1000), "memory_bytes" => (end_mem - start_mem), "name" => "main"}
puts(JSON.pretty_generate(result))
