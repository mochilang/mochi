# Generated by Mochi transpiler v0.10.59 on 2025-08-07 08:16 +0700
require 'json'

$now_seed = 0
$now_seeded = false
s = ENV['MOCHI_NOW_SEED']
if (!s || s == '') && ENV['MOCHI_BENCHMARK']
  s = '1'
end
if s && s != ''
  begin
    $now_seed = Integer(s)
    $now_seeded = true
  rescue StandardError
  end
end
def _now()
  if $now_seeded
    $now_seed += 1_000_000
    $now_seed
  else
    Process.clock_gettime(Process::CLOCK_MONOTONIC, :nanosecond)
  end
end


require 'objspace'
def _mem()
  ObjectSpace.memsize_of_all
end


def _add(a, b)
  if a.is_a?(Array) && b.is_a?(String)
    a.join + b
  elsif a.is_a?(String) && b.is_a?(Array)
    a + b.join
  else
    a + b
  end
end


def _padStart(s, len, ch)
  s.to_s.rjust(len, ch)
end


def _str(x)
  if x.is_a?(Float) && x == x.to_i
    x.to_i.to_s
  else
    x.to_s
  end
end


class String
  alias each each_char
end


def panic(msg)
  raise RuntimeError, msg
end

start_mem = _mem()
start = _now()
  def dense_to_one_hot(labels, num_classes)
    result = [].clone
    i = 0.clone
    while i < labels.length
      row = [].clone
      j = 0.clone
      while j < num_classes
        if j == labels[i]
          row = (row << (1)).clone
        else
          row = (row << (0)).clone
        end
        j = _add(j, 1).clone
      end
      result = (result << (row)).clone
      i = _add(i, 1).clone
    end
    return result
  end
  def new_dataset(images, labels)
    return DataSet.new(images: images, labels: labels, num_examples: images.length, index_in_epoch: 0, epochs_completed: 0)
  end
  def next_batch(ds, batch_size)
    start = ds.index_in_epoch
    if _add(start, batch_size) > ds.num_examples
      rest = ds.num_examples - start
      images_rest = ds.images[start...ds.num_examples]
      labels_rest = ds.labels[start...ds.num_examples]
      new_index = batch_size - rest
      images_new = ds.images[0...new_index]
      labels_new = ds.labels[0...new_index]
      batch_images = _add(images_rest, images_new)
      batch_labels = _add(labels_rest, labels_new)
      new_ds = DataSet.new(images: ds.images, labels: ds.labels, num_examples: ds.num_examples, index_in_epoch: new_index, epochs_completed: _add(ds.epochs_completed, 1))
      return BatchResult.new(dataset: new_ds, images: batch_images, labels: batch_labels)
    else
      end_ = _add(start, batch_size)
      batch_images = ds.images[start...end_]
      batch_labels = ds.labels[start...end_]
      new_ds = DataSet.new(images: ds.images, labels: ds.labels, num_examples: ds.num_examples, index_in_epoch: end_, epochs_completed: ds.epochs_completed)
      return BatchResult.new(dataset: new_ds, images: batch_images, labels: batch_labels)
    end
  end
  def read_data_sets(train_images, train_labels_raw, test_images, test_labels_raw, validation_size, num_classes)
    train_labels = dense_to_one_hot(train_labels_raw, num_classes)
    test_labels = dense_to_one_hot(test_labels_raw, num_classes)
    validation_images = train_images[0...validation_size]
    validation_labels = train_labels[0...validation_size]
    train_images_rest = train_images[validation_size...train_images.length]
    train_labels_rest = train_labels[validation_size...train_labels.length]
    train = new_dataset(train_images_rest, train_labels_rest)
    validation = new_dataset(validation_images, validation_labels)
    testset = new_dataset(test_images, test_labels)
    return Datasets.new(train: train, validation: validation, test_ds: testset)
  end
  def main()
    train_images = [[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]
    train_labels_raw = [0, 1, 2, 3, 4]
    test_images = [[5, 6], [6, 7]]
    test_labels_raw = [5, 6]
    data = read_data_sets(train_images, train_labels_raw, test_images, test_labels_raw, 2, 10)
    ds = data.train.clone
    res = next_batch(ds, 2).clone
    ds = res.dataset.clone
    puts(((x = res.images); x.is_a?(Array) ? ("[" + x.map{ |x| if x.is_a?(Hash) then '{' + x.to_h.map{ |k,v| "#{k}: #{v.is_a?(String) ? v : v.to_s}" }.join(', ') + '}' else x.to_s end }.join(' ') + "]") : x.to_s))
    puts(((x = res.labels); x.is_a?(Array) ? ("[" + x.map{ |x| if x.is_a?(Hash) then '{' + x.to_h.map{ |k,v| "#{k}: #{v.is_a?(String) ? v : v.to_s}" }.join(', ') + '}' else x.to_s end }.join(' ') + "]") : x.to_s))
    res = next_batch(ds, 2).clone
    ds = res.dataset.clone
    puts(((x = res.images); x.is_a?(Array) ? ("[" + x.map{ |x| if x.is_a?(Hash) then '{' + x.to_h.map{ |k,v| "#{k}: #{v.is_a?(String) ? v : v.to_s}" }.join(', ') + '}' else x.to_s end }.join(' ') + "]") : x.to_s))
    puts(((x = res.labels); x.is_a?(Array) ? ("[" + x.map{ |x| if x.is_a?(Hash) then '{' + x.to_h.map{ |k,v| "#{k}: #{v.is_a?(String) ? v : v.to_s}" }.join(', ') + '}' else x.to_s end }.join(' ') + "]") : x.to_s))
    res = next_batch(ds, 2).clone
    ds = res.dataset.clone
    puts(((x = res.images); x.is_a?(Array) ? ("[" + x.map{ |x| if x.is_a?(Hash) then '{' + x.to_h.map{ |k,v| "#{k}: #{v.is_a?(String) ? v : v.to_s}" }.join(', ') + '}' else x.to_s end }.join(' ') + "]") : x.to_s))
    puts(((x = res.labels); x.is_a?(Array) ? ("[" + x.map{ |x| if x.is_a?(Hash) then '{' + x.to_h.map{ |k,v| "#{k}: #{v.is_a?(String) ? v : v.to_s}" }.join(', ') + '}' else x.to_s end }.join(' ') + "]") : x.to_s))
  end
  DataSet = Struct.new(:images, :labels, :num_examples, :index_in_epoch, :epochs_completed, keyword_init: true)
  Datasets = Struct.new(:train, :validation, :test_ds, keyword_init: true)
  BatchResult = Struct.new(:dataset, :images, :labels, keyword_init: true)
  main()
end_time = _now()
end_mem = _mem()
result = {"duration_us" => ((end_time - start) / 1000), "memory_bytes" => (end_mem - start_mem), "name" => "main"}
puts(JSON.pretty_generate(result))
