(* Generated by Mochi transpiler v0.10.67 on 2025-08-16 21:23 +0700 *)


let rec __is_list v =
  let open Obj in
  let r = repr v in
  if is_int r then false
  else
    match tag r with
    | 0 ->
        let t = field r 1 in
        if is_int t then (magic t : int) = 0 else __is_list (magic t)
    | _ -> false

let rec __show v =
  let open Obj in
  let r = repr v in
  if __is_list v then
    __show_list (Obj.magic v)
  else if is_int r then
    string_of_int (magic v : int)
  else
    match tag r with
    | 0 -> __show_tuple (Obj.magic v)
    | 252 -> Printf.sprintf "'%s'" (String.escaped (magic v : string))
    | 253 -> string_of_float (magic v)
    | _ -> "<value>"
and __show_list l =
  match l with
  | [] -> "[]"
  | _ -> "[" ^ String.concat " " (List.map __show l) ^ "]"
and __show_tuple t =
  let open Obj in
  let r = repr t in
  let rec aux i =
    if i >= size r then []
    else __show (Obj.obj (field r i)) :: aux (i + 1)
  in
  "(" ^ String.concat ", " (aux 0) ^ ")"
and __str v =
  let open Obj in
  let r = repr v in
  if __is_list v then
    __str_list (Obj.magic v)
  else if is_int r then
    string_of_int (magic v : int)
  else
    match tag r with
    | 0 -> __str_tuple (Obj.magic v)
    | 252 -> (magic v : string)
    | 253 -> string_of_float (magic v)
    | _ -> "<value>"
and __str_list l =
  match l with
  | [] -> "[]"
  | _ -> "[" ^ String.concat " " (List.map __str l) ^ "]"
and __str_tuple t =
  let open Obj in
  let r = repr t in
  let rec aux i =
    if i >= size r then []
    else __str (Obj.obj (field r i)) :: aux (i + 1)
  in
  "(" ^ String.concat ", " (aux 0) ^ ")"


let nil = Obj.repr 0


let _now_seed = ref 0
let _now_seeded = ref false

let _now () =
  if not !_now_seeded then (
    match Sys.getenv_opt "MOCHI_NOW_SEED" with
    | Some s -> (try _now_seed := int_of_string s; _now_seeded := true with _ -> ())
    | None -> ()
  );
  if !_now_seeded then (
    _now_seed := (!(_now_seed) * 1664525 + 1013904223) mod 2147483647;
    !_now_seed
  ) else int_of_float (Sys.time () *. 1000000000.)


let _mem () =
  int_of_float (Gc.allocated_bytes ())

exception Break
exception Continue

exception Return

let rec dot x y =
  let __ret = ref 0.0 in
  (try
  let x = (Obj.magic x : float list) in
  let y = (Obj.magic y : float list) in
  let sum = ref (0.0) in
  let i = ref (0) in
  (try while (!i < List.length (x)) do
    try
  sum := (!sum +. ((let __l = x in let __i = !i in if __i < 0 then 0. else match List.nth_opt __l __i with Some v -> (Obj.magic v : float) | None -> 0.) *. (let __l = y in let __i = !i in if __i < 0 then 0. else match List.nth_opt __l __i with Some v -> (Obj.magic v : float) | None -> 0.)));
  i := (!i + 1);
    with Continue -> ()
  done with Break -> ());
  __ret := (Obj.magic (!sum) : float); raise Return
  with Return -> !__ret)

and run_steep_gradient_descent data_x data_y len_data alpha theta =
  let __ret = ref ([] : (float) list) in
  (try
  let data_x = (Obj.magic data_x : float list list) in
  let data_y = (Obj.magic data_y : float list) in
  let len_data = (Obj.magic len_data : int) in
  let alpha = (Obj.magic alpha : float) in
  let theta = (Obj.magic theta : float list) in
  let gradients = ref (([] : (float) list)) in
  let j = ref (0) in
  (try while (!j < List.length (theta)) do
    try
  gradients := (Obj.magic ((List.append (!gradients) [(Obj.magic (0.0) : float)])) : float list);
  j := (!j + 1);
    with Continue -> ()
  done with Break -> ());
  let i = ref (0) in
  (try while (!i < len_data) do
    try
  let prediction = dot (theta) ((let __l = data_x in let __i = !i in if __i < 0 then [] else match List.nth_opt __l __i with Some v -> v | None -> [])) in
  let error = (prediction -. (let __l = data_y in let __i = !i in if __i < 0 then 0. else match List.nth_opt __l __i with Some v -> (Obj.magic v : float) | None -> 0.)) in
  let k = ref (0) in
  (try while (!k < List.length (theta)) do
    try
  gradients := (List.mapi (fun __i __x -> if __i = !k then ((let __l = !gradients in let __i = !k in if __i < 0 then 0. else match List.nth_opt __l __i with Some v -> (Obj.magic v : float) | None -> 0.) +. (error *. (let __l = (let __l = data_x in let __i = !i in if __i < 0 then [] else match List.nth_opt __l __i with Some v -> v | None -> []) in let __i = !k in if __i < 0 then 0. else match List.nth_opt __l __i with Some v -> (Obj.magic v : float) | None -> 0.))) else __x) (!gradients));
  k := (!k + 1);
    with Continue -> ()
  done with Break -> ());
  i := (!i + 1);
    with Continue -> ()
  done with Break -> ());
  let t = ref (([] : (float) list)) in
  let g = ref (0) in
  (try while (!g < List.length (theta)) do
    try
  t := (Obj.magic ((List.append (!t) [(Obj.magic (((let __l = theta in let __i = !g in if __i < 0 then 0. else match List.nth_opt __l __i with Some v -> (Obj.magic v : float) | None -> 0.) -. ((alpha /. float_of_int (len_data)) *. (let __l = !gradients in let __i = !g in if __i < 0 then 0. else match List.nth_opt __l __i with Some v -> (Obj.magic v : float) | None -> 0.)))) : float)])) : float list);
  g := (!g + 1);
    with Continue -> ()
  done with Break -> ());
  __ret := (Obj.magic (!t) : float list); raise Return
  with Return -> !__ret)

and sum_of_square_error data_x data_y len_data theta =
  let __ret = ref 0.0 in
  (try
  let data_x = (Obj.magic data_x : float list list) in
  let data_y = (Obj.magic data_y : float list) in
  let len_data = (Obj.magic len_data : int) in
  let theta = (Obj.magic theta : float list) in
  let total = ref (0.0) in
  let i = ref (0) in
  (try while (!i < len_data) do
    try
  let prediction = dot (theta) ((let __l = data_x in let __i = !i in if __i < 0 then [] else match List.nth_opt __l __i with Some v -> v | None -> [])) in
  let diff = (prediction -. (let __l = data_y in let __i = !i in if __i < 0 then 0. else match List.nth_opt __l __i with Some v -> (Obj.magic v : float) | None -> 0.)) in
  total := (!total +. (diff *. diff));
  i := (!i + 1);
    with Continue -> ()
  done with Break -> ());
  __ret := (Obj.magic ((!total /. (2.0 *. float_of_int (len_data)))) : float); raise Return
  with Return -> !__ret)

and run_linear_regression data_x data_y =
  let __ret = ref ([] : (float) list) in
  (try
  let data_x = (Obj.magic data_x : float list list) in
  let data_y = (Obj.magic data_y : float list) in
  let iterations = 10 in
  let alpha = 0.01 in
  let no_features = List.length ((let __l = data_x in let __i = 0 in if __i < 0 then [] else match List.nth_opt __l __i with Some v -> v | None -> [])) in
  let len_data = List.length (data_x) in
  let theta = ref (([] : (float) list)) in
  let i = ref (0) in
  (try while (!i < no_features) do
    try
  theta := (Obj.magic ((List.append (!theta) [(Obj.magic (0.0) : float)])) : float list);
  i := (!i + 1);
    with Continue -> ()
  done with Break -> ());
  let iter = ref (0) in
  (try while (!iter < iterations) do
    try
  theta := (Obj.magic (run_steep_gradient_descent (data_x) (data_y) (Obj.repr (len_data)) (Obj.repr (alpha)) (!theta)) : float list);
  let error = sum_of_square_error (data_x) (data_y) (Obj.repr (len_data)) (!theta) in
  print_endline (((("At Iteration " ^ (string_of_int (Obj.magic ((!iter + 1)) : int))) ^ " - Error is ") ^ (Printf.sprintf "%.16g" (Obj.magic (error) : float))));
  iter := (!iter + 1);
    with Continue -> ()
  done with Break -> ());
  __ret := (Obj.magic (!theta) : float list); raise Return
  with Return -> !__ret)

and absf x =
  let __ret = ref 0.0 in
  (try
  let x = (Obj.magic x : float) in
  if (x < 0.0) then (
  __ret := (Obj.magic ((-.(x))) : float); raise Return
  ) else (
  __ret := (Obj.magic (x) : float); raise Return
  );
    !__ret
  with Return -> !__ret)

and mean_absolute_error predicted_y original_y =
  let __ret = ref 0.0 in
  (try
  let predicted_y = (Obj.magic predicted_y : float list) in
  let original_y = (Obj.magic original_y : float list) in
  let total = ref (0.0) in
  let i = ref (0) in
  (try while (!i < List.length (predicted_y)) do
    try
  let diff = absf (Obj.repr (((let __l = predicted_y in let __i = !i in if __i < 0 then 0. else match List.nth_opt __l __i with Some v -> (Obj.magic v : float) | None -> 0.) -. (let __l = original_y in let __i = !i in if __i < 0 then 0. else match List.nth_opt __l __i with Some v -> (Obj.magic v : float) | None -> 0.)))) in
  total := (!total +. diff);
  i := (!i + 1);
    with Continue -> ()
  done with Break -> ());
  __ret := (Obj.magic ((!total /. float_of_int (List.length (predicted_y)))) : float); raise Return
  with Return -> !__ret)


let data_x = ref ([[1.0; 1.0]; [1.0; 2.0]; [1.0; 3.0]])
let data_y = ref ([1.0; 2.0; 3.0])
let theta = ref (run_linear_regression (!data_x) (!data_y))
let () =
  let bench_mem_start = _mem () in
  let bench_start = _now () in
  print_endline ("Resultant Feature vector :");
  let i = ref (0) in
  (try while (!i < List.length (!theta)) do
    try
  print_endline ((Printf.sprintf "%.16g" (Obj.magic ((let __l = !theta in let __i = !i in if __i < 0 then 0. else match List.nth_opt __l __i with Some v -> (Obj.magic v : float) | None -> 0.)) : float)));
  i := (!i + 1);
    with Continue -> ()
  done with Break -> ());
  let predicted_y = ref ([3.0; (-.(0.5)); 2.0; 7.0]) in
  let original_y = ref ([2.5; 0.0; 2.0; 8.0]) in
  let mae = mean_absolute_error (!predicted_y) (!original_y) in
  print_endline (("Mean Absolute Error : " ^ (Printf.sprintf "%.16g" (Obj.magic (mae) : float))));
  let bench_finish = _now () in
  let bench_mem_end = _mem () in
  let bench_dur = (bench_finish - bench_start) / 1000 in
  let bench_mem_bytes = Stdlib.max 0 (bench_mem_end - bench_mem_start) in
  Printf.printf "{\n  \"duration_us\": %d,\n  \"memory_bytes\": %d,\n  \"name\": \"%s\"\n}\n" bench_dur bench_mem_bytes "main";
  ()
