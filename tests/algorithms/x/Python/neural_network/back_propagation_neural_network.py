# Code generated by Mochi transpiler.
# Version 0.10.61, generated on 2025-08-08 17:42 +0700
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Dict
import dataclasses

import sys
sys.set_int_max_str_digits(0)
import os
if os.path.dirname(__file__) in sys.path:
    sys.path.remove(os.path.dirname(__file__))


def panic(msg):
    raise RuntimeError(msg)


def _append(lst, v):
    return (lst or []) + [v]


def _set_index(lst, idx, val):
    if lst is None:
        lst = []
    if idx >= len(lst):
        lst.extend([None] * (idx - len(lst) + 1))
    lst[idx] = val
    return lst

seed = 1
def rand():
    global seed
    seed = (seed * 1103515245 + 12345) % 2147483648
    return seed
def random():
    return (1.0 * rand()) / 2.147483648e+09
def expApprox(x):
    y = x
    is_neg = False
    if x < 0.0:
        is_neg = True
        y = -x
    term = 1.0
    sum = 1.0
    n = 1
    while n < 30:
        term = term * y / (float(n))
        sum = sum + term
        n = n + 1
    if is_neg:
        return 1.0 / sum
    return sum
def sigmoid(z):
    return 1.0 / (1.0 + expApprox(-z))
def sigmoid_vec(v):
    res = []
    i = 0
    while i < len(v):
        res = _append(res, sigmoid(v[i]))
        i = i + 1
    return res
def sigmoid_derivative(out):
    res = []
    i = 0
    while i < len(out):
        val = out[i]
        res = _append(res, val * (1.0 - val))
        i = i + 1
    return res
@dataclass
class Layer:
    units: int
    weight: [[float]]
    bias: [float]
    output: [float]
    xdata: [float]
    learn_rate: float

def random_vector(n):
    v = []
    i = 0
    while i < n:
        v = _append(v, random() - 0.5)
        i = i + 1
    return v
def random_matrix(r, c):
    m = []
    i = 0
    while i < r:
        m = _append(m, random_vector(c))
        i = i + 1
    return m
def matvec(mat, vec):
    res = []
    i = 0
    while i < len(mat):
        s = 0.0
        j = 0
        while j < len(vec):
            s = s + mat[i][j] * vec[j]
            j = j + 1
        res = _append(res, s)
        i = i + 1
    return res
def matTvec(mat, vec):
    cols = len(mat[0])
    res = []
    j = 0
    while j < cols:
        s = 0.0
        i = 0
        while i < len(mat):
            s = s + mat[i][j] * vec[i]
            i = i + 1
        res = _append(res, s)
        j = j + 1
    return res
def vec_sub(a, b):
    res = []
    i = 0
    while i < len(a):
        res = _append(res, a[i] - b[i])
        i = i + 1
    return res
def vec_mul(a, b):
    res = []
    i = 0
    while i < len(a):
        res = _append(res, a[i] * b[i])
        i = i + 1
    return res
def vec_scalar_mul(v, s):
    res = []
    i = 0
    while i < len(v):
        res = _append(res, v[i] * s)
        i = i + 1
    return res
def outer(a, b):
    res = []
    i = 0
    while i < len(a):
        row = []
        j = 0
        while j < len(b):
            row = _append(row, a[i] * b[j])
            j = j + 1
        res = _append(res, row)
        i = i + 1
    return res
def mat_scalar_mul(mat, s):
    res = []
    i = 0
    while i < len(mat):
        row = []
        j = 0
        while j < len(mat[i]):
            row = _append(row, mat[i][j] * s)
            j = j + 1
        res = _append(res, row)
        i = i + 1
    return res
def mat_sub(a, b):
    res = []
    i = 0
    while i < len(a):
        row = []
        j = 0
        while j < len(a[i]):
            row = _append(row, a[i][j] - b[i][j])
            j = j + 1
        res = _append(res, row)
        i = i + 1
    return res
def init_layer(units, back_units, lr):
    return Layer(units=units, weight=random_matrix(units, back_units), bias=random_vector(units), output=[], xdata=[], learn_rate=lr)
def forward(layers, x):
    data = x
    i = 0
    while i < len(layers):
        layer = layers[i]
        layer.xdata = data
        if i == 0:
            layer.output = data
        else:
            z = vec_sub(matvec(layer.weight, data), layer.bias)
            layer.output = sigmoid_vec(z)
            data = layer.output
        layers[i] = layer
        i = i + 1
    return layers
def backward(layers, grad):
    g = grad
    i = len(layers) - 1
    while i > 0:
        layer = layers[i]
        deriv = sigmoid_derivative(layer.output)
        delta = vec_mul(g, deriv)
        grad_w = outer(delta, layer.xdata)
        layer.weight = mat_sub(layer.weight, mat_scalar_mul(grad_w, layer.learn_rate))
        layer.bias = vec_sub(layer.bias, vec_scalar_mul(delta, layer.learn_rate))
        g = matTvec(layer.weight, delta)
        layers[i] = layer
        i = i - 1
    return layers
def calc_loss(y, yhat):
    s = 0.0
    i = 0
    while i < len(y):
        d = y[i] - yhat[i]
        s = s + d * d
        i = i + 1
    return s
def calc_gradient(y, yhat):
    g = []
    i = 0
    while i < len(y):
        g = _append(g, 2.0 * (yhat[i] - y[i]))
        i = i + 1
    return g
def train(layers, xdata, ydata, rounds, acc):
    r = 0
    while r < rounds:
        i = 0
        while i < len(xdata):
            layers = forward(layers, xdata[i])
            out = layers[len(layers) - 1].output
            grad = calc_gradient(ydata[i], out)
            layers = backward(layers, grad)
            i = i + 1
        r = r + 1
    return 0.0
@dataclass
class Data:
    x: [[float]]
    y: [[float]]

def create_data():
    x = []
    i = 0
    while i < 10:
        x = _append(x, random_vector(10))
        i = i + 1
    y = [[0.8, 0.4], [0.4, 0.3], [0.34, 0.45], [0.67, 0.32], [0.88, 0.67], [0.78, 0.77], [0.55, 0.66], [0.55, 0.43], [0.54, 0.1], [0.1, 0.5]]
    return Data(x=x, y=y)
def main():
    data = create_data()
    x = data.x
    y = data.y
    layers = []
    layers = _append(layers, init_layer(10, 0, 0.3))
    layers = _append(layers, init_layer(20, 10, 0.3))
    layers = _append(layers, init_layer(30, 20, 0.3))
    layers = _append(layers, init_layer(2, 30, 0.3))
    final_mse = train(layers, x, y, 100, 0.01)
    print(final_mse)
main()
