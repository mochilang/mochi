# Code generated by Mochi transpiler.
# Version 0.10.59, generated on 2025-08-06 22:12 +0700
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Dict
import dataclasses

import sys
sys.set_int_max_str_digits(0)
import os
if os.path.dirname(__file__) in sys.path:
    sys.path.remove(os.path.dirname(__file__))


def _append(lst, v):
    if lst is None:
        return [v]
    return lst + [v]

@dataclass
class Token:
    offset: int
    length: int
    indicator: str

def token_to_string(t):
    return "(" + str(t.offset) + ", " + str(t.length) + ", " + t.indicator + ")"
def tokens_to_string(ts):
    res = "["
    i = 0
    while i < len(ts):
        res = res + token_to_string(ts[i])
        if i < len(ts) - 1:
            res = res + ", "
        i = i + 1
    return res + "]"
def match_length_from_index(text, window, text_index, window_index):
    if text_index >= len(text) or window_index >= len(window):
        return 0
    tc = text[text_index:text_index + 1]
    wc = window[window_index:window_index + 1]
    if tc != wc:
        return 0
    return 1 + match_length_from_index(text, window + tc, text_index + 1, window_index + 1)
def find_encoding_token(text, search_buffer):
    if len(text) == 0:
        panic("We need some text to work with.")
    length = 0
    offset = 0
    if len(search_buffer) == 0:
        return Token(offset=offset, length=length, indicator=text[0:1])
    i = 0
    while i < len(search_buffer):
        ch = search_buffer[i:i + 1]
        found_offset = len(search_buffer) - i
        if ch == text[0:1]:
            found_length = match_length_from_index(text, search_buffer, 0, i)
            if found_length >= length:
                offset = found_offset
                length = found_length
        i = i + 1
    return Token(offset=offset, length=length, indicator=text[length:length + 1])
def lz77_compress(text, window_size, lookahead):
    search_buffer_size = window_size - lookahead
    output = []
    search_buffer = ""
    remaining = text
    while len(remaining) > 0:
        token = find_encoding_token(remaining, search_buffer)
        add_len = token.length + 1
        search_buffer = search_buffer + remaining[0:add_len]
        if len(search_buffer) > search_buffer_size:
            search_buffer = search_buffer[len(search_buffer) - search_buffer_size:len(search_buffer)]
        remaining = remaining[add_len:len(remaining)]
        output = _append(output, token)
    return output
def lz77_decompress(tokens):
    output = ""
    for t in tokens:
        i = 0
        while i < t.length:
            output = output + output[len(output) - t.offset:len(output) - t.offset + 1]
            i = i + 1
        output = output + t.indicator
    return output
c1 = lz77_compress("ababcbababaa", 13, 6)
print(tokens_to_string(c1))
c2 = lz77_compress("aacaacabcabaaac", 13, 6)
print(tokens_to_string(c2))
tokens_example = [Token(offset=0, length=0, indicator="c"), Token(offset=0, length=0, indicator="a"), Token(offset=0, length=0, indicator="b"), Token(offset=0, length=0, indicator="r"), Token(offset=3, length=1, indicator="c"), Token(offset=2, length=1, indicator="d"), Token(offset=7, length=4, indicator="r"), Token(offset=3, length=5, indicator="d")]
print(lz77_decompress(tokens_example))
