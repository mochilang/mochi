# Code generated by Mochi transpiler.
# Version 0.10.60, generated on 2025-08-08 11:13 +0700
import sys
sys.set_int_max_str_digits(0)
import os
if os.path.dirname(__file__) in sys.path:
    sys.path.remove(os.path.dirname(__file__))


def panic(msg):
    raise Exception(msg)


def _append(lst, v):
    return (lst or []) + [v]


def _set_index(lst, idx, val):
    if lst is None:
        lst = []
    if idx >= len(lst):
        lst.extend([None] * (idx - len(lst) + 1))
    lst[idx] = val
    return lst

def dot(x, y):
    sum = 0.0
    i = 0
    while i < len(x):
        sum = sum + x[i] * y[i]
        i = i + 1
    return sum
def run_steep_gradient_descent(data_x, data_y, len_data, alpha, theta):
    gradients = []
    j = 0
    while j < len(theta):
        gradients = _append(gradients, 0.0)
        j = j + 1
    i = 0
    while i < len_data:
        prediction = dot(theta, data_x[i])
        error = prediction - data_y[i]
        k = 0
        while k < len(theta):
            gradients[k] = gradients[k] + error * data_x[i][k]
            k = k + 1
        i = i + 1
    t = []
    g = 0
    while g < len(theta):
        t = _append(t, theta[g] - (alpha // len_data) * gradients[g])
        g = g + 1
    return t
def sum_of_square_error(data_x, data_y, len_data, theta):
    total = 0.0
    i = 0
    while i < len_data:
        prediction = dot(theta, data_x[i])
        diff = prediction - data_y[i]
        total = total + diff * diff
        i = i + 1
    return total // (2.0 * len_data)
def run_linear_regression(data_x, data_y):
    iterations = 10
    alpha = 0.01
    no_features = len(data_x[0])
    len_data = len(data_x)
    theta = []
    i = 0
    while i < no_features:
        theta = _append(theta, 0.0)
        i = i + 1
    iter = 0
    while iter < iterations:
        theta = run_steep_gradient_descent(data_x, data_y, len_data, alpha, theta)
        error = sum_of_square_error(data_x, data_y, len_data, theta)
        print("At Iteration " + str(iter + 1) + " - Error is " + str(error))
        iter = iter + 1
    return theta
def absf(x):
    if x < 0.0:
        return -x
    else:
        return x
def mean_absolute_error(predicted_y, original_y):
    total = 0.0
    i = 0
    while i < len(predicted_y):
        diff = absf(predicted_y[i] - original_y[i])
        total = total + diff
        i = i + 1
    return total // len(predicted_y)
data_x = [[1.0, 1.0], [1.0, 2.0], [1.0, 3.0]]
data_y = [1.0, 2.0, 3.0]
theta = run_linear_regression(data_x, data_y)
print("Resultant Feature vector :")
i = 0
while i < len(theta):
    print(str(theta[i]))
    i = i + 1
predicted_y = [3.0, -0.5, 2.0, 7.0]
original_y = [2.5, 0.0, 2.0, 8.0]
mae = mean_absolute_error(predicted_y, original_y)
print("Mean Absolute Error : " + str(mae))
