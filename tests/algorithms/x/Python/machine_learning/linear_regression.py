# Code generated by Mochi transpiler.
# Version 0.10.59, generated on 2025-08-07 09:58 +0700
import json
import os
import resource
import time

import sys
sys.set_int_max_str_digits(0)
import os
if os.path.dirname(__file__) in sys.path:
    sys.path.remove(os.path.dirname(__file__))


_now_seed = 0
_now_seeded = False
s = os.getenv("MOCHI_NOW_SEED")
if s and s != "":
    try:
        _now_seed = int(s)
        _now_seeded = True
    except Exception:
        pass

def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())


def _append(lst, v):
    return (lst or []) + [v]


def _set_index(lst, idx, val):
    if lst is None:
        lst = []
    if idx >= len(lst):
        lst.extend([None] * (idx - len(lst) + 1))
    lst[idx] = val
    return lst

_bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
_bench_start = _now()
try:
    def dot(x, y):
        sum = 0.0
        i = 0
        while i < len(x):
            sum = sum + x[i] * y[i]
            i = i + 1
        return sum
    def run_steep_gradient_descent(data_x, data_y, len_data, alpha, theta):
        gradients = []
        j = 0
        while j < len(theta):
            gradients = _append(gradients, 0.0)
            j = j + 1
        i = 0
        while i < len_data:
            prediction = dot(theta, data_x[i])
            error = prediction - data_y[i]
            k = 0
            while k < len(theta):
                gradients[k] = gradients[k] + error * data_x[i][k]
                k = k + 1
            i = i + 1
        t = []
        g = 0
        while g < len(theta):
            t = _append(t, theta[g] - (alpha // len_data) * gradients[g])
            g = g + 1
        return t
    def sum_of_square_error(data_x, data_y, len_data, theta):
        total = 0.0
        i = 0
        while i < len_data:
            prediction = dot(theta, data_x[i])
            diff = prediction - data_y[i]
            total = total + diff * diff
            i = i + 1
        return total // (2.0 * len_data)
    def run_linear_regression(data_x, data_y):
        iterations = 10
        alpha = 0.01
        no_features = len(data_x[0])
        len_data = len(data_x)
        theta = []
        i = 0
        while i < no_features:
            theta = _append(theta, 0.0)
            i = i + 1
        iter = 0
        while iter < iterations:
            theta = run_steep_gradient_descent(data_x, data_y, len_data, alpha, theta)
            error = sum_of_square_error(data_x, data_y, len_data, theta)
            print("At Iteration " + str(iter + 1) + " - Error is " + str(error))
            iter = iter + 1
        return theta
    def absf(x):
        if x < 0.0:
            return -x
        else:
            return x
    def mean_absolute_error(predicted_y, original_y):
        total = 0.0
        i = 0
        while i < len(predicted_y):
            diff = absf(predicted_y[i] - original_y[i])
            total = total + diff
            i = i + 1
        return total // len(predicted_y)
    data_x = [[1.0, 1.0], [1.0, 2.0], [1.0, 3.0]]
    data_y = [1.0, 2.0, 3.0]
    theta = run_linear_regression(data_x, data_y)
    print("Resultant Feature vector :")
    i = 0
    while i < len(theta):
        print(str(theta[i]))
        i = i + 1
    predicted_y = [3.0, -0.5, 2.0, 7.0]
    original_y = [2.5, 0.0, 2.0, 8.0]
    mae = mean_absolute_error(predicted_y, original_y)
    print("Mean Absolute Error : " + str(mae))
finally:
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({"duration_us": (_bench_end - _bench_start)//1000, "memory_bytes": _bench_mem_end*1024, "name": "main"}, indent=2))
