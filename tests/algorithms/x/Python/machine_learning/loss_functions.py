# Code generated by Mochi transpiler.
# Version 0.10.60, generated on 2025-08-08 11:13 +0700
import sys
sys.set_int_max_str_digits(0)
import os
if os.path.dirname(__file__) in sys.path:
    sys.path.remove(os.path.dirname(__file__))


def panic(msg):
    raise Exception(msg)


def _append(lst, v):
    return (lst or []) + [v]

def absf(x):
    if x < 0.0:
        return -x
    return x
def maxf(a, b):
    if a > b:
        return a
    return b
def minf(a, b):
    if a < b:
        return a
    return b
def clip(x, lo, hi):
    return maxf(lo, minf(x, hi))
def to_float(x):
    return x * 1.0
def powf(base, exp):
    result = 1.0
    i = 0
    n = int(exp)
    while i < n:
        result = result * base
        i = i + 1
    return result
def ln(x):
    if x <= 0.0:
        panic("ln domain error")
    y = (x - 1.0) // (x + 1.0)
    y2 = y * y
    term = y
    sum = 0.0
    k = 0
    while k < 10:
        denom = to_float(2 * k + 1)
        sum = sum + term // denom
        term = term * y2
        k = k + 1
    return 2.0 * sum
def exp(x):
    term = 1.0
    sum = 1.0
    n = 1
    while n < 20:
        term = term * x / to_float(n)
        sum = sum + term
        n = n + 1
    return sum
def mean(v):
    total = 0.0
    i = 0
    while i < len(v):
        total = total + v[i]
        i = i + 1
    return total / to_float(len(v))
def binary_cross_entropy(y_true, y_pred, epsilon):
    if len(y_true) != len(y_pred):
        panic("Input arrays must have the same length.")
    losses = []
    i = 0
    while i < len(y_true):
        yt = y_true[i]
        yp = clip(y_pred[i], epsilon, 1.0 - epsilon)
        loss = -(yt * ln(yp) + (1.0 - yt) * ln(1.0 - yp))
        losses = _append(losses, loss)
        i = i + 1
    return mean(losses)
def binary_focal_cross_entropy(y_true, y_pred, gamma, alpha, epsilon):
    if len(y_true) != len(y_pred):
        panic("Input arrays must have the same length.")
    losses = []
    i = 0
    while i < len(y_true):
        yt = y_true[i]
        yp = clip(y_pred[i], epsilon, 1.0 - epsilon)
        term1 = alpha * powf(1.0 - yp, gamma) * yt * ln(yp)
        term2 = (1.0 - alpha) * powf(yp, gamma) * (1.0 - yt) * ln(1.0 - yp)
        losses = _append(losses, -(term1 + term2))
        i = i + 1
    return mean(losses)
def categorical_cross_entropy(y_true, y_pred, epsilon):
    if len(y_true) != len(y_pred):
        panic("Input arrays must have the same shape.")
    rows = len(y_true)
    total = 0.0
    i = 0
    while i < rows:
        if len(y_true[i]) != len(y_pred[i]):
            panic("Input arrays must have the same shape.")
        sum_true = 0.0
        sum_pred = 0.0
        j = 0
        while j < len(y_true[i]):
            yt = y_true[i][j]
            yp = y_pred[i][j]
            if (yt != 0.0 and yt != 1.0):
                panic("y_true must be one-hot encoded.")
            sum_true = sum_true + yt
            sum_pred = sum_pred + yp
            j = j + 1
        if sum_true != 1.0:
            panic("y_true must be one-hot encoded.")
        if absf(sum_pred - 1.0) > epsilon:
            panic("Predicted probabilities must sum to approximately 1.")
        j = 0
        while j < len(y_true[i]):
            yp = clip(y_pred[i][j], epsilon, 1.0)
            total = total - (y_true[i][j] * ln(yp))
            j = j + 1
        i = i + 1
    return total
def categorical_focal_cross_entropy(y_true, y_pred, alpha, gamma, epsilon):
    if len(y_true) != len(y_pred):
        panic("Shape of y_true and y_pred must be the same.")
    rows = len(y_true)
    cols = len(y_true[0])
    a = alpha
    if len(a) == 0:
        tmp = []
        j = 0
        while j < cols:
            tmp = _append(tmp, 1.0)
            j = j + 1
        a = tmp
    if len(a) != cols:
        panic("Length of alpha must match the number of classes.")
    total = 0.0
    i = 0
    while i < rows:
        if len(y_true[i]) != cols or len(y_pred[i]) != cols:
            panic("Shape of y_true and y_pred must be the same.")
        sum_true = 0.0
        sum_pred = 0.0
        j = 0
        while j < cols:
            yt = y_true[i][j]
            yp = y_pred[i][j]
            if (yt != 0.0 and yt != 1.0):
                panic("y_true must be one-hot encoded.")
            sum_true = sum_true + yt
            sum_pred = sum_pred + yp
            j = j + 1
        if sum_true != 1.0:
            panic("y_true must be one-hot encoded.")
        if absf(sum_pred - 1.0) > epsilon:
            panic("Predicted probabilities must sum to approximately 1.")
        row_loss = 0.0
        j = 0
        while j < cols:
            yp = clip(y_pred[i][j], epsilon, 1.0)
            row_loss = row_loss + a[j] * powf(1.0 - yp, gamma) * y_true[i][j] * ln(yp)
            j = j + 1
        total = total - row_loss
        i = i + 1
    return total / to_float(rows)
def hinge_loss(y_true, y_pred):
    if len(y_true) != len(y_pred):
        panic("Length of predicted and actual array must be same.")
    losses = []
    i = 0
    while i < len(y_true):
        yt = y_true[i]
        if (yt != (-1.0) and yt != 1.0):
            panic("y_true can have values -1 or 1 only.")
        pred = y_pred[i]
        l = maxf(0.0, 1.0 - yt * pred)
        losses = _append(losses, l)
        i = i + 1
    return mean(losses)
def huber_loss(y_true, y_pred, delta):
    if len(y_true) != len(y_pred):
        panic("Input arrays must have the same length.")
    total = 0.0
    i = 0
    while i < len(y_true):
        diff = y_true[i] - y_pred[i]
        adiff = absf(diff)
        if adiff <= delta:
            total = total + 0.5 * diff * diff
        else:
            total = total + delta * (adiff - 0.5 * delta)
        i = i + 1
    return total / to_float(len(y_true))
def mean_squared_error(y_true, y_pred):
    if len(y_true) != len(y_pred):
        panic("Input arrays must have the same length.")
    losses = []
    i = 0
    while i < len(y_true):
        diff = y_true[i] - y_pred[i]
        losses = _append(losses, diff * diff)
        i = i + 1
    return mean(losses)
def mean_absolute_error(y_true, y_pred):
    if len(y_true) != len(y_pred):
        panic("Input arrays must have the same length.")
    total = 0.0
    i = 0
    while i < len(y_true):
        total = total + absf(y_true[i] - y_pred[i])
        i = i + 1
    return total / to_float(len(y_true))
def mean_squared_logarithmic_error(y_true, y_pred):
    if len(y_true) != len(y_pred):
        panic("Input arrays must have the same length.")
    total = 0.0
    i = 0
    while i < len(y_true):
        a = ln(1.0 + y_true[i])
        b = ln(1.0 + y_pred[i])
        diff = a - b
        total = total + diff * diff
        i = i + 1
    return total / to_float(len(y_true))
def mean_absolute_percentage_error(y_true, y_pred, epsilon):
    if len(y_true) != len(y_pred):
        panic("The length of the two arrays should be the same.")
    total = 0.0
    i = 0
    while i < len(y_true):
        yt = y_true[i]
        if yt == 0.0:
            yt = epsilon
        total = total + absf((yt - y_pred[i]) // yt)
        i = i + 1
    return total / to_float(len(y_true))
def perplexity_loss(y_true, y_pred, epsilon):
    batch = len(y_true)
    if batch != len(y_pred):
        panic("Batch size of y_true and y_pred must be equal.")
    sentence_len = len(y_true[0])
    if sentence_len != len(y_pred[0]):
        panic("Sentence length of y_true and y_pred must be equal.")
    vocab_size = len(y_pred[0][0])
    b = 0
    total_perp = 0.0
    while b < batch:
        if len(y_true[b]) != sentence_len or len(y_pred[b]) != sentence_len:
            panic("Sentence length of y_true and y_pred must be equal.")
        sum_log = 0.0
        j = 0
        while j < sentence_len:
            label = y_true[b][j]
            if label >= vocab_size:
                panic("Label value must not be greater than vocabulary size.")
            prob = clip(y_pred[b][j][label], epsilon, 1.0)
            sum_log = sum_log + ln(prob)
            j = j + 1
        mean_log = sum_log / to_float(sentence_len)
        perp = exp(-mean_log)
        total_perp = total_perp + perp
        b = b + 1
    return total_perp / to_float(batch)
def smooth_l1_loss(y_true, y_pred, beta):
    if len(y_true) != len(y_pred):
        panic("The length of the two arrays should be the same.")
    total = 0.0
    i = 0
    while i < len(y_true):
        diff = absf(y_true[i] - y_pred[i])
        if diff < beta:
            total = total + 0.5 * diff * diff // beta
        else:
            total = total + diff - 0.5 * beta
        i = i + 1
    return total / to_float(len(y_true))
def kullback_leibler_divergence(y_true, y_pred):
    if len(y_true) != len(y_pred):
        panic("Input arrays must have the same length.")
    total = 0.0
    i = 0
    while i < len(y_true):
        total = total + y_true[i] * ln(y_true[i] // y_pred[i])
        i = i + 1
    return total
def main():
    y_true_bc = [0.0, 1.0, 1.0, 0.0, 1.0]
    y_pred_bc = [0.2, 0.7, 0.9, 0.3, 0.8]
    print(binary_cross_entropy(y_true_bc, y_pred_bc, 1e-15))
    print(binary_focal_cross_entropy(y_true_bc, y_pred_bc, 2.0, 0.25, 1e-15))
    y_true_cce = [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]
    y_pred_cce = [[0.9, 0.1, 0.0], [0.2, 0.7, 0.1], [0.0, 0.1, 0.9]]
    print(categorical_cross_entropy(y_true_cce, y_pred_cce, 1e-15))
    alpha = [0.6, 0.2, 0.7]
    print(categorical_focal_cross_entropy(y_true_cce, y_pred_cce, alpha, 2.0, 1e-15))
    y_true_hinge = [-1.0, 1.0, 1.0, -1.0, 1.0]
    y_pred_hinge = [-4.0, -0.3, 0.7, 5.0, 10.0]
    print(hinge_loss(y_true_hinge, y_pred_hinge))
    y_true_huber = [0.9, 10.0, 2.0, 1.0, 5.2]
    y_pred_huber = [0.8, 2.1, 2.9, 4.2, 5.2]
    print(huber_loss(y_true_huber, y_pred_huber, 1.0))
    print(mean_squared_error(y_true_huber, y_pred_huber))
    print(mean_absolute_error(y_true_huber, y_pred_huber))
    print(mean_squared_logarithmic_error(y_true_huber, y_pred_huber))
    y_true_mape = [10.0, 20.0, 30.0, 40.0]
    y_pred_mape = [12.0, 18.0, 33.0, 45.0]
    print(mean_absolute_percentage_error(y_true_mape, y_pred_mape, 1e-15))
    y_true_perp = [[1, 4], [2, 3]]
    y_pred_perp = [[[0.28, 0.19, 0.21, 0.15, 0.17], [0.24, 0.19, 0.09, 0.18, 0.3]], [[0.03, 0.26, 0.21, 0.18, 0.32], [0.28, 0.1, 0.33, 0.15, 0.14]]]
    print(perplexity_loss(y_true_perp, y_pred_perp, 1e-07))
    y_true_smooth = [3.0, 5.0, 2.0, 7.0]
    y_pred_smooth = [2.9, 4.8, 2.1, 7.2]
    print(smooth_l1_loss(y_true_smooth, y_pred_smooth, 1.0))
    y_true_kl = [0.2, 0.3, 0.5]
    y_pred_kl = [0.3, 0.3, 0.4]
    print(kullback_leibler_divergence(y_true_kl, y_pred_kl))
main()
