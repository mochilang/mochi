# Code generated by Mochi transpiler.
# Version 0.10.66, generated on 2025-08-16 11:48 +0700
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Dict
import dataclasses

import sys
if hasattr(sys, "set_int_max_str_digits"):
    sys.set_int_max_str_digits(0)
sys.setrecursionlimit(1000000)
import os
if os.path.dirname(__file__) in sys.path:
    sys.path.remove(os.path.dirname(__file__))


def _append(lst, v):
    if lst is None:
        lst = []
    return lst + [v]


def _str(v):
    if isinstance(v, float):
        if abs(v - round(v)) < 1e-9:
            return str(float(round(v)))
        return format(v, ".15g")
    return str(v)

@dataclass
class DataPoint:
    x: [float]
    y: float

def absf(x):
    if x < 0.0:
        return -x
    return x
def hypothesis_value(input_, params):
    value = params[0]
    i = 0
    while i < len(input_):
        value = value + input_[i] * params[i + 1]
        i = i + 1
    return value
def calc_error(dp, params):
    return hypothesis_value(dp.x, params) - dp.y
def summation_of_cost_derivative(index, params, data):
    sum_ = 0.0
    i = 0
    while i < len(data):
        dp = data[i]
        e = calc_error(dp, params)
        if index == (-1):
            sum_ = sum_ + e
        else:
            sum_ = sum_ + e * dp.x[index]
        i = i + 1
    return sum_
def get_cost_derivative(index, params, data):
    return summation_of_cost_derivative(index, params, data) / (float(len(data)))
def allclose(a, b, atol, rtol):
    i = 0
    while i < len(a):
        diff = absf(a[i] - b[i])
        limit = atol + rtol * absf(b[i])
        if diff > limit:
            return False
        i = i + 1
    return True
def run_gradient_descent(train_data, initial_params):
    learning_rate = 0.009
    absolute_error_limit = 2e-06
    relative_error_limit = 0.0
    j = 0
    params = initial_params
    while True:
        j = j + 1
        temp = []
        i = 0
        while i < len(params):
            deriv = get_cost_derivative(i - 1, params, train_data)
            temp = _append(temp, params[i] - learning_rate * deriv)
            i = i + 1
        if allclose(params, temp, absolute_error_limit, relative_error_limit):
            print("Number of iterations:" + _str(j))
            break
        params = temp
    return params
def test_gradient_descent(test_data, params):
    i = 0
    while i < len(test_data):
        dp = test_data[i]
        print("Actual output value:" + _str(dp.y))
        print("Hypothesis output:" + _str(hypothesis_value(dp.x, params)))
        i = i + 1
train_data = [DataPoint(x=[5.0, 2.0, 3.0], y=15.0), DataPoint(x=[6.0, 5.0, 9.0], y=25.0), DataPoint(x=[11.0, 12.0, 13.0], y=41.0), DataPoint(x=[1.0, 1.0, 1.0], y=8.0), DataPoint(x=[11.0, 12.0, 13.0], y=41.0)]
test_data = [DataPoint(x=[515.0, 22.0, 13.0], y=555.0), DataPoint(x=[61.0, 35.0, 49.0], y=150.0)]
parameter_vector = [2.0, 4.0, 1.0, 5.0]
parameter_vector = run_gradient_descent(train_data, parameter_vector)
print("\nTesting gradient descent for a linear hypothesis function.\n")
test_gradient_descent(test_data, parameter_vector)

