# Code generated by Mochi transpiler.
# Version 0.10.60, generated on 2025-08-08 11:13 +0700
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Dict
import dataclasses

import sys
sys.set_int_max_str_digits(0)
import os
if os.path.dirname(__file__) in sys.path:
    sys.path.remove(os.path.dirname(__file__))


def panic(msg):
    raise Exception(msg)


def _append(lst, v):
    return (lst or []) + [v]

@dataclass
class DataPoint:
    x: [float]
    y: float

def absf(x):
    if x < 0.0:
        return -x
    return x
def hypothesis_value(input, params):
    value = params[0]
    i = 0
    while i < len(input):
        value = value + input[i] * params[i + 1]
        i = i + 1
    return value
def calc_error(dp, params):
    return hypothesis_value(dp.x, params) - dp.y
def summation_of_cost_derivative(index, params, data):
    sum = 0.0
    i = 0
    while i < len(data):
        dp = data[i]
        e = calc_error(dp, params)
        if index == (-1):
            sum = sum + e
        else:
            sum = sum + e * dp.x[index]
        i = i + 1
    return sum
def get_cost_derivative(index, params, data):
    return summation_of_cost_derivative(index, params, data) / (float(len(data)))
def allclose(a, b, atol, rtol):
    i = 0
    while i < len(a):
        diff = absf(a[i] - b[i])
        limit = atol + rtol * absf(b[i])
        if diff > limit:
            return False
        i = i + 1
    return True
def run_gradient_descent(train_data, initial_params):
    learning_rate = 0.009
    absolute_error_limit = 2e-06
    relative_error_limit = 0.0
    j = 0
    params = initial_params
    while True:
        j = j + 1
        temp = []
        i = 0
        while i < len(params):
            deriv = get_cost_derivative(i - 1, params, train_data)
            temp = _append(temp, params[i] - learning_rate * deriv)
            i = i + 1
        if allclose(params, temp, absolute_error_limit, relative_error_limit):
            print("Number of iterations:" + str(j))
            break
        params = temp
    return params
def test_gradient_descent(test_data, params):
    i = 0
    while i < len(test_data):
        dp = test_data[i]
        print("Actual output value:" + str(dp.y))
        print("Hypothesis output:" + str(hypothesis_value(dp.x, params)))
        i = i + 1
train_data = [DataPoint(x=[5.0, 2.0, 3.0], y=15.0), DataPoint(x=[6.0, 5.0, 9.0], y=25.0), DataPoint(x=[11.0, 12.0, 13.0], y=41.0), DataPoint(x=[1.0, 1.0, 1.0], y=8.0), DataPoint(x=[11.0, 12.0, 13.0], y=41.0)]
test_data = [DataPoint(x=[515.0, 22.0, 13.0], y=555.0), DataPoint(x=[61.0, 35.0, 49.0], y=150.0)]
parameter_vector = [2.0, 4.0, 1.0, 5.0]
parameter_vector = run_gradient_descent(train_data, parameter_vector)
print("\nTesting gradient descent for a linear hypothesis function.\n")
test_gradient_descent(test_data, parameter_vector)
