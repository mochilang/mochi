# Code generated by Mochi transpiler.
# Version 0.10.59, generated on 2025-08-07 09:58 +0700
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Dict
import dataclasses
import json
import os
import resource
import time

import sys
sys.set_int_max_str_digits(0)
import os
if os.path.dirname(__file__) in sys.path:
    sys.path.remove(os.path.dirname(__file__))


_now_seed = 0
_now_seeded = False
s = os.getenv("MOCHI_NOW_SEED")
if s and s != "":
    try:
        _now_seed = int(s)
        _now_seeded = True
    except Exception:
        pass

def _now():
    global _now_seed
    if _now_seeded:
        _now_seed = (_now_seed * 1664525 + 1013904223) % 2147483647
        return _now_seed
    return int(time.time_ns())


def _append(lst, v):
    return (lst or []) + [v]

_bench_mem_start = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
_bench_start = _now()
try:
    @dataclass
    class DataPoint:
        x: [float]
        y: float
    def absf(x):
        if x < 0.0:
            return -x
        return x
    def hypothesis_value(input, params):
        value = params[0]
        i = 0
        while i < len(input):
            value = value + input[i] * params[i + 1]
            i = i + 1
        return value
    def calc_error(dp, params):
        return hypothesis_value(dp.x, params) - dp.y
    def summation_of_cost_derivative(index, params, data):
        sum = 0.0
        i = 0
        while i < len(data):
            dp = data[i]
            e = calc_error(dp, params)
            if index == (-1):
                sum = sum + e
            else:
                sum = sum + e * dp.x[index]
            i = i + 1
        return sum
    def get_cost_derivative(index, params, data):
        return summation_of_cost_derivative(index, params, data) / (float(len(data)))
    def allclose(a, b, atol, rtol):
        i = 0
        while i < len(a):
            diff = absf(a[i] - b[i])
            limit = atol + rtol * absf(b[i])
            if diff > limit:
                return False
            i = i + 1
        return True
    def run_gradient_descent(train_data, initial_params):
        learning_rate = 0.009
        absolute_error_limit = 2e-06
        relative_error_limit = 0.0
        j = 0
        params = initial_params
        while True:
            j = j + 1
            temp = []
            i = 0
            while i < len(params):
                deriv = get_cost_derivative(i - 1, params, train_data)
                temp = _append(temp, params[i] - learning_rate * deriv)
                i = i + 1
            if allclose(params, temp, absolute_error_limit, relative_error_limit):
                print("Number of iterations:" + str(j))
                break
            params = temp
        return params
    def test_gradient_descent(test_data, params):
        i = 0
        while i < len(test_data):
            dp = test_data[i]
            print("Actual output value:" + str(dp.y))
            print("Hypothesis output:" + str(hypothesis_value(dp.x, params)))
            i = i + 1
    train_data = [DataPoint(x=[5.0, 2.0, 3.0], y=15.0), DataPoint(x=[6.0, 5.0, 9.0], y=25.0), DataPoint(x=[11.0, 12.0, 13.0], y=41.0), DataPoint(x=[1.0, 1.0, 1.0], y=8.0), DataPoint(x=[11.0, 12.0, 13.0], y=41.0)]
    test_data = [DataPoint(x=[515.0, 22.0, 13.0], y=555.0), DataPoint(x=[61.0, 35.0, 49.0], y=150.0)]
    parameter_vector = [2.0, 4.0, 1.0, 5.0]
    parameter_vector = run_gradient_descent(train_data, parameter_vector)
    print("\nTesting gradient descent for a linear hypothesis function.\n")
    test_gradient_descent(test_data, parameter_vector)
finally:
    _bench_end = _now()
    _bench_mem_end = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
    print(json.dumps({"duration_us": (_bench_end - _bench_start)//1000, "memory_bytes": _bench_mem_end*1024, "name": "main"}, indent=2))
