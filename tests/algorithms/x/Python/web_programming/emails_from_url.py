# Code generated by Mochi transpiler.
# Version 0.10.65, generated on 2025-08-13 16:39 +0700
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Dict
import dataclasses

import sys
if hasattr(sys, "set_int_max_str_digits"):
    sys.set_int_max_str_digits(0)
import os
if os.path.dirname(__file__) in sys.path:
    sys.path.remove(os.path.dirname(__file__))


def _append(lst, v):
    if lst is None:
        lst = []
    lst.append(v)
    return lst


def _str(v):
    if isinstance(v, float):
        if v.is_integer():
            return str(int(v))
        return format(v, ".17g")
    return str(v)


def _set_index(lst, idx, val):
    if lst is None:
        lst = []
    if idx >= len(lst):
        lst.extend([None] * (idx - len(lst) + 1))
    lst[idx] = val
    return lst

@dataclass
class Page:
    url: str
    html: str

def index_of(s, ch):
    i = 0
    while i < len(s):
        if s[i] == ch:
            return i
        i = i + 1
    return -1
def index_of_substring(s, sub):
    n = len(s)
    m = len(sub)
    if m == 0:
        return 0
    i = 0
    while i <= n - m:
        j = 0
        is_match = True
        while j < m:
            if s[i + j] != sub[j]:
                is_match = False
                break
            j = j + 1
        if is_match:
            return i
        i = i + 1
    return -1
def split(s, sep):
    parts = []
    last = 0
    i = 0
    while i < len(s):
        ch = s[i]
        if ch == sep:
            parts = _append(parts, s[last:i])
            last = i + 1
        if i + 1 == len(s):
            parts = _append(parts, s[last:i + 1])
        i = i + 1
    return parts
def get_sub_domain_name(url):
    proto_pos = index_of_substring(url, "://")
    start = 0
    if proto_pos >= 0:
        start = proto_pos + 3
    i = start
    while i < len(url):
        if url[i] == "/":
            break
        i = i + 1
    return url[start:i]
def get_domain_name(url):
    sub = get_sub_domain_name(url)
    parts = sub.split(".")
    if len(parts) >= 2:
        return parts[len(parts) - 2] + "." + parts[len(parts) - 1]
    return sub
def is_alnum(ch):
    chars = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
    return index_of(chars, ch) >= 0
def contains(xs, x):
    i = 0
    while i < len(xs):
        if xs[i] == x:
            return True
        i = i + 1
    return False
def bubble_sort(xs):
    arr = xs
    n = len(arr)
    i = 0
    while i < n:
        j = 0
        while j + 1 < n - i:
            if arr[j] > arr[j + 1]:
                tmp = arr[j]
                arr[j] = arr[j + 1]
                arr[j + 1] = tmp
            j = j + 1
        i = i + 1
    return arr
def extract_links(domain, html):
    links = []
    pos = index_of_substring(html, "href=")
    while pos >= 0:
        start_quote = index_of(html[pos + 5:len(html)], "\"")
        if start_quote < 0:
            break
        rest = pos + 5 + start_quote + 1
        end_quote = index_of(html[rest:len(html)], "\"")
        if end_quote < 0:
            break
        link = html[rest:rest + end_quote]
        if not contains(links, link):
            absolute = link
            if not (index_of_substring(link, "http://") == 0 or index_of_substring(link, "https://") == 0):
                if index_of_substring(link, "/") == 0:
                    absolute = "https://" + domain + link
                else:
                    absolute = "https://" + domain + "/" + link
            links = _append(links, absolute)
        pos = index_of_substring(html[rest + end_quote:len(html)], "href=")
        if pos >= 0:
            pos = pos + rest + end_quote
    return links
def extract_emails(domain, text):
    emails = []
    i = 0
    while i < len(text):
        if text[i] == "@":
            if text[i + 1:i + 1 + len(domain)] == domain:
                j = i - 1
                while j >= 0 and is_alnum(text[j]):
                    j = j - 1
                local = text[j + 1:i]
                if len(local) > 0:
                    email = local + "@" + domain
                    if not contains(emails, email):
                        emails = _append(emails, email)
        i = i + 1
    return emails
def find_page(pages, url):
    i = 0
    while i < len(pages):
        p = pages[i]
        if p.url == url:
            return p.html
        i = i + 1
    return ""
def emails_from_url(url, pages):
    domain = get_domain_name(url)
    base_html = find_page(pages, url)
    links = extract_links(domain, base_html)
    found = []
    i = 0
    while i < len(links):
        html = find_page(pages, links[i])
        emails = extract_emails(domain, html)
        j = 0
        while j < len(emails):
            if not contains(found, emails[j]):
                found = _append(found, emails[j])
            j = j + 1
        i = i + 1
    sorted = bubble_sort(found)
    return sorted
pages = [Page(url="https://example.com", html="<html><body><a href=\"/contact\">Contact</a></body></html>"), Page(url="https://example.com/contact", html="<html>Contact us at info@example.com or support@example.com</html>")]
emails = emails_from_url("https://example.com", pages)
print(_str(len(emails)) + " emails found:")
k = 0
while k < len(emails):
    print(emails[k])
    k = k + 1

