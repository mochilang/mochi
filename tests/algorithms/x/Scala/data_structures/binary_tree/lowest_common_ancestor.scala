// Generated by Mochi v0.10.59 on 2025-08-06 22:22:33 GMT+7
import scala.collection.mutable.{ArrayBuffer, Map}
import scala.math.BigInt
import scala.collection.immutable.ListMap
object Main {
  private var _nowSeed: Long = 0L
  private var _nowSeeded: Boolean = false
  private def _now(): Int = {
    if (!_nowSeeded) {
      sys.env.get("MOCHI_NOW_SEED").foreach { s =>
      try { _nowSeed = s.toInt; _nowSeeded = true } catch { case _ : NumberFormatException => () }
    }
  }
  if (_nowSeeded) {
    _nowSeed = (_nowSeed * 1664525 + 1013904223) % 2147483647
    _nowSeed.toInt
  } else {
    Math.abs((System.nanoTime() / 1000).toInt)
  }
}

def toJson(value: Any, indent: Int = 0): String = value match {
  case m: scala.collection.Map[_, _] =>
  val items = ListMap(m.toSeq.sortBy(_._1.toString): _*).toSeq.map{ case (k,v) => "  "*(indent+1)+"\""+k.toString+"\": "+toJson(v, indent+1) }
  "{\n"+items.mkString(",\n")+"\n"+"  "*indent+"}"
  case s: Seq[_] =>
  val items = s.map(x => "  "*(indent+1)+toJson(x, indent+1))
  "[\n"+items.mkString(",\n")+"\n"+"  "*indent+"]"
  case s: String => "\""+s+"\""
  case other => other.toString
}

def pow2(exp: BigInt): BigInt = {
  var res: BigInt = 1
  var i: BigInt = 0
  while (i < exp) {
    res = res * 2
    i = i + 1
  }
  return res
}

def create_sparse(max_node: BigInt, parent: ArrayBuffer[ArrayBuffer[BigInt]]): ArrayBuffer[ArrayBuffer[BigInt]] = {
  var j: BigInt = 1
  while (pow2(j) < max_node) {
    var i: BigInt = 1
    while (i <= max_node) {
      parent((j).toInt)((i).toInt) = parent(((j - 1).toInt).toInt)((parent(((j - 1).toInt).toInt)((i.toInt).toInt).toInt).toInt)
      i = i + 1
    }
    j = j + 1
  }
  return parent
}

def lowest_common_ancestor(_u: BigInt, _v: BigInt, level: ArrayBuffer[BigInt], parent: ArrayBuffer[ArrayBuffer[BigInt]]): BigInt = {
  var u: BigInt = _u
  var v: BigInt = _v
  if (level((u.toInt).toInt) < level((v.toInt).toInt)) {
    val temp: BigInt = u
    u = v
    v = temp
  }
  var i: BigInt = 18
  while (i >= 0) {
    if (level((u.toInt).toInt) - pow2(i) >= level((v.toInt).toInt)) {
      u = parent((i.toInt).toInt)((u.toInt).toInt)
    }
    i = i - 1
  }
  if (u == v) {
    return u
  }
  i = 18
  while (i >= 0) {
    val pu: BigInt = parent((i.toInt).toInt)((u.toInt).toInt)
    val pv: BigInt = parent((i.toInt).toInt)((v.toInt).toInt)
    if ((pu != 0 && pu != pv).asInstanceOf[Boolean]) {
      u = pu
      v = pv
    }
    i = i - 1
  }
  return parent((0.toInt).toInt)((u.toInt).toInt)
}

def breadth_first_search(level: ArrayBuffer[BigInt], parent: ArrayBuffer[ArrayBuffer[BigInt]], max_node: BigInt, graph: scala.collection.mutable.Map[BigInt,ArrayBuffer[BigInt]], root: BigInt): Any = {
  level((root).toInt) = 0
  var q: ArrayBuffer[BigInt] = ArrayBuffer()
  q = q :+ root
  var head: BigInt = 0
  while (head < BigInt((q).size)) {
    val u: BigInt = q((head.toInt).toInt)
    head = head + 1
    val adj: ArrayBuffer[BigInt] = graph.getOrElse(u, ArrayBuffer()).asInstanceOf[ArrayBuffer[BigInt]]
    var j: BigInt = 0
    while (j < BigInt((adj).size)) {
      val v: BigInt = adj((j.toInt).toInt)
      if (level((v.toInt).toInt) == 0 - 1) {
        level((v).toInt) = level((u.toInt).toInt) + 1
        parent((0).toInt)((v).toInt) = u
        q = q :+ v
      }
      j = j + 1
    }
  }
}

def main(): Any = {
  val max_node: BigInt = 13
  var parent: ArrayBuffer[ArrayBuffer[BigInt]] = ArrayBuffer()
  var i: BigInt = 0
  while (i < 20) {
    var row: ArrayBuffer[BigInt] = ArrayBuffer()
    var j: BigInt = 0
    while (j < max_node + 10) {
      row = row :+ BigInt(0)
      j = j + 1
    }
    parent = parent :+ row
    i = i + 1
  }
  var level: ArrayBuffer[BigInt] = ArrayBuffer()
  i = 0
  while (i < max_node + 10) {
    level = level :+ 0 - 1
    i = i + 1
  }
  var graph: scala.collection.mutable.Map[BigInt,ArrayBuffer[BigInt]] = (scala.collection.mutable.Map()).asInstanceOf[scala.collection.mutable.Map[BigInt,ArrayBuffer[BigInt]]]
  graph.update(1, ArrayBuffer(2, 3, 4))
  graph.update(2, ArrayBuffer(5))
  graph.update(3, ArrayBuffer(6, 7))
  graph.update(4, ArrayBuffer(8))
  graph.update(5, ArrayBuffer(9, 10))
  graph.update(6, ArrayBuffer(11))
  graph.update(7, ArrayBuffer())
  graph.update(8, ArrayBuffer(12, 13))
  graph.update(9, ArrayBuffer())
  graph.update(10, ArrayBuffer())
  graph.update(11, ArrayBuffer())
  graph.update(12, ArrayBuffer())
  graph.update(13, ArrayBuffer())
  breadth_first_search(level, parent, max_node, graph, 1)
  parent = create_sparse(max_node, parent)
  println("LCA of node 1 and 3 is: " + String.valueOf(lowest_common_ancestor(1, 3, level, parent)))
  println("LCA of node 5 and 6 is: " + String.valueOf(lowest_common_ancestor(5, 6, level, parent)))
  println("LCA of node 7 and 11 is: " + String.valueOf(lowest_common_ancestor(7, 11, level, parent)))
  println("LCA of node 6 and 7 is: " + String.valueOf(lowest_common_ancestor(6, 7, level, parent)))
  println("LCA of node 4 and 12 is: " + String.valueOf(lowest_common_ancestor(4, 12, level, parent)))
  println("LCA of node 8 and 8 is: " + String.valueOf(lowest_common_ancestor(8, 8, level, parent)))
}

def main(args: Array[String]): Unit = {
  {
    System.gc()
    val _startMem = Runtime.getRuntime.totalMemory() - Runtime.getRuntime.freeMemory()
    val _start = _now()
    main()
    val _end = _now()
    System.gc()
    val _endMem = Runtime.getRuntime.totalMemory() - Runtime.getRuntime.freeMemory()
    val _durUs = (_end - _start) / 1000
    var _memDiff = _endMem - _startMem
    if (_memDiff <= 0) _memDiff = _endMem
    println(toJson(scala.collection.immutable.Map("duration_us" -> _durUs, "memory_bytes" -> _memDiff, "name" -> "main")))
  }
}
}
