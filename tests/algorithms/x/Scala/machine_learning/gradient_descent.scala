// Generated by Mochi v0.10.59 on 2025-08-07 12:43:24 GMT+7
import scala.collection.mutable.{ArrayBuffer, Map}
import scala.math.BigInt
import scala.collection.immutable.ListMap
import scala.util.control.Breaks
import scala.util.control.Breaks._
object Main {
  private var _nowSeed: Long = 0L
  private var _nowSeeded: Boolean = false
  private def _now(): Int = {
    if (!_nowSeeded) {
      sys.env.get("MOCHI_NOW_SEED").foreach { s =>
      try { _nowSeed = s.toInt; _nowSeeded = true } catch { case _ : NumberFormatException => () }
    }
  }
  if (_nowSeeded) {
    _nowSeed = (_nowSeed * 1664525 + 1013904223) % 2147483647
    _nowSeed.toInt
  } else {
    Math.abs((System.nanoTime() / 1000).toInt)
  }
}

def toJson(value: Any, indent: Int = 0): String = value match {
  case m: scala.collection.Map[_, _] =>
  val items = ListMap(m.toSeq.sortBy(_._1.toString): _*).toSeq.map{ case (k,v) => "  "*(indent+1)+"\""+k.toString+"\": "+toJson(v, indent+1) }
  "{\n"+items.mkString(",\n")+"\n"+"  "*indent+"}"
  case s: Seq[_] =>
  val items = s.map(x => "  "*(indent+1)+toJson(x, indent+1))
  "[\n"+items.mkString(",\n")+"\n"+"  "*indent+"]"
  case s: String => "\""+s+"\""
  case other => other.toString
}

case class DataPoint(var x: ArrayBuffer[Double], var y: Double)

def absf(x: Double): Double = {
  if (x < 0.0) {
    return -x
  }
  return x
}

def hypothesis_value(input: ArrayBuffer[Double], params: ArrayBuffer[Double]): Double = {
  var value: Double = params((BigInt(0).toInt).toInt)
  var i: BigInt = BigInt(0)
  while (i < BigInt((input).size)) {
    value = value + input((i.toInt).toInt) * params(((i + BigInt(1)).toInt).toInt)
    i = i + BigInt(1)
  }
  return value
}

def calc_error(dp: DataPoint, params: ArrayBuffer[Double]): Double = {
  return hypothesis_value(dp.asInstanceOf[DataPoint].x, params) - dp.asInstanceOf[DataPoint].y
}

def summation_of_cost_derivative(index: BigInt, params: ArrayBuffer[Double], data: ArrayBuffer[DataPoint]): Double = {
  var sum: Double = 0.0
  var i: BigInt = BigInt(0)
  while (i < BigInt((data).size)) {
    val dp: DataPoint = data((i.toInt).toInt)
    val e: Double = calc_error(dp, params)
    if (index == -BigInt(1)) {
      sum = sum + e
    } else {
      sum = sum + e * dp.asInstanceOf[DataPoint].x((index.toInt).toInt)
    }
    i = i + BigInt(1)
  }
  return sum
}

def get_cost_derivative(index: BigInt, params: ArrayBuffer[Double], data: ArrayBuffer[DataPoint]): Double = {
  return summation_of_cost_derivative(index, params, data) / (BigInt((data).size)).toString.toDouble
}

def allclose(a: ArrayBuffer[Double], b: ArrayBuffer[Double], atol: Double, rtol: Double): Boolean = {
  var i: BigInt = BigInt(0)
  while (i < BigInt((a).size)) {
    val diff: Double = absf(a((i.toInt).toInt) - b((i.toInt).toInt))
    val limit: Double = atol + rtol * absf(b((i.toInt).toInt))
    if (diff > limit) {
      return false
    }
    i = i + BigInt(1)
  }
  return true
}

def run_gradient_descent(train_data: ArrayBuffer[DataPoint], initial_params: ArrayBuffer[Double]): ArrayBuffer[Double] = {
  val learning_rate: Double = 0.009
  val absolute_error_limit: Double = 0.000002
  val relative_error_limit: Double = 0.0
  var j: BigInt = BigInt(0)
  var params: ArrayBuffer[Double] = initial_params
  val _br3 = new Breaks
  _br3.breakable {
    while (true) {
      j = j + BigInt(1)
      var temp: ArrayBuffer[Double] = ArrayBuffer[Double]()
      var i: BigInt = BigInt(0)
      while (i < BigInt((params).size)) {
        val deriv: Double = get_cost_derivative(i - BigInt(1), params, train_data)
        temp = temp :+ params((i.toInt).toInt) - learning_rate * deriv
        i = i + BigInt(1)
      }
      if (allclose(params, temp, absolute_error_limit, relative_error_limit)) {
        println("Number of iterations:" + String.valueOf(j))
        _br3.break()
      }
      params = temp
    }
  }
  return params
}

def test_gradient_descent(test_data: ArrayBuffer[DataPoint], params: ArrayBuffer[Double]): Any = {
  var i: BigInt = BigInt(0)
  while (i < BigInt((test_data).size)) {
    val dp: DataPoint = test_data((i.toInt).toInt)
    println("Actual output value:" + String.valueOf(dp.asInstanceOf[DataPoint].y))
    println("Hypothesis output:" + String.valueOf(hypothesis_value(dp.asInstanceOf[DataPoint].x, params)))
    i = i + BigInt(1)
  }
}

val train_data: ArrayBuffer[DataPoint] = ArrayBuffer(DataPoint(ArrayBuffer(5.0, 2.0, 3.0), 15.0), DataPoint(ArrayBuffer(6.0, 5.0, 9.0), 25.0), DataPoint(ArrayBuffer(11.0, 12.0, 13.0), 41.0), DataPoint(ArrayBuffer(1.0, 1.0, 1.0), 8.0), DataPoint(ArrayBuffer(11.0, 12.0, 13.0), 41.0))

val test_data: ArrayBuffer[DataPoint] = ArrayBuffer(DataPoint(ArrayBuffer(515.0, 22.0, 13.0), 555.0), DataPoint(ArrayBuffer(61.0, 35.0, 49.0), 150.0))

def main(args: Array[String]): Unit = {
  {
    System.gc()
    val _startMem = Runtime.getRuntime.totalMemory() - Runtime.getRuntime.freeMemory()
    val _start = _now()
    var parameter_vector: ArrayBuffer[Double] = ArrayBuffer(2.0, 4.0, 1.0, 5.0)
    parameter_vector = run_gradient_descent(train_data, parameter_vector)
    println("\nTesting gradient descent for a linear hypothesis function.\n")
    test_gradient_descent(test_data, parameter_vector)
    val _end = _now()
    System.gc()
    val _endMem = Runtime.getRuntime.totalMemory() - Runtime.getRuntime.freeMemory()
    val _durUs = (_end - _start) / 1000
    var _memDiff = _endMem - _startMem
    if (_memDiff <= 0) _memDiff = _endMem
    println(toJson(scala.collection.immutable.Map("duration_us" -> _durUs, "memory_bytes" -> _memDiff, "name" -> "main")))
  }
}
}
