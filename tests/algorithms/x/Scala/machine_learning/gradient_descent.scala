// Generated by Mochi v0.10.67 on 2025-08-16 21:29:38 GMT+7
import scala.collection.mutable.{ArrayBuffer, Map}
import scala.math.BigInt
import scala.collection.immutable.ListMap
import scala.util.control.Breaks
import scala.util.control.Breaks._
object Main {
  private var _nowSeed: Long = 0L
  private var _nowSeeded: Boolean = false
  private def _now(): Int = {
    if (!_nowSeeded) {
      sys.env.get("MOCHI_NOW_SEED").foreach { s =>
      try { _nowSeed = s.toInt; _nowSeeded = true } catch { case _ : NumberFormatException => () }
    }
    if (!_nowSeeded) { _nowSeed = 0L; _nowSeeded = true }
  }
  if (_nowSeeded) {
    _nowSeed = (_nowSeed * 1664525 + 1013904223) % 2147483647
    _nowSeed.toInt
  } else {
    Math.abs((System.nanoTime() / 1000).toInt)
  }
}

def toJson(value: Any, indent: Int = 0): String = value match {
  case m: scala.collection.Map[_, _] =>
  val items = ListMap(m.toSeq.sortBy(_._1.toString): _*).toSeq.map{ case (k,v) => "  "*(indent+1)+"\""+k.toString+"\": "+toJson(v, indent+1) }
  "{\n"+items.mkString(",\n")+"\n"+"  "*indent+"}"
  case s: Seq[_] =>
  val items = s.map(x => "  "*(indent+1)+toJson(x, indent+1))
  "[\n"+items.mkString(",\n")+"\n"+"  "*indent+"]"
  case s: String => "\""+s+"\""
  case other => other.toString
}

private def _str(x: Any): String = x match {
  case m: scala.collection.Map[_, _] => scala.collection.immutable.ListMap(m.toSeq.sortBy(_._1.toString): _*).toString.replace("ListMap", "Map")
  case d: Double => { val s = java.lang.Double.toString(d); if (s.indexOf('.') < 0 && !s.contains("e") && !s.contains("E")) s + ".0" else s }
  case other => String.valueOf(other)
}

case class DataPoint(var x: ArrayBuffer[Double], var y: Double)

def absf(x: Double): Double = {
  if (x < 0.0) {
    return -x
  }
  return x
}

def hypothesis_value(input: ArrayBuffer[Double], params: ArrayBuffer[Double]): Double = {
  var value: Double = params.lift((BigInt(0).toInt).toInt).getOrElse(0.0)
  var i: BigInt = BigInt(0)
  while (i < BigInt((input).size)) {
    value = value + input.lift((i.toInt).toInt).getOrElse(0.0) * params.lift(((i + BigInt(1)).toInt).toInt).getOrElse(0.0)
    i = i + BigInt(1)
  }
  return value
}

def calc_error(dp: DataPoint, params: ArrayBuffer[Double]): Double = {
  return hypothesis_value(dp.asInstanceOf[DataPoint].x, params) - dp.asInstanceOf[DataPoint].y
}

def summation_of_cost_derivative(index: BigInt, params: ArrayBuffer[Double], data: ArrayBuffer[DataPoint]): Double = {
  var sum: Double = 0.0
  var i: BigInt = BigInt(0)
  while (i < BigInt((data).size)) {
    var dp: DataPoint = data.lift((i.toInt).toInt).getOrElse(null)
    var e: Double = calc_error(dp, params)
    if (index == -BigInt(1)) {
      sum = sum + e
    } else {
      sum = sum + e * dp.asInstanceOf[DataPoint].x.lift((index.toInt).toInt).getOrElse(0.0)
    }
    i = i + BigInt(1)
  }
  return sum
}

def get_cost_derivative(index: BigInt, params: ArrayBuffer[Double], data: ArrayBuffer[DataPoint]): Double = {
  return summation_of_cost_derivative(index, params, data) / (BigInt((data).size)).toDouble
}

def allclose(a: ArrayBuffer[Double], b: ArrayBuffer[Double], atol: Double, rtol: Double): Boolean = {
  var i: BigInt = BigInt(0)
  while (i < BigInt((a).size)) {
    var diff: Double = absf(a.lift((i.toInt).toInt).getOrElse(0.0) - b.lift((i.toInt).toInt).getOrElse(0.0))
    var limit: Double = atol + rtol * absf(b.lift((i.toInt).toInt).getOrElse(0.0))
    if (diff > limit) {
      return false
    }
    i = i + BigInt(1)
  }
  return true
}

def run_gradient_descent(train_data: ArrayBuffer[DataPoint], initial_params: ArrayBuffer[Double]): ArrayBuffer[Double] = {
  var learning_rate: Double = 0.009
  var absolute_error_limit: Double = 0.000002
  var relative_error_limit: Double = 0.0
  var j: BigInt = BigInt(0)
  var params: ArrayBuffer[Double] = initial_params
  val _br3 = new Breaks
  _br3.breakable {
    while (true) {
      j = j + BigInt(1)
      var temp: ArrayBuffer[Double] = ArrayBuffer[Double]()
      var i: BigInt = BigInt(0)
      while (i < BigInt((params).size)) {
        var deriv: Double = get_cost_derivative(i - BigInt(1), params, train_data)
        temp = (temp :+ params.lift((i.toInt).toInt).getOrElse(0.0) - learning_rate * deriv)
        i = i + BigInt(1)
      }
      if (allclose(params, temp, absolute_error_limit, relative_error_limit)) {
        println(_str("Number of iterations:" + _str(j)))
        _br3.break()
      }
      params = temp
    }
  }
  return params
}

def test_gradient_descent(test_data: ArrayBuffer[DataPoint], params: ArrayBuffer[Double]): Any = {
  var i: BigInt = BigInt(0)
  while (i < BigInt((test_data).size)) {
    var dp: DataPoint = test_data.lift((i.toInt).toInt).getOrElse(null)
    println(_str("Actual output value:" + _str(dp.asInstanceOf[DataPoint].y)))
    println(_str("Hypothesis output:" + _str(hypothesis_value(dp.asInstanceOf[DataPoint].x, params))))
    i = i + BigInt(1)
  }
}

var train_data: ArrayBuffer[DataPoint] = ArrayBuffer(DataPoint(ArrayBuffer(5.0, 2.0, 3.0), 15.0), DataPoint(ArrayBuffer(6.0, 5.0, 9.0), 25.0), DataPoint(ArrayBuffer(11.0, 12.0, 13.0), 41.0), DataPoint(ArrayBuffer(1.0, 1.0, 1.0), 8.0), DataPoint(ArrayBuffer(11.0, 12.0, 13.0), 41.0))

var test_data: ArrayBuffer[DataPoint] = ArrayBuffer(DataPoint(ArrayBuffer(515.0, 22.0, 13.0), 555.0), DataPoint(ArrayBuffer(61.0, 35.0, 49.0), 150.0))

var parameter_vector: ArrayBuffer[Double] = ArrayBuffer(2.0, 4.0, 1.0, 5.0)

def main(args: Array[String]): Unit = {
  {
    System.gc()
    val _startMem = Runtime.getRuntime.totalMemory() - Runtime.getRuntime.freeMemory()
    val _start = _now()
    parameter_vector = run_gradient_descent(train_data, parameter_vector)
    println(_str("\nTesting gradient descent for a linear hypothesis function.\n"))
    test_gradient_descent(test_data, parameter_vector)
    val _end = _now()
    System.gc()
    val _endMem = Runtime.getRuntime.totalMemory() - Runtime.getRuntime.freeMemory()
    val _durUs = (_end - _start).abs / 1000
    var _memDiff = _endMem - _startMem
    if (_memDiff <= 0) _memDiff = _endMem
    println(toJson(scala.collection.immutable.Map("duration_us" -> _durUs, "memory_bytes" -> _memDiff, "name" -> "main")))
  }
}
}
