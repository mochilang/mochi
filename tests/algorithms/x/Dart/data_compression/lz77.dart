// Generated by Mochi transpiler
String _substr(String s, num start, num end) {
  var n = s.length;
  int s0 = start.toInt();
  int e0 = end.toInt();
  if (s0 < 0) s0 += n;
  if (e0 < 0) e0 += n;
  if (s0 < 0) s0 = 0;
  if (s0 > n) s0 = n;
  if (e0 < 0) e0 = 0;
  if (e0 > n) e0 = n;
  if (s0 > e0) s0 = e0;
  return s.substring(s0, e0);
}

class Token {
  int offset;
  int length;
  String indicator;
  Token({required this.offset, required this.length, required this.indicator});
}

String token_to_string(Token t) {
  return "(" + (t.offset).toString() + ", " + (t.length).toString() + ", " + t.indicator + ")";
}

String tokens_to_string(List<Token> ts) {
  String res = "[";
  int i = 0;
  while (i < ts.length) {
    res = res + token_to_string(ts[i]);
    if (i < ts.length - 1) {
    res = res + ", ";
  }
    i = i + 1;
  }
  return res + "]";
}

int match_length_from_index(String text, String window, int text_index, int window_index) {
  if (text_index >= text.length || window_index >= window.length) {
    return 0;
  }
  String tc = _substr(text, text_index, text_index + 1);
  String wc = _substr(window, window_index, window_index + 1);
  if (tc != wc) {
    return 0;
  }
  return 1 + match_length_from_index(text, window + tc, text_index + 1, window_index + 1);
}

Token find_encoding_token(String text, String search_buffer) {
  if (text.length == 0) {
    throw Exception("We need some text to work with.");
  }
  int length = 0;
  int offset = 0;
  if (search_buffer.length == 0) {
    return Token(offset: offset, length: length, indicator: _substr(text, 0, 1));
  }
  int i = 0;
  while (i < search_buffer.length) {
    String ch = _substr(search_buffer, i, i + 1);
    int found_offset = search_buffer.length - i;
    if (ch == _substr(text, 0, 1)) {
    int found_length = match_length_from_index(text, search_buffer, 0, i);
    if (found_length >= length) {
    offset = found_offset;
    length = found_length;
  };
  }
    i = i + 1;
  }
  return Token(offset: offset, length: length, indicator: _substr(text, length, length + 1));
}

List<Token> lz77_compress(String text, int window_size, int lookahead) {
  int search_buffer_size = window_size - lookahead;
  List<Token> output = <Token>[];
  String search_buffer = "";
  String remaining = text;
  while (remaining.length > 0) {
    Token token = find_encoding_token(remaining, search_buffer);
    int add_len = token.length + 1;
    search_buffer = search_buffer + _substr(remaining, 0, add_len);
    if (search_buffer.length > search_buffer_size) {
    search_buffer = _substr(search_buffer, search_buffer.length - search_buffer_size, search_buffer.length);
  }
    remaining = _substr(remaining, add_len, remaining.length);
    output = [...output, token];
  }
  return output;
}

String lz77_decompress(List<Token> tokens) {
  String output = "";
  for (Token t in tokens) {
    int i = 0;
    while (i < t.length) {
    output = output + _substr(output, output.length - t.offset, output.length - t.offset + 1);
    i = i + 1;
  }
    output = output + t.indicator;
  }
  return output;
}

List<Token> c1 = lz77_compress("ababcbababaa", 13, 6);
List<Token> c2 = lz77_compress("aacaacabcabaaac", 13, 6);
List<Token> tokens_example = [Token(offset: 0, length: 0, indicator: "c"), Token(offset: 0, length: 0, indicator: "a"), Token(offset: 0, length: 0, indicator: "b"), Token(offset: 0, length: 0, indicator: "r"), Token(offset: 3, length: 1, indicator: "c"), Token(offset: 2, length: 1, indicator: "d"), Token(offset: 7, length: 4, indicator: "r"), Token(offset: 3, length: 5, indicator: "d")];
void main() {
  print(tokens_to_string(c1));
  print(tokens_to_string(c2));
  print(lz77_decompress(tokens_example));
}
