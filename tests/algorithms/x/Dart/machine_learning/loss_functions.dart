// Generated by Mochi transpiler
String _substr(String s, num start, num end) {
  var n = s.length;
  int s0 = start.toInt();
  int e0 = end.toInt();
  if (s0 < 0) s0 += n;
  if (e0 < 0) e0 += n;
  if (s0 < 0) s0 = 0;
  if (s0 > n) s0 = n;
  if (e0 < 0) e0 = 0;
  if (e0 > n) e0 = n;
  if (s0 > e0) s0 = e0;
  return s.substring(s0, e0);
}

double absf(double x) {
  if (x < 0.0) {
    return -x;
  }
  return x;
}

double maxf(double a, double b) {
  if (a > b) {
    return a;
  }
  return b;
}

double minf(double a, double b) {
  if (a < b) {
    return a;
  }
  return b;
}

double clip(double x, double lo, double hi) {
  return maxf(lo, minf(x, hi));
}

double to_float(int x) {
  return x * 1.0;
}

double powf(double base, double exp) {
  double result = 1.0;
  int i = 0;
  int n = (exp).toInt();
  while (i < n) {
    result = result * base;
    i = i + 1;
  }
  return result;
}

double ln(double x) {
  if (x <= 0.0) {
    throw Exception("ln domain error");
  }
  double y = (x - 1.0) / (x + 1.0);
  double y2 = y * y;
  double term = y;
  double sum = 0.0;
  int k = 0;
  while (k < 10) {
    double denom = to_float(2 * k + 1);
    sum = sum + term / denom;
    term = term * y2;
    k = k + 1;
  }
  return 2.0 * sum;
}

double exp(double x) {
  double term = 1.0;
  double sum = 1.0;
  int n = 1;
  while (n < 20) {
    term = term * x / to_float(n);
    sum = sum + term;
    n = n + 1;
  }
  return sum;
}

double mean(List<double> v) {
  double total = 0.0;
  int i = 0;
  while (i < v.length) {
    total = total + v[i];
    i = i + 1;
  }
  return total / to_float(v.length);
}

double binary_cross_entropy(List<double> y_true, List<double> y_pred, double epsilon) {
  if (y_true.length != y_pred.length) {
    throw Exception("Input arrays must have the same length.");
  }
  List<double> losses = <double>[];
  int i = 0;
  while (i < y_true.length) {
    double yt = y_true[i];
    double yp = clip(y_pred[i], epsilon, 1.0 - epsilon);
    num loss = -(yt * ln(yp) + (1.0 - yt) * ln(1.0 - yp));
    losses = List<double>.from([...losses, loss]);
    i = i + 1;
  }
  return mean(losses);
}

double binary_focal_cross_entropy(List<double> y_true, List<double> y_pred, double gamma, double alpha, double epsilon) {
  if (y_true.length != y_pred.length) {
    throw Exception("Input arrays must have the same length.");
  }
  List<double> losses = <double>[];
  int i = 0;
  while (i < y_true.length) {
    double yt = y_true[i];
    double yp = clip(y_pred[i], epsilon, 1.0 - epsilon);
    double term1 = alpha * powf(1.0 - yp, gamma) * yt * ln(yp);
    double term2 = (1.0 - alpha) * powf(yp, gamma) * (1.0 - yt) * ln(1.0 - yp);
    losses = List<double>.from([...losses, -(term1 + term2)]);
    i = i + 1;
  }
  return mean(losses);
}

double categorical_cross_entropy(List<List<double>> y_true, List<List<double>> y_pred, double epsilon) {
  if (y_true.length != y_pred.length) {
    throw Exception("Input arrays must have the same shape.");
  }
  int rows = y_true.length;
  double total = 0.0;
  int i = 0;
  while (i < rows) {
    if (y_true[i].length != y_pred[i].length) {
    throw Exception("Input arrays must have the same shape.");
  }
    double sum_true = 0.0;
    double sum_pred = 0.0;
    int j = 0;
    while (j < y_true[i].length) {
    double yt = y_true[i][j];
    double yp = y_pred[i][j];
    if (yt != 0.0 && yt != 1.0) {
    throw Exception("y_true must be one-hot encoded.");
  }
    sum_true = sum_true + yt;
    sum_pred = sum_pred + yp;
    j = j + 1;
  }
    if (sum_true != 1.0) {
    throw Exception("y_true must be one-hot encoded.");
  }
    if (absf(sum_pred - 1.0) > epsilon) {
    throw Exception("Predicted probabilities must sum to approximately 1.");
  }
    j = 0;
    while (j < y_true[i].length) {
    double yp = clip(y_pred[i][j], epsilon, 1.0);
    total = total - y_true[i][j] * ln(yp);
    j = j + 1;
  }
    i = i + 1;
  }
  return total;
}

double categorical_focal_cross_entropy(List<List<double>> y_true, List<List<double>> y_pred, List<double> alpha, double gamma, double epsilon) {
  if (y_true.length != y_pred.length) {
    throw Exception("Shape of y_true and y_pred must be the same.");
  }
  int rows = y_true.length;
  int cols = y_true[0].length;
  List<double> a = alpha;
  if (a.length == 0) {
    List<double> tmp = <double>[];
    int j = 0;
    while (j < cols) {
    tmp = [...tmp, 1.0];
    j = j + 1;
  };
    a = tmp;
  }
  if (a.length != cols) {
    throw Exception("Length of alpha must match the number of classes.");
  }
  double total = 0.0;
  int i = 0;
  while (i < rows) {
    if (y_true[i].length != cols || y_pred[i].length != cols) {
    throw Exception("Shape of y_true and y_pred must be the same.");
  }
    double sum_true = 0.0;
    double sum_pred = 0.0;
    int j = 0;
    while (j < cols) {
    double yt = y_true[i][j];
    double yp = y_pred[i][j];
    if (yt != 0.0 && yt != 1.0) {
    throw Exception("y_true must be one-hot encoded.");
  }
    sum_true = sum_true + yt;
    sum_pred = sum_pred + yp;
    j = j + 1;
  }
    if (sum_true != 1.0) {
    throw Exception("y_true must be one-hot encoded.");
  }
    if (absf(sum_pred - 1.0) > epsilon) {
    throw Exception("Predicted probabilities must sum to approximately 1.");
  }
    double row_loss = 0.0;
    j = 0;
    while (j < cols) {
    double yp = clip(y_pred[i][j], epsilon, 1.0);
    row_loss = row_loss + a[j] * powf(1.0 - yp, gamma) * y_true[i][j] * ln(yp);
    j = j + 1;
  }
    total = total - row_loss;
    i = i + 1;
  }
  return total / to_float(rows);
}

double hinge_loss(List<double> y_true, List<double> y_pred) {
  if (y_true.length != y_pred.length) {
    throw Exception("Length of predicted and actual array must be same.");
  }
  List<double> losses = <double>[];
  int i = 0;
  while (i < y_true.length) {
    double yt = y_true[i];
    if (yt != -1.0 && yt != 1.0) {
    throw Exception("y_true can have values -1 or 1 only.");
  }
    double pred = y_pred[i];
    double l = maxf(0.0, 1.0 - yt * pred);
    losses = [...losses, l];
    i = i + 1;
  }
  return mean(losses);
}

double huber_loss(List<double> y_true, List<double> y_pred, double delta) {
  if (y_true.length != y_pred.length) {
    throw Exception("Input arrays must have the same length.");
  }
  double total = 0.0;
  int i = 0;
  while (i < y_true.length) {
    double diff = y_true[i] - y_pred[i];
    double adiff = absf(diff);
    if (adiff <= delta) {
    total = total + 0.5 * diff * diff;
  } else {
    total = total + delta * (adiff - 0.5 * delta);
  }
    i = i + 1;
  }
  return total / to_float(y_true.length);
}

double mean_squared_error(List<double> y_true, List<double> y_pred) {
  if (y_true.length != y_pred.length) {
    throw Exception("Input arrays must have the same length.");
  }
  List<double> losses = <double>[];
  int i = 0;
  while (i < y_true.length) {
    double diff = y_true[i] - y_pred[i];
    losses = [...losses, diff * diff];
    i = i + 1;
  }
  return mean(losses);
}

double mean_absolute_error(List<double> y_true, List<double> y_pred) {
  if (y_true.length != y_pred.length) {
    throw Exception("Input arrays must have the same length.");
  }
  double total = 0.0;
  int i = 0;
  while (i < y_true.length) {
    total = total + absf(y_true[i] - y_pred[i]);
    i = i + 1;
  }
  return total / to_float(y_true.length);
}

double mean_squared_logarithmic_error(List<double> y_true, List<double> y_pred) {
  if (y_true.length != y_pred.length) {
    throw Exception("Input arrays must have the same length.");
  }
  double total = 0.0;
  int i = 0;
  while (i < y_true.length) {
    double a = ln(1.0 + y_true[i]);
    double b = ln(1.0 + y_pred[i]);
    double diff = a - b;
    total = total + diff * diff;
    i = i + 1;
  }
  return total / to_float(y_true.length);
}

double mean_absolute_percentage_error(List<double> y_true, List<double> y_pred, double epsilon) {
  if (y_true.length != y_pred.length) {
    throw Exception("The length of the two arrays should be the same.");
  }
  double total = 0.0;
  int i = 0;
  while (i < y_true.length) {
    double yt = y_true[i];
    if (yt == 0.0) {
    yt = epsilon;
  }
    total = total + absf((yt - y_pred[i]) / yt);
    i = i + 1;
  }
  return total / to_float(y_true.length);
}

double perplexity_loss(List<List<int>> y_true, List<List<List<double>>> y_pred, double epsilon) {
  int batch = y_true.length;
  if (batch != y_pred.length) {
    throw Exception("Batch size of y_true and y_pred must be equal.");
  }
  int sentence_len = y_true[0].length;
  if (sentence_len != y_pred[0].length) {
    throw Exception("Sentence length of y_true and y_pred must be equal.");
  }
  int vocab_size = y_pred[0][0].length;
  int b = 0;
  double total_perp = 0.0;
  while (b < batch) {
    if (y_true[b].length != sentence_len || y_pred[b].length != sentence_len) {
    throw Exception("Sentence length of y_true and y_pred must be equal.");
  }
    double sum_log = 0.0;
    int j = 0;
    while (j < sentence_len) {
    int label = y_true[b][j];
    if (label >= vocab_size) {
    throw Exception("Label value must not be greater than vocabulary size.");
  }
    double prob = clip(y_pred[b][j][label], epsilon, 1.0);
    sum_log = sum_log + ln(prob);
    j = j + 1;
  }
    double mean_log = sum_log / to_float(sentence_len);
    double perp = exp(-mean_log);
    total_perp = total_perp + perp;
    b = b + 1;
  }
  return total_perp / to_float(batch);
}

double smooth_l1_loss(List<double> y_true, List<double> y_pred, double beta) {
  if (y_true.length != y_pred.length) {
    throw Exception("The length of the two arrays should be the same.");
  }
  double total = 0.0;
  int i = 0;
  while (i < y_true.length) {
    double diff = absf(y_true[i] - y_pred[i]);
    if (diff < beta) {
    total = total + 0.5 * diff * diff / beta;
  } else {
    total = total + diff - 0.5 * beta;
  }
    i = i + 1;
  }
  return total / to_float(y_true.length);
}

double kullback_leibler_divergence(List<double> y_true, List<double> y_pred) {
  if (y_true.length != y_pred.length) {
    throw Exception("Input arrays must have the same length.");
  }
  double total = 0.0;
  int i = 0;
  while (i < y_true.length) {
    total = total + y_true[i] * ln(y_true[i] / y_pred[i]);
    i = i + 1;
  }
  return total;
}

dynamic _main() {
  List<double> y_true_bc = [0.0, 1.0, 1.0, 0.0, 1.0];
  List<double> y_pred_bc = [0.2, 0.7, 0.9, 0.3, 0.8];
  print(binary_cross_entropy(y_true_bc, y_pred_bc, 0.000000000000001));
  print(binary_focal_cross_entropy(y_true_bc, y_pred_bc, 2.0, 0.25, 0.000000000000001));
  List<List<double>> y_true_cce = [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]];
  List<List<double>> y_pred_cce = [[0.9, 0.1, 0.0], [0.2, 0.7, 0.1], [0.0, 0.1, 0.9]];
  print(categorical_cross_entropy(y_true_cce, y_pred_cce, 0.000000000000001));
  List<double> alpha = [0.6, 0.2, 0.7];
  print(categorical_focal_cross_entropy(y_true_cce, y_pred_cce, alpha, 2.0, 0.000000000000001));
  List<dynamic> y_true_hinge = [-1.0, 1.0, 1.0, -1.0, 1.0];
  List<dynamic> y_pred_hinge = [-4.0, -0.3, 0.7, 5.0, 10.0];
  print(hinge_loss(List<double>.from(y_true_hinge), List<double>.from(y_pred_hinge)));
  List<double> y_true_huber = [0.9, 10.0, 2.0, 1.0, 5.2];
  List<double> y_pred_huber = [0.8, 2.1, 2.9, 4.2, 5.2];
  print(huber_loss(y_true_huber, y_pred_huber, 1.0));
  print(mean_squared_error(y_true_huber, y_pred_huber));
  print(mean_absolute_error(y_true_huber, y_pred_huber));
  print(mean_squared_logarithmic_error(y_true_huber, y_pred_huber));
  List<double> y_true_mape = [10.0, 20.0, 30.0, 40.0];
  List<double> y_pred_mape = [12.0, 18.0, 33.0, 45.0];
  print(mean_absolute_percentage_error(y_true_mape, y_pred_mape, 0.000000000000001));
  List<List<int>> y_true_perp = [[1, 4], [2, 3]];
  List<List<List<double>>> y_pred_perp = [[[0.28, 0.19, 0.21, 0.15, 0.17], [0.24, 0.19, 0.09, 0.18, 0.3]], [[0.03, 0.26, 0.21, 0.18, 0.32], [0.28, 0.1, 0.33, 0.15, 0.14]]];
  print(perplexity_loss(y_true_perp, y_pred_perp, 0.0000001));
  List<double> y_true_smooth = [3.0, 5.0, 2.0, 7.0];
  List<double> y_pred_smooth = [2.9, 4.8, 2.1, 7.2];
  print(smooth_l1_loss(y_true_smooth, y_pred_smooth, 1.0));
  List<double> y_true_kl = [0.2, 0.3, 0.5];
  List<double> y_pred_kl = [0.3, 0.3, 0.4];
  print(kullback_leibler_divergence(y_true_kl, y_pred_kl));
}

void _start() {
  _main();
}

void main() => _start();
