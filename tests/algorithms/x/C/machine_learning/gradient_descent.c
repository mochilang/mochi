// Generated by Mochi 0.10.32 on 2025-08-16 13:53 +0700
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <unistd.h>
#include <malloc.h>
#include <math.h>

size_t run_gradient_descent_len;

size_t append_len;
static char* str_concat(const char *a, const char *b) {
    size_t len1 = strlen(a);
    size_t len2 = strlen(b);
    char *res = malloc(len1 + len2 + 1);
    memcpy(res, a, len1);
    memcpy(res + len1, b, len2);
    res[len1 + len2] = 0;
    return res;
}

static char* str_int(long long v) {
    char buf[32];
    snprintf(buf, sizeof(buf), "%lld", v);
    return strdup(buf);
}

static char* str_float(double v) {
    char buf[64];
    snprintf(buf, sizeof(buf), "%f", v);
    char *p = buf + strlen(buf) - 1;
    while (p > buf && *p == '0') *p-- = '\0';
    if (p > buf && *p == '.') *p = '\0';
    return strdup(buf);
}

static double* list_append_double(double *arr, size_t *len, double val) {
    arr = realloc(arr, (*len + 1) * sizeof(double));
    arr[*len] = val;
    (*len)++;
    return arr;
}

#include <time.h>
static int seeded_now = 0;
static long long now_seed = 0;
static long long _now(void) {
    if (!seeded_now) {
        const char *s = getenv("MOCHI_NOW_SEED");
        if (s && *s) {
            now_seed = atoll(s);
            seeded_now = 1;
        }
    }
    if (seeded_now) {
        now_seed = (now_seed * 1664525 + 1013904223) % 2147483647;
        return now_seed;
    }
    struct timespec ts;
    clock_gettime(CLOCK_REALTIME, &ts);
    return (long long)(ts.tv_sec * 1000000000LL + ts.tv_nsec);
}

static long long _mem(void) {
    long long size = 0, rss = 0;
    FILE *f = fopen("/proc/self/statm", "r");
    if (f) {
        if (fscanf(f, "%lld %lld", &size, &rss) != 2) rss = 0;
        fclose(f);
    }
    return rss * (long long)sysconf(_SC_PAGESIZE);
}

static void panic(const char *msg) {
    fputs(msg, stderr);
    fputc('\n', stderr);
    exit(1);
}

typedef struct DataPoint DataPoint;

struct DataPoint {
    double *x;
    size_t x_len;
    double y;
};

DataPoint train_data_init[5] = {(DataPoint){.x = (double[]){5LL, 2LL, 3LL}, .x_len = 3, .y = 15LL}, (DataPoint){.x = (double[]){6LL, 5LL, 9LL}, .x_len = 3, .y = 25LL}, (DataPoint){.x = (double[]){11LL, 12LL, 13LL}, .x_len = 3, .y = 41LL}, (DataPoint){.x = (long long[]){1LL, 1LL, 1LL}, .x_len = 3, .y = 8LL}, (DataPoint){.x = (double[]){11LL, 12LL, 13LL}, .x_len = 3, .y = 41LL}};
DataPoint *train_data = train_data_init;
size_t train_data_len = 5;
DataPoint test_data_init[2] = {(DataPoint){.x = (double[]){515LL, 22LL, 13LL}, .x_len = 3, .y = 555LL}, (DataPoint){.x = (double[]){61LL, 35LL, 49LL}, .x_len = 3, .y = 150LL}};
DataPoint *test_data = test_data_init;
size_t test_data_len = 2;
double parameter_vector_init[4] = {2LL, 4LL, 1LL, 5LL};
double *parameter_vector = parameter_vector_init;
size_t parameter_vector_len = 4;

double absf(double x);
double hypothesis_value(double * input, size_t input_len, double * params, size_t params_len);
double calc_error(DataPoint dp, double * params, size_t params_len);
double summation_of_cost_derivative(long long index, double * params, size_t params_len, DataPoint * data, size_t data_len);
double get_cost_derivative(long long index, double * params, size_t params_len, DataPoint * data, size_t data_len);
long long allclose(double * a, size_t a_len, double * b, size_t b_len, double atol, double rtol);
double * run_gradient_descent(DataPoint * train_data, size_t train_data_len, double * initial_params, size_t initial_params_len);
void test_gradient_descent(DataPoint * test_data, size_t test_data_len, double * params, size_t params_len);
int main(void);

double absf(double x) {
    if (x < 0LL) {
        return -(x);
    }
    return x;
}

double hypothesis_value(double * input, size_t input_len, double * params, size_t params_len) {
    double value = params[(int)({long long _mochi_idx = 0LL; _mochi_idx < 0 ? params_len + _mochi_idx : _mochi_idx;})];
    long long i = 0LL;
    while (i < input_len) {
        value = value + (input[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? input_len + _mochi_idx : _mochi_idx;})] * params[(int)({long long _mochi_idx = i + 1LL; _mochi_idx < 0 ? params_len + _mochi_idx : _mochi_idx;})]);
        i = i + 1LL;
    }
    return value;
}

double calc_error(DataPoint dp, double * params, size_t params_len) {
    return hypothesis_value(dp.x, dp.x_len, params, params_len) - dp.y;
}

double summation_of_cost_derivative(long long index, double * params, size_t params_len, DataPoint * data, size_t data_len) {
    double sum = 0LL;
    long long i = 0LL;
    while (i < data_len) {
        DataPoint dp = data[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? data_len + _mochi_idx : _mochi_idx;})];
        double e = calc_error(dp, params, params_len);
        if (index == -1LL) {
            sum = sum + e;
        } else {
            sum = sum + (e * dp.x[(int)({long long _mochi_idx = index; _mochi_idx < 0 ? dp.x_len + _mochi_idx : _mochi_idx;})]);
        }
        i = i + 1LL;
    }
    return sum;
}

double get_cost_derivative(long long index, double * params, size_t params_len, DataPoint * data, size_t data_len) {
    return summation_of_cost_derivative(index, params, params_len, data, data_len) / (double)(data_len);
}

long long allclose(double * a, size_t a_len, double * b, size_t b_len, double atol, double rtol) {
    long long i = 0LL;
    while (i < a_len) {
        double diff = absf(a[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? a_len + _mochi_idx : _mochi_idx;})] - b[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? b_len + _mochi_idx : _mochi_idx;})]);
        double limit = atol + (rtol * absf(b[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? b_len + _mochi_idx : _mochi_idx;})]));
        if (diff > limit) {
            return 0LL;
        }
        i = i + 1LL;
    }
    return 1LL;
}

double * run_gradient_descent(DataPoint * train_data, size_t train_data_len, double * initial_params, size_t initial_params_len) {
    double learning_rate = 0.009;
    double absolute_error_limit = 2e-06;
    long long relative_error_limit = 0LL;
    long long j = 0LL;
    double *params = initial_params;
    size_t params_len = initial_params_len;
    while (1LL) {
        j = j + 1LL;
        double *temp = NULL;
        size_t temp_len = 0;
        long long i = 0LL;
        while (i < params_len) {
            double deriv = get_cost_derivative(i - 1LL, params, params_len, train_data, train_data_len);
            temp = list_append_double(temp, &temp_len, params[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? params_len + _mochi_idx : _mochi_idx;})] - (learning_rate * deriv));
            i = i + 1LL;
        }
        if (allclose(params, params_len, temp, temp_len, absolute_error_limit, relative_error_limit)) {
            puts(str_concat("Number of iterations:", str_int(j)));
            break;
        }
        params = temp;
        params_len = temp_len;
    }
    return run_gradient_descent_len = params_len, params;
}

void test_gradient_descent(DataPoint * test_data, size_t test_data_len, double * params, size_t params_len) {
    long long i = 0LL;
    while (i < test_data_len) {
        DataPoint dp = test_data[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? test_data_len + _mochi_idx : _mochi_idx;})];
        puts(str_concat("Actual output value:", str_float(dp.y)));
        puts(str_concat("Hypothesis output:", str_float(hypothesis_value(dp.x, dp.x_len, params, params_len))));
        i = i + 1LL;
    }
}

int main(void) {
    {
        long long __start = _now();
        parameter_vector = run_gradient_descent(train_data, train_data_len, parameter_vector, parameter_vector_len);
        parameter_vector_len = run_gradient_descent_len;
        puts("\nTesting gradient descent for a linear hypothesis function.\n");
        test_gradient_descent(test_data, test_data_len, parameter_vector, parameter_vector_len);
        long long __end = _now();
        long long __dur_us = (__end - __start) / 1000;
        long long __mem_bytes = _mem();
        printf("{\n  \"duration_us\": %-lld,\n  \"memory_bytes\": %-lld,\n  \"name\": \"main\"\n}\n", __dur_us, __mem_bytes);
    }
    return 0;
}
