// Generated by Mochi 0.10.32 on 2025-08-11 21:16 +0700
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <unistd.h>
#include <malloc.h>
#include <math.h>

size_t run_steep_gradient_descent_len;
size_t run_linear_regression_len;

static char* str_concat(const char *a, const char *b) {
    size_t len1 = strlen(a);
    size_t len2 = strlen(b);
    char *res = malloc(len1 + len2 + 1);
    memcpy(res, a, len1);
    memcpy(res + len1, b, len2);
    res[len1 + len2] = 0;
    return res;
}

static char* str_int(long long v) {
    char buf[32];
    snprintf(buf, sizeof(buf), "%lld", v);
    return strdup(buf);
}

static char* str_float(double v) {
    char buf[64];
    snprintf(buf, sizeof(buf), "%g", v);
    return strdup(buf);
}

static double* list_append_double(double *arr, size_t *len, double val) {
    arr = realloc(arr, (*len + 1) * sizeof(double));
    arr[*len] = val;
    (*len)++;
    return arr;
}

#include <time.h>
#include <stdlib.h>
static int seeded_now = 0;
static long long now_seed = 0;
static long long _now(void) {
    if (!seeded_now) {
        const char *s = getenv("MOCHI_NOW_SEED");
        if (s && *s) {
            now_seed = atoll(s);
            seeded_now = 1;
        }
    }
    if (seeded_now) {
        now_seed = (now_seed * 1664525 + 1013904223) % 2147483647;
        return now_seed;
    }
    struct timespec ts;
    clock_gettime(CLOCK_REALTIME, &ts);
    return (long long)(ts.tv_sec * 1000000000LL + ts.tv_nsec);
}

static long long _mem(void) {
    long long size = 0, rss = 0;
    FILE *f = fopen("/proc/self/statm", "r");
    if (f) {
        if (fscanf(f, "%lld %lld", &size, &rss) != 2) rss = 0;
        fclose(f);
    }
    return rss * (long long)sysconf(_SC_PAGESIZE);
}

static void panic(const char *msg) {
    fputs(msg, stderr);
    fputc('\n', stderr);
    exit(1);
}

static long long* list_append_long_long(long long *arr, size_t *len, long long val) {
    arr = realloc(arr, (*len + 1) * sizeof(long long));
    arr[*len] = val;
    (*len)++;
    return arr;
}

double **data_x = NULL;
size_t data_x_len = 0;
size_t *data_x_lens = NULL;
size_t data_x_lens_len = 0;
double *data_y = NULL;
size_t data_y_len = 0;
double *theta = NULL;
size_t theta_len = 0;
long long i = 0LL;
double *predicted_y = NULL;
size_t predicted_y_len = 0;
double *original_y = NULL;
size_t original_y_len = 0;
double mae = 0;

double dot(double * x, size_t x_len, double * y, size_t y_len);
double * run_steep_gradient_descent(double * * data_x, size_t data_x_len, size_t* data_x_lens, size_t data_x_lens_len, double * data_y, size_t data_y_len, long long len_data, double alpha, double * theta, size_t theta_len);
double sum_of_square_error(double * * data_x, size_t data_x_len, size_t* data_x_lens, size_t data_x_lens_len, double * data_y, size_t data_y_len, long long len_data, double * theta, size_t theta_len);
double * run_linear_regression(double * * data_x, size_t data_x_len, size_t* data_x_lens, size_t data_x_lens_len, double * data_y, size_t data_y_len);
double absf(double x);
double mean_absolute_error(double * predicted_y, size_t predicted_y_len, double * original_y, size_t original_y_len);
int main(void);

double dot(double * x, size_t x_len, double * y, size_t y_len) {
    double sum = 0.0;
    long long i = 0LL;
    while (i < x_len) {
        sum = sum + (x[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? x_len + _mochi_idx : _mochi_idx;})] * y[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? y_len + _mochi_idx : _mochi_idx;})]);
        i = i + 1LL;
    }
    return sum;
}

double * run_steep_gradient_descent(double * * data_x, size_t data_x_len, size_t* data_x_lens, size_t data_x_lens_len, double * data_y, size_t data_y_len, long long len_data, double alpha, double * theta, size_t theta_len) {
    double *gradients = NULL;
    size_t gradients_len = 0;
    long long j = 0LL;
    while (j < theta_len) {
        gradients = list_append_long_long(gradients, &gradients_len, 0.0);
        j = j + 1LL;
    }
    long long i = 0LL;
    while (i < len_data) {
        double prediction = dot(theta, theta_len, data_x[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? data_x_len + _mochi_idx : _mochi_idx;})], data_x_lens[i]);
        double error = prediction - data_y[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? data_y_len + _mochi_idx : _mochi_idx;})];
        long long k = 0LL;
        while (k < theta_len) {
            gradients[(int)(k)] = gradients[(int)({long long _mochi_idx = k; _mochi_idx < 0 ? gradients_len + _mochi_idx : _mochi_idx;})] + (error * data_x[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? data_x_len + _mochi_idx : _mochi_idx;})][(int)({long long _mochi_idx = k; _mochi_idx < 0 ? data_x_lens[i] + _mochi_idx : _mochi_idx;})]);
            k = k + 1LL;
        }
        i = i + 1LL;
    }
    double *t = NULL;
    size_t t_len = 0;
    long long g = 0LL;
    while (g < theta_len) {
        t = list_append_double(t, &t_len, theta[(int)({long long _mochi_idx = g; _mochi_idx < 0 ? theta_len + _mochi_idx : _mochi_idx;})] - ((alpha / len_data) * gradients[(int)({long long _mochi_idx = g; _mochi_idx < 0 ? gradients_len + _mochi_idx : _mochi_idx;})]));
        g = g + 1LL;
    }
    return run_steep_gradient_descent_len = t_len, t;
}

double sum_of_square_error(double * * data_x, size_t data_x_len, size_t* data_x_lens, size_t data_x_lens_len, double * data_y, size_t data_y_len, long long len_data, double * theta, size_t theta_len) {
    double total = 0.0;
    long long i = 0LL;
    while (i < len_data) {
        double prediction = dot(theta, theta_len, data_x[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? data_x_len + _mochi_idx : _mochi_idx;})], data_x_lens[i]);
        double diff = prediction - data_y[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? data_y_len + _mochi_idx : _mochi_idx;})];
        total = total + (diff * diff);
        i = i + 1LL;
    }
    return total / (2.0 * len_data);
}

double * run_linear_regression(double * * data_x, size_t data_x_len, size_t* data_x_lens, size_t data_x_lens_len, double * data_y, size_t data_y_len) {
    long long iterations = 10LL;
    double alpha = 0.01;
    long long no_features = data_x_lens[0LL];
    long long len_data = data_x_len;
    double *theta = NULL;
    size_t theta_len = 0;
    long long i = 0LL;
    while (i < no_features) {
        theta = list_append_long_long(theta, &theta_len, 0.0);
        i = i + 1LL;
    }
    long long iter = 0LL;
    while (iter < iterations) {
        theta = run_steep_gradient_descent(data_x, data_x_len, data_x_lens, data_x_len, data_y, data_y_len, len_data, alpha, theta, theta_len);
        theta_len = run_steep_gradient_descent_len;
        double error = sum_of_square_error(data_x, data_x_len, data_x_lens, data_x_len, data_y, data_y_len, len_data, theta, theta_len);
        puts(str_concat(str_concat(str_concat("At Iteration ", str_int(iter + 1LL)), " - Error is "), str_float(error)));
        iter = iter + 1LL;
    }
    return run_linear_regression_len = theta_len, theta;
}

double absf(double x) {
    if (x < 0.0) {
        return -(x);
    } else {
        return x;
    }
}

double mean_absolute_error(double * predicted_y, size_t predicted_y_len, double * original_y, size_t original_y_len) {
    double total = 0.0;
    long long i = 0LL;
    while (i < predicted_y_len) {
        double diff = absf(predicted_y[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? predicted_y_len + _mochi_idx : _mochi_idx;})] - original_y[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? original_y_len + _mochi_idx : _mochi_idx;})]);
        total = total + diff;
        i = i + 1LL;
    }
    return total / predicted_y_len;
}

int main(void) {
    {
        long long __start = _now();
        data_x = ({long long** tmp = malloc(3 * sizeof(long long*)); tmp[0] = ({long long *tmp = malloc(2 * sizeof(long long)); tmp[0] = 1.0; tmp[1] = 1.0; tmp;}); tmp[1] = ({long long *tmp = malloc(2 * sizeof(long long)); tmp[0] = 1.0; tmp[1] = 2.0; tmp;}); tmp[2] = ({long long *tmp = malloc(2 * sizeof(long long)); tmp[0] = 1.0; tmp[1] = 3.0; tmp;}); tmp;});
        data_x_len = 3;
        data_x_lens = ({size_t *tmp = malloc(3 * sizeof(size_t)); tmp[0] = 2; tmp[1] = 2; tmp[2] = 2; tmp;});
        data_x_lens_len = 3;
        data_y = ({long long *tmp = malloc(3 * sizeof(long long)); tmp[0] = 1.0; tmp[1] = 2.0; tmp[2] = 3.0; tmp;});
        data_y_len = 3;
        theta = run_linear_regression(data_x, data_x_len, data_x_lens, data_x_len, data_y, data_y_len);
        theta_len = run_linear_regression_len;
        puts("Resultant Feature vector :");
        while (i < theta_len) {
            puts(str_float(theta[(int)({long long _mochi_idx = i; _mochi_idx < 0 ? theta_len + _mochi_idx : _mochi_idx;})]));
            i = i + 1LL;
        }
        predicted_y = ({double *tmp = malloc(4 * sizeof(double)); tmp[0] = 3.0; tmp[1] = -(0.5); tmp[2] = 2.0; tmp[3] = 7.0; tmp;});
        predicted_y_len = 4;
        original_y = ({double *tmp = malloc(4 * sizeof(double)); tmp[0] = 2.5; tmp[1] = 0.0; tmp[2] = 2.0; tmp[3] = 8.0; tmp;});
        original_y_len = 4;
        mae = mean_absolute_error(predicted_y, predicted_y_len, original_y, original_y_len);
        puts(str_concat("Mean Absolute Error : ", str_float(mae)));
        long long __end = _now();
        long long __dur_us = (__end - __start) / 1000;
        long long __mem_bytes = _mem();
        printf("{\n  \"duration_us\": %-lld,\n  \"memory_bytes\": %-lld,\n  \"name\": \"main\"\n}\n", __dur_us, __mem_bytes);
    }
    return 0;
}
