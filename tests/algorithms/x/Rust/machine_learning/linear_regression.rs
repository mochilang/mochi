// Generated by Mochi transpiler v0.10.63
use std::sync::atomic::{AtomicBool, AtomicI64, Ordering};
use std::time::{SystemTime, UNIX_EPOCH};
static NOW_SEEDED: AtomicBool = AtomicBool::new(false);
static NOW_SEED: AtomicI64 = AtomicI64::new(0);
fn _now() -> i64 {
    if !NOW_SEEDED.load(Ordering::SeqCst) {
        if let Ok(s) = std::env::var("MOCHI_NOW_SEED") {
            if let Ok(v) = s.parse::<i64>() {
                NOW_SEED.store(v, Ordering::SeqCst);
                NOW_SEEDED.store(true, Ordering::SeqCst);
            }
        }
    }
    if NOW_SEEDED.load(Ordering::SeqCst) {
        let seed = (NOW_SEED.load(Ordering::SeqCst)*1664525 + 1013904223) % 2147483647;
        NOW_SEED.store(seed, Ordering::SeqCst);
        seed
    } else {
        SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_nanos() as i64
    }
}
fn _mem() -> i64 {
    if let Ok(mut f) = std::fs::File::open("/proc/self/statm") {
        let mut s = String::new();
        use std::io::Read;
        if f.read_to_string(&mut s).is_ok() {
            if let Some(rss) = s.split_whitespace().nth(1) {
                if let Ok(v) = rss.parse::<i64>() {
                    return v * 4096;
                }
            }
        }
    }
    0
}
static mut g_data_x: Vec<Vec<f64>> = Vec::new();
static mut g_data_y: Vec<f64> = Vec::new();
static mut g_i: i64 = 0;
static mut g_predicted_y: Vec<f64> = Vec::new();
static mut g_original_y: Vec<f64> = Vec::new();
fn main() {
    unsafe {
        g_data_x = vec![vec![1.0, 1.0].clone(), vec![1.0, 2.0].clone(), vec![1.0, 3.0].clone()];
        g_data_y = vec![1.0, 2.0, 3.0];
        g_i = 0;
        g_predicted_y = vec![3.0, -0.5, 2.0, 7.0];
        g_original_y = vec![2.5, 0.0, 2.0, 8.0];
                let _start: i64 = _now();
        unsafe fn dot(mut x: Vec<f64>, mut y: Vec<f64>) -> f64 {
    let mut sum: f64 = 0.0;
    let mut i: i64 = 0;
    while (i < (x.len() as i64)) {
        sum = (sum + (x[i as usize] * y[i as usize]));
        i = (i + 1);
    }
    return sum
};
        unsafe fn run_steep_gradient_descent(mut data_x: Vec<Vec<f64>>, mut data_y: Vec<f64>, mut len_data: i64, mut alpha: f64, mut theta: Vec<f64>) -> Vec<f64> {
    let mut gradients: Vec<f64> = vec![];
    let mut j: i64 = 0;
    while (j < (theta.len() as i64)) {
        gradients = { let mut _v = gradients.clone(); _v.push(0.0); _v };
        j = (j + 1);
    }
    let mut i: i64 = 0;
    while (i < len_data) {
        let mut prediction: f64 = dot(theta.clone(), data_x[i as usize].clone());
        let mut error: f64 = (prediction - data_y[i as usize]);
        let mut k: i64 = 0;
        while (k < (theta.len() as i64)) {
            gradients[k as usize] = (gradients[k as usize] + (error * data_x[i as usize].clone()[k as usize]));
            k = (k + 1);
        }
        i = (i + 1);
    }
    let mut t: Vec<f64> = vec![];
    let mut g: i64 = 0;
    while (g < (theta.len() as i64)) {
        t = { let mut _v = t.clone(); _v.push((theta[g as usize] - ((alpha / (len_data as f64)) * gradients[g as usize]))); _v };
        g = (g + 1);
    }
    return t
};
        unsafe fn sum_of_square_error(mut data_x: Vec<Vec<f64>>, mut data_y: Vec<f64>, mut len_data: i64, mut theta: Vec<f64>) -> f64 {
    let mut total: f64 = 0.0;
    let mut i: i64 = 0;
    while (i < len_data) {
        let mut prediction: f64 = dot(theta.clone(), data_x[i as usize].clone());
        let mut diff: f64 = (prediction - data_y[i as usize]);
        total = (total + (diff * diff));
        i = (i + 1);
    }
    return (total / (2.0 * (len_data as f64)))
};
        unsafe fn run_linear_regression(mut data_x: Vec<Vec<f64>>, mut data_y: Vec<f64>) -> Vec<f64> {
    let mut iterations: i64 = 10;
    let mut alpha: f64 = 0.01;
    let mut no_features: i64 = (data_x[0 as usize].clone().len() as i64);
    let mut len_data: i64 = (data_x.len() as i64);
    let mut theta: Vec<f64> = vec![];
    let mut i: i64 = 0;
    while (i < no_features) {
        theta = { let mut _v = theta.clone(); _v.push(0.0); _v };
        i = (i + 1);
    }
    let mut iter: i64 = 0;
    while (iter < iterations) {
        theta = run_steep_gradient_descent(data_x.clone(), data_y.clone(), len_data, alpha, theta.clone());
        let mut error: f64 = sum_of_square_error(data_x.clone(), data_y.clone(), len_data, theta.clone());
        println!("{}", format!("{}{}", format!("{}{}", format!("{}{}", "At Iteration ", (iter + 1).to_string()), " - Error is "), error.to_string()));
        iter = (iter + 1);
    }
    return theta
};
        unsafe fn absf(mut x: f64) -> f64 {
    if (x < 0.0) {
        return -x
    } else {
        return x
    }
};
        unsafe fn mean_absolute_error(mut predicted_y: Vec<f64>, mut original_y: Vec<f64>) -> f64 {
    let mut total: f64 = 0.0;
    let mut i: i64 = 0;
    while (i < (predicted_y.len() as i64)) {
        let mut diff: f64 = absf((predicted_y[i as usize] - original_y[i as usize]));
        total = (total + diff);
        i = (i + 1);
    }
    return (total / ((predicted_y.len() as i64) as f64))
};
        let mut theta: Vec<f64> = run_linear_regression(g_data_x.clone(), g_data_y.clone());
        println!("{}", "Resultant Feature vector :");
        while (g_i < (theta.len() as i64)) {
            println!("{}", theta[g_i as usize].to_string());
            g_i = (g_i + 1);
        }
        let mut mae: f64 = mean_absolute_error(g_predicted_y.clone(), g_original_y.clone());
        println!("{}", format!("{}{}", "Mean Absolute Error : ", mae.to_string()));
        let _end: i64 = _now();
        let duration_us: i64 = ((_end - _start) / 1000);
        let memory_bytes: i64 = _mem();
        println!("{{\n  \"duration_us\": {},\n  \"memory_bytes\": {},\n  \"name\": \"{}\"\n}}", duration_us, memory_bytes, "main");

    }
}
