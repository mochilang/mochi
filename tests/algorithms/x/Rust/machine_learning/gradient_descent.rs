// Generated by Mochi transpiler v0.10.67
use std::collections::HashMap;
use std::sync::atomic::{AtomicBool, AtomicI64, Ordering};
use std::time::{SystemTime, UNIX_EPOCH};
static NOW_SEEDED: AtomicBool = AtomicBool::new(false);
static NOW_SEED: AtomicI64 = AtomicI64::new(0);
fn _now() -> i64 {
    if !NOW_SEEDED.load(Ordering::SeqCst) {
        if let Ok(s) = std::env::var("MOCHI_NOW_SEED") {
            if let Ok(v) = s.parse::<i64>() {
                NOW_SEED.store(v, Ordering::SeqCst);
                NOW_SEEDED.store(true, Ordering::SeqCst);
            }
        }
    }
    if NOW_SEEDED.load(Ordering::SeqCst) {
        let seed = (NOW_SEED.load(Ordering::SeqCst)*1664525 + 1013904223) % 2147483647;
        NOW_SEED.store(seed, Ordering::SeqCst);
        seed
    } else {
        SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_nanos() as i64
    }
}
fn _mem() -> i64 {
    if let Ok(mut f) = std::fs::File::open("/proc/self/statm") {
        let mut s = String::new();
        use std::io::Read;
        if f.read_to_string(&mut s).is_ok() {
            if let Some(rss) = s.split_whitespace().nth(1) {
                if let Ok(v) = rss.parse::<i64>() {
                    return v * 4096;
                }
            }
        }
    }
    0
}
#[derive(Debug, Clone, Default)]
struct DataPoint {
    x: Vec<f64>,
    y: f64,
}
impl std::fmt::Display for DataPoint {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{{")?;
        write!(f, "\"x\": {:?}", self.x)?;
        write!(f, ", ")?;
        write!(f, "\"y\": {}", self.y)?;
        write!(f, "}}")
    }
}

fn main() {
        let _start: i64 = _now();
    fn absf(mut x: f64) -> f64 {
    if (x < 0.0) {
        return -x
    }
    return x
};
    fn hypothesis_value(mut input: Vec<f64>, mut params: Vec<f64>) -> f64 {
    let mut value: f64 = params[0 as usize];
    let mut i: i64 = 0;
    while (i < (input.len() as i64)) {
        value = (value + (input[i as usize] * params[(i + 1) as usize]));
        i = (i + 1);
    }
    return value
};
    let mut calc_error = |dp: DataPoint, params: Vec<f64>| -> f64 {
    return (hypothesis_value(dp.x.clone().clone(), params.clone()) - dp.y)
};
    let mut summation_of_cost_derivative = |index: i64, params: Vec<f64>, data: Vec<DataPoint>| -> f64 {
    let mut sum: f64 = 0.0;
    let mut i: i64 = 0;
    while (i < (data.len() as i64)) {
        let dp: DataPoint = data[i as usize].clone();
        let e: f64 = calc_error(dp.clone(), params.clone());
        if (index == -1) {
            sum = (sum + e);
        } else {
            sum = (sum + (e * dp.x.clone()[index as usize]));
        }
        i = (i + 1);
    }
    return sum
};
    let mut get_cost_derivative = |index: i64, params: Vec<f64>, data: Vec<DataPoint>| -> f64 {
    return (summation_of_cost_derivative(index, params.clone(), data.clone()) / ((data.len() as i64) as f64))
};
    let mut allclose = |a: Vec<f64>, b: Vec<f64>, atol: f64, rtol: f64| -> bool {
    let mut i: i64 = 0;
    while (i < (a.len() as i64)) {
        let diff: f64 = absf((a[i as usize] - b[i as usize]));
        let limit: f64 = (atol + (rtol * absf(b[i as usize])));
        if (diff > limit) {
            return false
        }
        i = (i + 1);
    }
    return true
};
    let mut run_gradient_descent = |train_data: Vec<DataPoint>, initial_params: Vec<f64>| -> Vec<f64> {
    let learning_rate: f64 = 0.009;
    let absolute_error_limit: f64 = 0.000002;
    let relative_error_limit: f64 = 0.0;
    let mut j: i64 = 0;
    let mut params: Vec<f64> = initial_params.clone();
    loop {
        j = (j + 1);
        let mut temp: Vec<f64> = vec![];
        let mut i: i64 = 0;
        while (i < (params.len() as i64)) {
            let deriv: f64 = get_cost_derivative((i - 1), params.clone(), train_data.clone());
            temp = { let mut _v = temp.clone(); _v.push((params[i as usize] - (learning_rate * deriv))); _v };
            i = (i + 1);
        }
        if allclose(params.clone(), temp.clone(), absolute_error_limit, relative_error_limit) {
            println!("{}", format!("{}{}", "Number of iterations:", j.to_string()));
            break
        }
        params = temp.clone();
    }
    return params
};
    fn test_gradient_descent(mut test_data: Vec<DataPoint>, mut params: Vec<f64>) {
    let mut i: i64 = 0;
    while (i < (test_data.len() as i64)) {
        let dp: DataPoint = test_data[i as usize].clone();
        println!("{}", format!("{}{}", "Actual output value:", dp.y.to_string()));
        println!("{}", format!("{}{}", "Hypothesis output:", hypothesis_value(dp.x.clone().clone(), params.clone()).to_string()));
        i = (i + 1);
    }
};
    let train_data: Vec<DataPoint> = vec![DataPoint {x: vec![5.0, 2.0, 3.0], y: 15.0}.clone(), DataPoint {x: vec![6.0, 5.0, 9.0], y: 25.0}.clone(), DataPoint {x: vec![11.0, 12.0, 13.0], y: 41.0}.clone(), DataPoint {x: vec![1.0, 1.0, 1.0], y: 8.0}.clone(), DataPoint {x: vec![11.0, 12.0, 13.0], y: 41.0}.clone()];
    let test_data: Vec<DataPoint> = vec![DataPoint {x: vec![515.0, 22.0, 13.0], y: 555.0}.clone(), DataPoint {x: vec![61.0, 35.0, 49.0], y: 150.0}.clone()];
    let mut parameter_vector: Vec<f64> = vec![2.0, 4.0, 1.0, 5.0];
    parameter_vector = run_gradient_descent(train_data.clone(), parameter_vector.clone());
    println!("{}", "\nTesting gradient descent for a linear hypothesis function.\n");
    test_gradient_descent(test_data.clone(), parameter_vector.clone());
    let _end: i64 = _now();
    let duration_us: i64 = ((_end - _start) / 1000);
    let memory_bytes: i64 = _mem();
    println!("{{\n  \"duration_us\": {},\n  \"memory_bytes\": {},\n  \"name\": \"{}\"\n}}", duration_us, memory_bytes, "main");

}
