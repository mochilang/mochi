// Generated by Mochi transpiler v0.10.63
use std::sync::atomic::{AtomicBool, AtomicI64, Ordering};
use std::time::{SystemTime, UNIX_EPOCH};
static NOW_SEEDED: AtomicBool = AtomicBool::new(false);
static NOW_SEED: AtomicI64 = AtomicI64::new(0);
fn _now() -> i64 {
    if !NOW_SEEDED.load(Ordering::SeqCst) {
        if let Ok(s) = std::env::var("MOCHI_NOW_SEED") {
            if let Ok(v) = s.parse::<i64>() {
                NOW_SEED.store(v, Ordering::SeqCst);
                NOW_SEEDED.store(true, Ordering::SeqCst);
            }
        }
    }
    if NOW_SEEDED.load(Ordering::SeqCst) {
        let seed = (NOW_SEED.load(Ordering::SeqCst)*1664525 + 1013904223) % 2147483647;
        NOW_SEED.store(seed, Ordering::SeqCst);
        seed
    } else {
        SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_nanos() as i64
    }
}
fn _mem() -> i64 {
    if let Ok(mut f) = std::fs::File::open("/proc/self/statm") {
        let mut s = String::new();
        use std::io::Read;
        if f.read_to_string(&mut s).is_ok() {
            if let Some(rss) = s.split_whitespace().nth(1) {
                if let Ok(v) = rss.parse::<i64>() {
                    return v * 4096;
                }
            }
        }
    }
    0
}
fn int(x: i64) -> i64 { x }
fn main() {
        let _start: i64 = _now();
    fn absf(mut x: f64) -> f64 {
    if (x < 0.0) {
        return -x
    }
    return x
};
    fn maxf(mut a: f64, mut b: f64) -> f64 {
    if (a > b) {
        return a
    }
    return b
};
    fn minf(mut a: f64, mut b: f64) -> f64 {
    if (a < b) {
        return a
    }
    return b
};
    fn clip(mut x: f64, mut lo: f64, mut hi: f64) -> f64 {
    return maxf(lo, minf(x, hi))
};
    fn to_float(mut x: i64) -> f64 {
    return ((x as f64) * 1.0)
};
    fn powf(mut base: f64, mut exp: f64) -> f64 {
    let mut result: f64 = 1.0;
    let mut i: i64 = 0;
    let mut n: i64 = (exp as i64);
    while (i < n) {
        result = (result * base);
        i = (i + 1);
    }
    return result
};
    fn ln(mut x: f64) -> f64 {
    if (x <= 0.0) {
        panic!("ln domain error");
    }
    let mut y: f64 = ((x - 1.0) / (x + 1.0));
    let mut y2: f64 = (y * y);
    let mut term: f64 = y;
    let mut sum: f64 = 0.0;
    let mut k: i64 = 0;
    while (k < 10) {
        let mut denom: f64 = to_float(((2 * k) + 1));
        sum = (sum + (term / denom));
        term = (term * y2);
        k = (k + 1);
    }
    return (2.0 * sum)
};
    fn exp(mut x: f64) -> f64 {
    let mut term: f64 = 1.0;
    let mut sum: f64 = 1.0;
    let mut n: i64 = 1;
    while (n < 20) {
        term = ((term * x) / to_float(n));
        sum = (sum + term);
        n = (n + 1);
    }
    return sum
};
    fn mean(mut v: Vec<f64>) -> f64 {
    let mut total: f64 = 0.0;
    let mut i: i64 = 0;
    while (i < (v.len() as i64)) {
        total = (total + v[i as usize]);
        i = (i + 1);
    }
    return (total / to_float((v.len() as i64)))
};
    fn binary_cross_entropy(mut y_true: Vec<f64>, mut y_pred: Vec<f64>, mut epsilon: f64) -> f64 {
    if ((y_true.len() as i64) != (y_pred.len() as i64)) {
        panic!("Input arrays must have the same length.");
    }
    let mut losses: Vec<f64> = vec![];
    let mut i: i64 = 0;
    while (i < (y_true.len() as i64)) {
        let mut yt: f64 = y_true[i as usize];
        let mut yp: f64 = clip(y_pred[i as usize], epsilon, (1.0 - epsilon));
        let mut loss: f64 = -((yt * ln(yp)) + ((1.0 - yt) * ln((1.0 - yp))));
        losses = { let mut _v = losses.clone(); _v.push(loss); _v };
        i = (i + 1);
    }
    return mean(losses.clone())
};
    fn binary_focal_cross_entropy(mut y_true: Vec<f64>, mut y_pred: Vec<f64>, mut gamma: f64, mut alpha: f64, mut epsilon: f64) -> f64 {
    if ((y_true.len() as i64) != (y_pred.len() as i64)) {
        panic!("Input arrays must have the same length.");
    }
    let mut losses: Vec<f64> = vec![];
    let mut i: i64 = 0;
    while (i < (y_true.len() as i64)) {
        let mut yt: f64 = y_true[i as usize];
        let mut yp: f64 = clip(y_pred[i as usize], epsilon, (1.0 - epsilon));
        let mut term1: f64 = (((alpha * powf((1.0 - yp), gamma)) * yt) * ln(yp));
        let mut term2: f64 = ((((1.0 - alpha) * powf(yp, gamma)) * (1.0 - yt)) * ln((1.0 - yp)));
        losses = { let mut _v = losses.clone(); _v.push(-(term1 + term2)); _v };
        i = (i + 1);
    }
    return mean(losses.clone())
};
    fn categorical_cross_entropy(mut y_true: Vec<Vec<f64>>, mut y_pred: Vec<Vec<f64>>, mut epsilon: f64) -> f64 {
    if ((y_true.len() as i64) != (y_pred.len() as i64)) {
        panic!("Input arrays must have the same shape.");
    }
    let mut rows: i64 = (y_true.len() as i64);
    let mut total: f64 = 0.0;
    let mut i: i64 = 0;
    while (i < rows) {
        if ((y_true[i as usize].clone().len() as i64) != (y_pred[i as usize].clone().len() as i64)) {
            panic!("Input arrays must have the same shape.");
        }
        let mut sum_true: f64 = 0.0;
        let mut sum_pred: f64 = 0.0;
        let mut j: i64 = 0;
        while (j < (y_true[i as usize].clone().len() as i64)) {
            let mut yt: f64 = y_true[i as usize].clone()[j as usize];
            let mut yp: f64 = y_pred[i as usize].clone()[j as usize];
            if ((yt != 0.0) && (yt != 1.0)) {
                panic!("y_true must be one-hot encoded.");
            }
            sum_true = (sum_true + yt);
            sum_pred = (sum_pred + yp);
            j = (j + 1);
        }
        if (sum_true != 1.0) {
            panic!("y_true must be one-hot encoded.");
        }
        if (absf((sum_pred - 1.0)) > epsilon) {
            panic!("Predicted probabilities must sum to approximately 1.");
        }
        j = 0;
        while (j < (y_true[i as usize].clone().len() as i64)) {
            let mut yp: f64 = clip(y_pred[i as usize].clone()[j as usize], epsilon, 1.0);
            total = (total - (y_true[i as usize].clone()[j as usize] * ln(yp)));
            j = (j + 1);
        }
        i = (i + 1);
    }
    return total
};
    fn categorical_focal_cross_entropy(mut y_true: Vec<Vec<f64>>, mut y_pred: Vec<Vec<f64>>, mut alpha: Vec<f64>, mut gamma: f64, mut epsilon: f64) -> f64 {
    if ((y_true.len() as i64) != (y_pred.len() as i64)) {
        panic!("Shape of y_true and y_pred must be the same.");
    }
    let mut rows: i64 = (y_true.len() as i64);
    let mut cols: i64 = (y_true[0 as usize].clone().len() as i64);
    let mut a: Vec<f64> = alpha.clone();
    if ((a.len() as i64) == 0) {
        let mut tmp: Vec<f64> = vec![];
        let mut j: i64 = 0;
        while (j < cols) {
            tmp = { let mut _v = tmp.clone(); _v.push(1.0); _v };
            j = (j + 1);
        }
        a = tmp.clone();
    }
    if ((a.len() as i64) != cols) {
        panic!("Length of alpha must match the number of classes.");
    }
    let mut total: f64 = 0.0;
    let mut i: i64 = 0;
    while (i < rows) {
        if (((y_true[i as usize].clone().len() as i64) != cols) || ((y_pred[i as usize].clone().len() as i64) != cols)) {
            panic!("Shape of y_true and y_pred must be the same.");
        }
        let mut sum_true: f64 = 0.0;
        let mut sum_pred: f64 = 0.0;
        let mut j: i64 = 0;
        while (j < cols) {
            let mut yt: f64 = y_true[i as usize].clone()[j as usize];
            let mut yp: f64 = y_pred[i as usize].clone()[j as usize];
            if ((yt != 0.0) && (yt != 1.0)) {
                panic!("y_true must be one-hot encoded.");
            }
            sum_true = (sum_true + yt);
            sum_pred = (sum_pred + yp);
            j = (j + 1);
        }
        if (sum_true != 1.0) {
            panic!("y_true must be one-hot encoded.");
        }
        if (absf((sum_pred - 1.0)) > epsilon) {
            panic!("Predicted probabilities must sum to approximately 1.");
        }
        let mut row_loss: f64 = 0.0;
        j = 0;
        while (j < cols) {
            let mut yp: f64 = clip(y_pred[i as usize].clone()[j as usize], epsilon, 1.0);
            row_loss = (row_loss + (((a[j as usize] * powf((1.0 - yp), gamma)) * y_true[i as usize].clone()[j as usize]) * ln(yp)));
            j = (j + 1);
        }
        total = (total - row_loss);
        i = (i + 1);
    }
    return (total / to_float(rows))
};
    fn hinge_loss(mut y_true: Vec<f64>, mut y_pred: Vec<f64>) -> f64 {
    if ((y_true.len() as i64) != (y_pred.len() as i64)) {
        panic!("Length of predicted and actual array must be same.");
    }
    let mut losses: Vec<f64> = vec![];
    let mut i: i64 = 0;
    while (i < (y_true.len() as i64)) {
        let mut yt: f64 = y_true[i as usize];
        if ((yt != -1.0) && (yt != 1.0)) {
            panic!("y_true can have values -1 or 1 only.");
        }
        let mut pred: f64 = y_pred[i as usize];
        let mut l: f64 = maxf(0.0, (1.0 - (yt * pred)));
        losses = { let mut _v = losses.clone(); _v.push(l); _v };
        i = (i + 1);
    }
    return mean(losses.clone())
};
    fn huber_loss(mut y_true: Vec<f64>, mut y_pred: Vec<f64>, mut delta: f64) -> f64 {
    if ((y_true.len() as i64) != (y_pred.len() as i64)) {
        panic!("Input arrays must have the same length.");
    }
    let mut total: f64 = 0.0;
    let mut i: i64 = 0;
    while (i < (y_true.len() as i64)) {
        let mut diff: f64 = (y_true[i as usize] - y_pred[i as usize]);
        let mut adiff: f64 = absf(diff);
        if (adiff <= delta) {
            total = (total + ((0.5 * diff) * diff));
        } else {
            total = (total + (delta * (adiff - (0.5 * delta))));
        }
        i = (i + 1);
    }
    return (total / to_float((y_true.len() as i64)))
};
    fn mean_squared_error(mut y_true: Vec<f64>, mut y_pred: Vec<f64>) -> f64 {
    if ((y_true.len() as i64) != (y_pred.len() as i64)) {
        panic!("Input arrays must have the same length.");
    }
    let mut losses: Vec<f64> = vec![];
    let mut i: i64 = 0;
    while (i < (y_true.len() as i64)) {
        let mut diff: f64 = (y_true[i as usize] - y_pred[i as usize]);
        losses = { let mut _v = losses.clone(); _v.push((diff * diff)); _v };
        i = (i + 1);
    }
    return mean(losses.clone())
};
    fn mean_absolute_error(mut y_true: Vec<f64>, mut y_pred: Vec<f64>) -> f64 {
    if ((y_true.len() as i64) != (y_pred.len() as i64)) {
        panic!("Input arrays must have the same length.");
    }
    let mut total: f64 = 0.0;
    let mut i: i64 = 0;
    while (i < (y_true.len() as i64)) {
        total = (total + absf((y_true[i as usize] - y_pred[i as usize])));
        i = (i + 1);
    }
    return (total / to_float((y_true.len() as i64)))
};
    fn mean_squared_logarithmic_error(mut y_true: Vec<f64>, mut y_pred: Vec<f64>) -> f64 {
    if ((y_true.len() as i64) != (y_pred.len() as i64)) {
        panic!("Input arrays must have the same length.");
    }
    let mut total: f64 = 0.0;
    let mut i: i64 = 0;
    while (i < (y_true.len() as i64)) {
        let mut a: f64 = ln((1.0 + y_true[i as usize]));
        let mut b: f64 = ln((1.0 + y_pred[i as usize]));
        let mut diff: f64 = (a - b);
        total = (total + (diff * diff));
        i = (i + 1);
    }
    return (total / to_float((y_true.len() as i64)))
};
    fn mean_absolute_percentage_error(mut y_true: Vec<f64>, mut y_pred: Vec<f64>, mut epsilon: f64) -> f64 {
    if ((y_true.len() as i64) != (y_pred.len() as i64)) {
        panic!("The length of the two arrays should be the same.");
    }
    let mut total: f64 = 0.0;
    let mut i: i64 = 0;
    while (i < (y_true.len() as i64)) {
        let mut yt: f64 = y_true[i as usize];
        if (yt == 0.0) {
            yt = epsilon;
        }
        total = (total + absf(((yt - y_pred[i as usize]) / yt)));
        i = (i + 1);
    }
    return (total / to_float((y_true.len() as i64)))
};
    fn perplexity_loss(mut y_true: Vec<Vec<i64>>, mut y_pred: Vec<Vec<Vec<f64>>>, mut epsilon: f64) -> f64 {
    let mut batch: i64 = (y_true.len() as i64);
    if (batch != (y_pred.len() as i64)) {
        panic!("Batch size of y_true and y_pred must be equal.");
    }
    let mut sentence_len: i64 = (y_true[0 as usize].clone().len() as i64);
    if (sentence_len != (y_pred[0 as usize].clone().len() as i64)) {
        panic!("Sentence length of y_true and y_pred must be equal.");
    }
    let mut vocab_size: i64 = (y_pred[0 as usize].clone()[0 as usize].clone().len() as i64);
    let mut b: i64 = 0;
    let mut total_perp: f64 = 0.0;
    while (b < batch) {
        if (((y_true[b as usize].clone().len() as i64) != sentence_len) || ((y_pred[b as usize].clone().len() as i64) != sentence_len)) {
            panic!("Sentence length of y_true and y_pred must be equal.");
        }
        let mut sum_log: f64 = 0.0;
        let mut j: i64 = 0;
        while (j < sentence_len) {
            let mut label: i64 = y_true[b as usize].clone()[j as usize];
            if (label >= vocab_size) {
                panic!("Label value must not be greater than vocabulary size.");
            }
            let mut prob: f64 = clip(y_pred[b as usize].clone()[j as usize].clone()[label as usize], epsilon, 1.0);
            sum_log = (sum_log + ln(prob));
            j = (j + 1);
        }
        let mut mean_log: f64 = (sum_log / to_float(sentence_len));
        let mut perp: f64 = exp(-mean_log);
        total_perp = (total_perp + perp);
        b = (b + 1);
    }
    return (total_perp / to_float(batch))
};
    fn smooth_l1_loss(mut y_true: Vec<f64>, mut y_pred: Vec<f64>, mut beta: f64) -> f64 {
    if ((y_true.len() as i64) != (y_pred.len() as i64)) {
        panic!("The length of the two arrays should be the same.");
    }
    let mut total: f64 = 0.0;
    let mut i: i64 = 0;
    while (i < (y_true.len() as i64)) {
        let mut diff: f64 = absf((y_true[i as usize] - y_pred[i as usize]));
        if (diff < beta) {
            total = (total + (((0.5 * diff) * diff) / beta));
        } else {
            total = ((total + diff) - (0.5 * beta));
        }
        i = (i + 1);
    }
    return (total / to_float((y_true.len() as i64)))
};
    fn kullback_leibler_divergence(mut y_true: Vec<f64>, mut y_pred: Vec<f64>) -> f64 {
    if ((y_true.len() as i64) != (y_pred.len() as i64)) {
        panic!("Input arrays must have the same length.");
    }
    let mut total: f64 = 0.0;
    let mut i: i64 = 0;
    while (i < (y_true.len() as i64)) {
        total = (total + (y_true[i as usize] * ln((y_true[i as usize] / y_pred[i as usize]))));
        i = (i + 1);
    }
    return total
};
    fn mochi_main() {
    let mut y_true_bc: Vec<f64> = vec![0.0, 1.0, 1.0, 0.0, 1.0];
    let mut y_pred_bc: Vec<f64> = vec![0.2, 0.7, 0.9, 0.3, 0.8];
    println!("{}", format!("{:?}", binary_cross_entropy(y_true_bc.clone(), y_pred_bc.clone(), 0.000000000000001)));
    println!("{}", format!("{:?}", binary_focal_cross_entropy(y_true_bc.clone(), y_pred_bc.clone(), 2.0, 0.25, 0.000000000000001)));
    let mut y_true_cce: Vec<Vec<f64>> = vec![vec![1.0, 0.0, 0.0].clone(), vec![0.0, 1.0, 0.0].clone(), vec![0.0, 0.0, 1.0].clone()];
    let mut y_pred_cce: Vec<Vec<f64>> = vec![vec![0.9, 0.1, 0.0].clone(), vec![0.2, 0.7, 0.1].clone(), vec![0.0, 0.1, 0.9].clone()];
    println!("{}", format!("{:?}", categorical_cross_entropy(y_true_cce.clone(), y_pred_cce.clone(), 0.000000000000001)));
    let mut alpha: Vec<f64> = vec![0.6, 0.2, 0.7];
    println!("{}", format!("{:?}", categorical_focal_cross_entropy(y_true_cce.clone(), y_pred_cce.clone(), alpha.clone(), 2.0, 0.000000000000001)));
    let mut y_true_hinge: Vec<f64> = vec![-1.0, 1.0, 1.0, -1.0, 1.0];
    let mut y_pred_hinge: Vec<f64> = vec![-4.0, -0.3, 0.7, 5.0, 10.0];
    println!("{}", format!("{:?}", hinge_loss(y_true_hinge.clone(), y_pred_hinge.clone())));
    let mut y_true_huber: Vec<f64> = vec![0.9, 10.0, 2.0, 1.0, 5.2];
    let mut y_pred_huber: Vec<f64> = vec![0.8, 2.1, 2.9, 4.2, 5.2];
    println!("{}", format!("{:?}", huber_loss(y_true_huber.clone(), y_pred_huber.clone(), 1.0)));
    println!("{}", format!("{:?}", mean_squared_error(y_true_huber.clone(), y_pred_huber.clone())));
    println!("{}", format!("{:?}", mean_absolute_error(y_true_huber.clone(), y_pred_huber.clone())));
    println!("{}", format!("{:?}", mean_squared_logarithmic_error(y_true_huber.clone(), y_pred_huber.clone())));
    let mut y_true_mape: Vec<f64> = vec![10.0, 20.0, 30.0, 40.0];
    let mut y_pred_mape: Vec<f64> = vec![12.0, 18.0, 33.0, 45.0];
    println!("{}", format!("{:?}", mean_absolute_percentage_error(y_true_mape.clone(), y_pred_mape.clone(), 0.000000000000001)));
    let mut y_true_perp: Vec<Vec<i64>> = vec![vec![1, 4].clone(), vec![2, 3].clone()];
    let mut y_pred_perp: Vec<Vec<Vec<f64>>> = vec![vec![vec![0.28, 0.19, 0.21, 0.15, 0.17].clone(), vec![0.24, 0.19, 0.09, 0.18, 0.3].clone()].clone(), vec![vec![0.03, 0.26, 0.21, 0.18, 0.32].clone(), vec![0.28, 0.1, 0.33, 0.15, 0.14].clone()].clone()];
    println!("{}", format!("{:?}", perplexity_loss(y_true_perp.clone(), y_pred_perp.clone(), 0.0000001)));
    let mut y_true_smooth: Vec<f64> = vec![3.0, 5.0, 2.0, 7.0];
    let mut y_pred_smooth: Vec<f64> = vec![2.9, 4.8, 2.1, 7.2];
    println!("{}", format!("{:?}", smooth_l1_loss(y_true_smooth.clone(), y_pred_smooth.clone(), 1.0)));
    let mut y_true_kl: Vec<f64> = vec![0.2, 0.3, 0.5];
    let mut y_pred_kl: Vec<f64> = vec![0.3, 0.3, 0.4];
    println!("{}", format!("{:?}", kullback_leibler_divergence(y_true_kl.clone(), y_pred_kl.clone())));
};
    mochi_main();
    let _end: i64 = _now();
    let duration_us: i64 = ((_end - _start) / 1000);
    let memory_bytes: i64 = _mem();
    println!("{{\n  \"duration_us\": {},\n  \"memory_bytes\": {},\n  \"name\": \"{}\"\n}}", duration_us, memory_bytes, "main");

}
