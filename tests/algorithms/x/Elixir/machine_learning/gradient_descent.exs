# Code generated by Mochi transpiler 2025-08-08 20:44 +0700
defmodule Main do
  defp _len(x) do
    cond do
      x == nil -> 0
      is_binary(x) -> String.length(x)
      true -> length(x)
    end
  end
  def absf(x) do
    try do
      throw {:return, ((if x < 0.0, do: -x, else: x))}
    catch
      {:return, val} -> val
    end
  end
  def hypothesis_value(input, params) do
    try do
      value = Enum.at(params, 0)
      i = 0
      while_fun = fn while_fun, i, value ->
        if i < _len(input) do
          value = value + Enum.at(input, i) * Enum.at(params, i + 1)
          i = i + 1
          while_fun.(while_fun, i, value)
        else
          {i, value}
        end
      end
      {i, value} = try do
          while_fun.(while_fun, i, value)
        catch
          {:break, {i, value}} -> {i, value}
        end

      throw {:return, value}
    catch
      {:return, val} -> val
    end
  end
  def calc_error(dp, params) do
    try do
      throw {:return, hypothesis_value(dp.x, params) - dp.y}
    catch
      {:return, val} -> val
    end
  end
  def summation_of_cost_derivative(index, params, data) do
    try do
      sum = 0.0
      i = 0
      while_fun_2 = fn while_fun_2, i, sum ->
        if i < _len(data) do
          dp = Enum.at(data, i)
          e = calc_error(dp, params)
          sum = (if index == (-1), do: sum + e, else: sum + e * Enum.at(dp.x, index))
          i = i + 1
          while_fun_2.(while_fun_2, i, sum)
        else
          {i, sum}
        end
      end
      {i, sum} = try do
          while_fun_2.(while_fun_2, i, sum)
        catch
          {:break, {i, sum}} -> {i, sum}
        end

      throw {:return, sum}
    catch
      {:return, val} -> val
    end
  end
  def get_cost_derivative(index, params, data) do
    try do
      throw {:return, summation_of_cost_derivative(index, params, data) / (:erlang.float(_len(data)))}
    catch
      {:return, val} -> val
    end
  end
  def allclose(a, b, atol, rtol) do
    try do
      i = 0
      while_fun_3 = fn while_fun_3, i ->
        if i < _len(a) do
          diff = absf(Enum.at(a, i) - Enum.at(b, i))
          limit = atol + rtol * absf(Enum.at(b, i))
          if diff > limit do
            throw {:return, false}
          end
          i = i + 1
          while_fun_3.(while_fun_3, i)
        else
          i
        end
      end
      i = try do
          while_fun_3.(while_fun_3, i)
        catch
          {:break, {i}} -> i
        end

      throw {:return, true}
    catch
      {:return, val} -> val
    end
  end
  def run_gradient_descent(train_data, initial_params) do
    try do
      learning_rate = 0.009
      absolute_error_limit = 0.000002
      relative_error_limit = 0.0
      j = 0
      params = initial_params
      while_fun_4 = fn while_fun_4, j, params ->
        if true do
          j = j + 1
          temp = []
          i = 0
          while_fun_5 = fn while_fun_5, i, temp ->
            if i < _len(params) do
              deriv = get_cost_derivative(i - 1, params, train_data)
              temp = (temp ++ [Enum.at(params, i) - learning_rate * deriv])
              i = i + 1
              while_fun_5.(while_fun_5, i, temp)
            else
              {i, temp}
            end
          end
          {i, temp} = try do
              while_fun_5.(while_fun_5, i, temp)
            catch
              {:break, {i, temp}} -> {i, temp}
            end

          if allclose(params, temp, absolute_error_limit, relative_error_limit) do
            IO.puts(("Number of iterations:" <> Kernel.to_string(j)))
            throw {:break, {j, params}}
          end
          params = temp
          while_fun_4.(while_fun_4, j, params)
        else
          {j, params}
        end
      end
      {j, params} = try do
          while_fun_4.(while_fun_4, j, params)
        catch
          {:break, {j, params}} -> {j, params}
        end

      throw {:return, params}
    catch
      {:return, val} -> val
    end
  end
  def test_gradient_descent(test_data, params) do
    try do
      i = 0
      while_fun_6 = fn while_fun_6, i ->
        if i < _len(test_data) do
          dp = Enum.at(test_data, i)
          IO.puts(("Actual output value:" <> Kernel.to_string(dp.y)))
          IO.puts(("Hypothesis output:" <> Kernel.inspect(hypothesis_value(dp.x, params))))
          i = i + 1
          while_fun_6.(while_fun_6, i)
        else
          i
        end
      end
      i = try do
          while_fun_6.(while_fun_6, i)
        catch
          {:break, {i}} -> i
        end

    catch
      {:return, val} -> val
    end
  end
  Process.put(:train_data, [%{x: [5.0, 2.0, 3.0], y: 15.0}, %{x: [6.0, 5.0, 9.0], y: 25.0}, %{x: [11.0, 12.0, 13.0], y: 41.0}, %{x: [1.0, 1.0, 1.0], y: 8.0}, %{x: [11.0, 12.0, 13.0], y: 41.0}])
  Process.put(:test_data, [%{x: [515.0, 22.0, 13.0], y: 555.0}, %{x: [61.0, 35.0, 49.0], y: 150.0}])
  Process.put(:parameter_vector, [2.0, 4.0, 1.0, 5.0])
  def main() do
    Process.put(:parameter_vector, run_gradient_descent(Process.get(:train_data), Process.get(:parameter_vector)))
    IO.puts("\nTesting gradient descent for a linear hypothesis function.\n")
    test_gradient_descent(Process.get(:test_data), Process.get(:parameter_vector))
  end
end
Main.main()
