# Code generated by Mochi transpiler 2025-08-17 08:49 +0700
defmodule Main do
  defp _bench_now() do
    System.monotonic_time(:microsecond)
  end
  defp _mem() do
    :erlang.process_info(self(), :memory) |> elem(1)
  end
  defp _len(x) do
    cond do
      x == nil -> 0
      is_binary(x) -> String.length(x)
      true -> length(x)
    end
  end
  def absf(x) do
    try do
      throw {:return, ((if x < 0.0, do: -x, else: x))}
    catch
      {:return, val} -> val
    end
  end
  def hypothesis_value(input, params) do
    try do
      value = Enum.at(params, 0, [])
      i = 0
      while_fun = fn while_fun, i, value ->
        if i < _len(input) do
          value = value + Enum.at(input, i, []) * Enum.at(params, i + 1, [])
          i = i + 1
          while_fun.(while_fun, i, value)
        else
          {i, value}
        end
      end
      {_, value} = try do
          while_fun.(while_fun, i, value)
        catch
          {:break, {_, value}} -> {0, value}
        end

      throw {:return, value}
    catch
      {:return, val} -> val
    end
  end
  def calc_error(dp, params) do
    try do
      throw {:return, hypothesis_value(dp.x, params) - dp.y}
    catch
      {:return, val} -> val
    end
  end
  def summation_of_cost_derivative(index, params, data) do
    try do
      sum = 0.0
      i = 0
      while_fun_2 = fn while_fun_2, i, sum ->
        if i < _len(data) do
          dp = Enum.at(data, i, [])
          e = calc_error(dp, params)
          sum = (if index == (-1), do: sum + e, else: sum + e * Enum.at(dp.x, index, 0))
          i = i + 1
          while_fun_2.(while_fun_2, i, sum)
        else
          {i, sum}
        end
      end
      {_, sum} = try do
          while_fun_2.(while_fun_2, i, sum)
        catch
          {:break, {_, sum}} -> {0, sum}
        end

      throw {:return, sum}
    catch
      {:return, val} -> val
    end
  end
  def get_cost_derivative(index, params, data) do
    try do
      throw {:return, summation_of_cost_derivative(index, params, data) / (:erlang.float(_len(data)))}
    catch
      {:return, val} -> val
    end
  end
  def allclose(a, b, atol, rtol) do
    try do
      i = 0
      while_fun_3 = fn while_fun_3, i ->
        if i < _len(a) do
          diff = absf(Enum.at(a, i, []) - Enum.at(b, i, []))
          limit = atol + rtol * absf(Enum.at(b, i, []))
          if diff > limit do
            throw {:return, false}
          end
          i = i + 1
          while_fun_3.(while_fun_3, i)
        else
          i
        end
      end
      _ = try do
          while_fun_3.(while_fun_3, i)
        catch
          {:break, {_}} -> 0
        end

      throw {:return, true}
    catch
      {:return, val} -> val
    end
  end
  def run_gradient_descent(train_data, initial_params) do
    try do
      learning_rate = 0.009
      absolute_error_limit = 0.000002
      relative_error_limit = 0.0
      j = 0
      params = initial_params
      while_fun_4 = fn while_fun_4, j, params ->
        if true do
          j = j + 1
          temp = []
          i = 0
          while_fun_5 = fn while_fun_5, i, temp ->
            if i < _len(params) do
              deriv = get_cost_derivative(i - 1, params, train_data)
              temp = (temp ++ [Enum.at(params, i, []) - learning_rate * deriv])
              i = i + 1
              while_fun_5.(while_fun_5, i, temp)
            else
              {i, temp}
            end
          end
          {_, temp} = try do
              while_fun_5.(while_fun_5, i, temp)
            catch
              {:break, {_, temp}} -> {0, temp}
            end

          if allclose(params, temp, absolute_error_limit, relative_error_limit) do
            IO.puts(("Number of iterations:" <> Kernel.to_string(j)))
            throw {:break, {j, params}}
          end
          params = temp
          while_fun_4.(while_fun_4, j, params)
        else
          {j, params}
        end
      end
      {_, params} = try do
          while_fun_4.(while_fun_4, j, params)
        catch
          {:break, {_, params}} -> {0, params}
        end

      throw {:return, params}
    catch
      {:return, val} -> val
    end
  end
  def test_gradient_descent(test_data, params) do
    try do
      i = 0
      while_fun_6 = fn while_fun_6, i ->
        if i < _len(test_data) do
          dp = Enum.at(test_data, i, [])
          IO.puts(("Actual output value:" <> Kernel.to_string(dp.y)))
          IO.puts(("Hypothesis output:" <> Kernel.inspect(hypothesis_value(dp.x, params))))
          i = i + 1
          while_fun_6.(while_fun_6, i)
        else
          i
        end
      end
      _ = try do
          while_fun_6.(while_fun_6, i)
        catch
          {:break, {_}} -> 0
        end

    catch
      {:return, val} -> val
    end
  end
  Process.put(:train_data, [%{x: [5.0, 2.0, 3.0], y: 15.0}, %{x: [6.0, 5.0, 9.0], y: 25.0}, %{x: [11.0, 12.0, 13.0], y: 41.0}, %{x: [1.0, 1.0, 1.0], y: 8.0}, %{x: [11.0, 12.0, 13.0], y: 41.0}])
  Process.put(:test_data, [%{x: [515.0, 22.0, 13.0], y: 555.0}, %{x: [61.0, 35.0, 49.0], y: 150.0}])
  Process.put(:parameter_vector, [2.0, 4.0, 1.0, 5.0])
  def main() do
    :erlang.garbage_collect()
    mem_start = _mem()
    t_start = _bench_now()
    Process.put(:parameter_vector, run_gradient_descent(Process.get(:train_data), Process.get(:parameter_vector)))
    IO.puts("\nTesting gradient descent for a linear hypothesis function.\n")
    test_gradient_descent(Process.get(:test_data), Process.get(:parameter_vector))
    mem_end = _mem()
    duration_us = max(_bench_now() - t_start, 1)
    :erlang.garbage_collect()
    mem_diff = abs(mem_end - mem_start)
    IO.puts("{\n  \"duration_us\": #{duration_us},\n  \"memory_bytes\": #{mem_diff},\n  \"name\": \"main\"\n}")
  end
end
Main.main()
