# Code generated by Mochi transpiler 2025-08-08 20:44 +0700
defmodule Main do
  defp _bench_now() do
    System.monotonic_time(:microsecond)
  end
  defp _mem() do
    :erlang.process_info(self(), :memory) |> elem(1)
  end
  defp _len(x) do
    cond do
      x == nil -> 0
      is_binary(x) -> String.length(x)
      true -> length(x)
    end
  end
  def absf(x) do
    try do
      throw {:return, ((if x < 0.0, do: -x, else: x))}
    catch
      {:return, val} -> val
    end
  end
  def maxf(a, b) do
    try do
      throw {:return, ((if a > b, do: a, else: b))}
    catch
      {:return, val} -> val
    end
  end
  def minf(a, b) do
    try do
      throw {:return, ((if a < b, do: a, else: b))}
    catch
      {:return, val} -> val
    end
  end
  def clip(x, lo, hi) do
    try do
      throw {:return, maxf(lo, minf(x, hi))}
    catch
      {:return, val} -> val
    end
  end
  def to_float(x) do
    try do
      throw {:return, x * 1.0}
    catch
      {:return, val} -> val
    end
  end
  def powf(base, exp) do
    try do
      result = 1.0
      i = 0
      n = Kernel.trunc(&exp/1)
      while_fun = fn while_fun, i, result ->
        if i < n do
          result = result * base
          i = i + 1
          while_fun.(while_fun, i, result)
        else
          {i, result}
        end
      end
      {i, result} = try do
          while_fun.(while_fun, i, result)
        catch
          {:break, {i, result}} -> {i, result}
        end

      throw {:return, result}
    catch
      {:return, val} -> val
    end
  end
  def ln(x) do
    try do
      if x <= 0.0 do
        raise("ln domain error")
      end
      y = (x - 1.0) / (x + 1.0)
      y2 = y * y
      term = y
      sum = 0.0
      k = 0
      while_fun_2 = fn while_fun_2, k, sum, term ->
        if k < 10 do
          denom = to_float(2 * k + 1)
          sum = sum + term / denom
          term = term * y2
          k = k + 1
          while_fun_2.(while_fun_2, k, sum, term)
        else
          {k, sum, term}
        end
      end
      {k, sum, term} = try do
          while_fun_2.(while_fun_2, k, sum, term)
        catch
          {:break, {k, sum, term}} -> {k, sum, term}
        end

      throw {:return, 2.0 * sum}
    catch
      {:return, val} -> val
    end
  end
  def exp(x) do
    try do
      term = 1.0
      sum = 1.0
      n = 1
      while_fun_3 = fn while_fun_3, n, sum, term ->
        if n < 20 do
          term = term * x / to_float(n)
          sum = sum + term
          n = n + 1
          while_fun_3.(while_fun_3, n, sum, term)
        else
          {n, sum, term}
        end
      end
      {n, sum, term} = try do
          while_fun_3.(while_fun_3, n, sum, term)
        catch
          {:break, {n, sum, term}} -> {n, sum, term}
        end

      throw {:return, sum}
    catch
      {:return, val} -> val
    end
  end
  def mean(v) do
    try do
      total = 0.0
      i = 0
      while_fun_4 = fn while_fun_4, i, total ->
        if i < _len(v) do
          total = total + Enum.at(v, i)
          i = i + 1
          while_fun_4.(while_fun_4, i, total)
        else
          {i, total}
        end
      end
      {i, total} = try do
          while_fun_4.(while_fun_4, i, total)
        catch
          {:break, {i, total}} -> {i, total}
        end

      throw {:return, total / to_float(_len(v))}
    catch
      {:return, val} -> val
    end
  end
  def binary_cross_entropy(y_true, y_pred, epsilon) do
    try do
      if _len(y_true) != _len(y_pred) do
        raise("Input arrays must have the same length.")
      end
      losses = []
      i = 0
      while_fun_5 = fn while_fun_5, i, losses ->
        if i < _len(y_true) do
          yt = Enum.at(y_true, i)
          yp = clip(Enum.at(y_pred, i), epsilon, 1.0 - epsilon)
          loss = -(yt * ln(yp) + (1.0 - yt) * ln(1.0 - yp))
          losses = (losses ++ [loss])
          i = i + 1
          while_fun_5.(while_fun_5, i, losses)
        else
          {i, losses}
        end
      end
      {i, losses} = try do
          while_fun_5.(while_fun_5, i, losses)
        catch
          {:break, {i, losses}} -> {i, losses}
        end

      throw {:return, mean(losses)}
    catch
      {:return, val} -> val
    end
  end
  def binary_focal_cross_entropy(y_true, y_pred, gamma, alpha, epsilon) do
    try do
      if _len(y_true) != _len(y_pred) do
        raise("Input arrays must have the same length.")
      end
      losses = []
      i = 0
      while_fun_6 = fn while_fun_6, i, losses ->
        if i < _len(y_true) do
          yt = Enum.at(y_true, i)
          yp = clip(Enum.at(y_pred, i), epsilon, 1.0 - epsilon)
          term1 = alpha * powf(1.0 - yp, gamma) * yt * ln(yp)
          term2 = (1.0 - alpha) * powf(yp, gamma) * (1.0 - yt) * ln(1.0 - yp)
          losses = (losses ++ [-(term1 + term2)])
          i = i + 1
          while_fun_6.(while_fun_6, i, losses)
        else
          {i, losses}
        end
      end
      {i, losses} = try do
          while_fun_6.(while_fun_6, i, losses)
        catch
          {:break, {i, losses}} -> {i, losses}
        end

      throw {:return, mean(losses)}
    catch
      {:return, val} -> val
    end
  end
  def categorical_cross_entropy(y_true, y_pred, epsilon) do
    try do
      if _len(y_true) != _len(y_pred) do
        raise("Input arrays must have the same shape.")
      end
      rows = _len(y_true)
      total = 0.0
      i = 0
      while_fun_7 = fn while_fun_7, i, total ->
        if i < rows do
          if _len(Enum.at(y_true, i)) != _len(Enum.at(y_pred, i)) do
            raise("Input arrays must have the same shape.")
          end
          sum_true = 0.0
          sum_pred = 0.0
          j = 0
          while_fun_8 = fn while_fun_8, j, sum_pred, sum_true ->
            if j < _len(Enum.at(y_true, i)) do
              yt = Enum.at(Enum.at(y_true, i), j)
              yp = Enum.at(Enum.at(y_pred, i), j)
              if (yt != 0.0 && yt != 1.0) do
                raise("y_true must be one-hot encoded.")
              end
              sum_true = sum_true + yt
              sum_pred = sum_pred + yp
              j = j + 1
              while_fun_8.(while_fun_8, j, sum_pred, sum_true)
            else
              {j, sum_pred, sum_true}
            end
          end
          {j, sum_pred, sum_true} = try do
              while_fun_8.(while_fun_8, j, sum_pred, sum_true)
            catch
              {:break, {j, sum_pred, sum_true}} -> {j, sum_pred, sum_true}
            end

          if sum_true != 1.0 do
            raise("y_true must be one-hot encoded.")
          end
          if absf(sum_pred - 1.0) > epsilon do
            raise("Predicted probabilities must sum to approximately 1.")
          end
          j = 0
          while_fun_9 = fn while_fun_9, j, total ->
            if j < _len(Enum.at(y_true, i)) do
              yp = clip(Enum.at(Enum.at(y_pred, i), j), epsilon, 1.0)
              total = total - (Enum.at(Enum.at(y_true, i), j) * ln(yp))
              j = j + 1
              while_fun_9.(while_fun_9, j, total)
            else
              {j, total}
            end
          end
          {j, total} = try do
              while_fun_9.(while_fun_9, j, total)
            catch
              {:break, {j, total}} -> {j, total}
            end

          i = i + 1
          while_fun_7.(while_fun_7, i, total)
        else
          {i, total}
        end
      end
      {i, total} = try do
          while_fun_7.(while_fun_7, i, total)
        catch
          {:break, {i, total}} -> {i, total}
        end

      throw {:return, total}
    catch
      {:return, val} -> val
    end
  end
  def categorical_focal_cross_entropy(y_true, y_pred, alpha, gamma, epsilon) do
    try do
      if _len(y_true) != _len(y_pred) do
        raise("Shape of y_true and y_pred must be the same.")
      end
      rows = _len(y_true)
      cols = _len(Enum.at(y_true, 0))
      a = alpha
      {a} = if _len(a) == 0 do
        tmp = []
        j = 0
        while_fun_10 = fn while_fun_10, j, tmp ->
          if j < cols do
            tmp = (tmp ++ [1.0])
            j = j + 1
            while_fun_10.(while_fun_10, j, tmp)
          else
            {j, tmp}
          end
        end
        {j, tmp} = try do
            while_fun_10.(while_fun_10, j, tmp)
          catch
            {:break, {j, tmp}} -> {j, tmp}
          end

        a = tmp
        {a}
      else
        {a}
      end
      if _len(a) != cols do
        raise("Length of alpha must match the number of classes.")
      end
      total = 0.0
      i = 0
      while_fun_11 = fn while_fun_11, i, total ->
        if i < rows do
          if _len(Enum.at(y_true, i)) != cols || _len(Enum.at(y_pred, i)) != cols do
            raise("Shape of y_true and y_pred must be the same.")
          end
          sum_true = 0.0
          sum_pred = 0.0
          j = 0
          while_fun_12 = fn while_fun_12, j, sum_pred, sum_true ->
            if j < cols do
              yt = Enum.at(Enum.at(y_true, i), j)
              yp = Enum.at(Enum.at(y_pred, i), j)
              if (yt != 0.0 && yt != 1.0) do
                raise("y_true must be one-hot encoded.")
              end
              sum_true = sum_true + yt
              sum_pred = sum_pred + yp
              j = j + 1
              while_fun_12.(while_fun_12, j, sum_pred, sum_true)
            else
              {j, sum_pred, sum_true}
            end
          end
          {j, sum_pred, sum_true} = try do
              while_fun_12.(while_fun_12, j, sum_pred, sum_true)
            catch
              {:break, {j, sum_pred, sum_true}} -> {j, sum_pred, sum_true}
            end

          if sum_true != 1.0 do
            raise("y_true must be one-hot encoded.")
          end
          if absf(sum_pred - 1.0) > epsilon do
            raise("Predicted probabilities must sum to approximately 1.")
          end
          row_loss = 0.0
          j = 0
          while_fun_13 = fn while_fun_13, j, row_loss ->
            if j < cols do
              yp = clip(Enum.at(Enum.at(y_pred, i), j), epsilon, 1.0)
              row_loss = row_loss + Enum.at(a, j) * powf(1.0 - yp, gamma) * Enum.at(Enum.at(y_true, i), j) * ln(yp)
              j = j + 1
              while_fun_13.(while_fun_13, j, row_loss)
            else
              {j, row_loss}
            end
          end
          {j, row_loss} = try do
              while_fun_13.(while_fun_13, j, row_loss)
            catch
              {:break, {j, row_loss}} -> {j, row_loss}
            end

          total = total - row_loss
          i = i + 1
          while_fun_11.(while_fun_11, i, total)
        else
          {i, total}
        end
      end
      {i, total} = try do
          while_fun_11.(while_fun_11, i, total)
        catch
          {:break, {i, total}} -> {i, total}
        end

      throw {:return, total / to_float(rows)}
    catch
      {:return, val} -> val
    end
  end
  def hinge_loss(y_true, y_pred) do
    try do
      if _len(y_true) != _len(y_pred) do
        raise("Length of predicted and actual array must be same.")
      end
      losses = []
      i = 0
      while_fun_14 = fn while_fun_14, i, losses ->
        if i < _len(y_true) do
          yt = Enum.at(y_true, i)
          if (yt != (-1.0) && yt != 1.0) do
            raise("y_true can have values -1 or 1 only.")
          end
          pred = Enum.at(y_pred, i)
          l = maxf(0.0, 1.0 - yt * pred)
          losses = (losses ++ [l])
          i = i + 1
          while_fun_14.(while_fun_14, i, losses)
        else
          {i, losses}
        end
      end
      {i, losses} = try do
          while_fun_14.(while_fun_14, i, losses)
        catch
          {:break, {i, losses}} -> {i, losses}
        end

      throw {:return, mean(losses)}
    catch
      {:return, val} -> val
    end
  end
  def huber_loss(y_true, y_pred, delta) do
    try do
      if _len(y_true) != _len(y_pred) do
        raise("Input arrays must have the same length.")
      end
      total = 0.0
      i = 0
      while_fun_15 = fn while_fun_15, i, total ->
        if i < _len(y_true) do
          diff = Enum.at(y_true, i) - Enum.at(y_pred, i)
          adiff = absf(diff)
          total = (if adiff <= delta, do: total + 0.5 * diff * diff, else: total + delta * (adiff - 0.5 * delta))
          i = i + 1
          while_fun_15.(while_fun_15, i, total)
        else
          {i, total}
        end
      end
      {i, total} = try do
          while_fun_15.(while_fun_15, i, total)
        catch
          {:break, {i, total}} -> {i, total}
        end

      throw {:return, total / to_float(_len(y_true))}
    catch
      {:return, val} -> val
    end
  end
  def mean_squared_error(y_true, y_pred) do
    try do
      if _len(y_true) != _len(y_pred) do
        raise("Input arrays must have the same length.")
      end
      losses = []
      i = 0
      while_fun_16 = fn while_fun_16, i, losses ->
        if i < _len(y_true) do
          diff = Enum.at(y_true, i) - Enum.at(y_pred, i)
          losses = (losses ++ [diff * diff])
          i = i + 1
          while_fun_16.(while_fun_16, i, losses)
        else
          {i, losses}
        end
      end
      {i, losses} = try do
          while_fun_16.(while_fun_16, i, losses)
        catch
          {:break, {i, losses}} -> {i, losses}
        end

      throw {:return, mean(losses)}
    catch
      {:return, val} -> val
    end
  end
  def mean_absolute_error(y_true, y_pred) do
    try do
      if _len(y_true) != _len(y_pred) do
        raise("Input arrays must have the same length.")
      end
      total = 0.0
      i = 0
      while_fun_17 = fn while_fun_17, i, total ->
        if i < _len(y_true) do
          total = total + absf(Enum.at(y_true, i) - Enum.at(y_pred, i))
          i = i + 1
          while_fun_17.(while_fun_17, i, total)
        else
          {i, total}
        end
      end
      {i, total} = try do
          while_fun_17.(while_fun_17, i, total)
        catch
          {:break, {i, total}} -> {i, total}
        end

      throw {:return, total / to_float(_len(y_true))}
    catch
      {:return, val} -> val
    end
  end
  def mean_squared_logarithmic_error(y_true, y_pred) do
    try do
      if _len(y_true) != _len(y_pred) do
        raise("Input arrays must have the same length.")
      end
      total = 0.0
      i = 0
      while_fun_18 = fn while_fun_18, i, total ->
        if i < _len(y_true) do
          a = ln(1.0 + Enum.at(y_true, i))
          b = ln(1.0 + Enum.at(y_pred, i))
          diff = a - b
          total = total + diff * diff
          i = i + 1
          while_fun_18.(while_fun_18, i, total)
        else
          {i, total}
        end
      end
      {i, total} = try do
          while_fun_18.(while_fun_18, i, total)
        catch
          {:break, {i, total}} -> {i, total}
        end

      throw {:return, total / to_float(_len(y_true))}
    catch
      {:return, val} -> val
    end
  end
  def mean_absolute_percentage_error(y_true, y_pred, epsilon) do
    try do
      if _len(y_true) != _len(y_pred) do
        raise("The length of the two arrays should be the same.")
      end
      total = 0.0
      i = 0
      while_fun_19 = fn while_fun_19, i, total ->
        if i < _len(y_true) do
          yt = Enum.at(y_true, i)
          {yt} = if yt == 0.0 do
            yt = epsilon
            {yt}
          else
            {yt}
          end
          total = total + absf((yt - Enum.at(y_pred, i)) / yt)
          i = i + 1
          while_fun_19.(while_fun_19, i, total)
        else
          {i, total}
        end
      end
      {i, total} = try do
          while_fun_19.(while_fun_19, i, total)
        catch
          {:break, {i, total}} -> {i, total}
        end

      throw {:return, total / to_float(_len(y_true))}
    catch
      {:return, val} -> val
    end
  end
  def perplexity_loss(y_true, y_pred, epsilon) do
    try do
      batch = _len(y_true)
      if batch != _len(y_pred) do
        raise("Batch size of y_true and y_pred must be equal.")
      end
      sentence_len = _len(Enum.at(y_true, 0))
      if sentence_len != _len(Enum.at(y_pred, 0)) do
        raise("Sentence length of y_true and y_pred must be equal.")
      end
      vocab_size = _len(Enum.at(Enum.at(y_pred, 0), 0))
      b = 0
      total_perp = 0.0
      while_fun_20 = fn while_fun_20, b, total_perp ->
        if b < batch do
          if _len(Enum.at(y_true, b)) != sentence_len || _len(Enum.at(y_pred, b)) != sentence_len do
            raise("Sentence length of y_true and y_pred must be equal.")
          end
          sum_log = 0.0
          j = 0
          while_fun_21 = fn while_fun_21, j, sum_log ->
            if j < sentence_len do
              label = Enum.at(Enum.at(y_true, b), j)
              if label >= vocab_size do
                raise("Label value must not be greater than vocabulary size.")
              end
              prob = clip(Enum.at(Enum.at(Enum.at(y_pred, b), j), label), epsilon, 1.0)
              sum_log = sum_log + ln(prob)
              j = j + 1
              while_fun_21.(while_fun_21, j, sum_log)
            else
              {j, sum_log}
            end
          end
          {j, sum_log} = try do
              while_fun_21.(while_fun_21, j, sum_log)
            catch
              {:break, {j, sum_log}} -> {j, sum_log}
            end

          mean_log = sum_log / to_float(sentence_len)
          perp = exp(-mean_log)
          total_perp = total_perp + perp
          b = b + 1
          while_fun_20.(while_fun_20, b, total_perp)
        else
          {b, total_perp}
        end
      end
      {b, total_perp} = try do
          while_fun_20.(while_fun_20, b, total_perp)
        catch
          {:break, {b, total_perp}} -> {b, total_perp}
        end

      throw {:return, total_perp / to_float(batch)}
    catch
      {:return, val} -> val
    end
  end
  def smooth_l1_loss(y_true, y_pred, beta) do
    try do
      if _len(y_true) != _len(y_pred) do
        raise("The length of the two arrays should be the same.")
      end
      total = 0.0
      i = 0
      while_fun_22 = fn while_fun_22, i, total ->
        if i < _len(y_true) do
          diff = absf(Enum.at(y_true, i) - Enum.at(y_pred, i))
          total = (if diff < beta, do: total + 0.5 * diff * diff / beta, else: total + diff - 0.5 * beta)
          i = i + 1
          while_fun_22.(while_fun_22, i, total)
        else
          {i, total}
        end
      end
      {i, total} = try do
          while_fun_22.(while_fun_22, i, total)
        catch
          {:break, {i, total}} -> {i, total}
        end

      throw {:return, total / to_float(_len(y_true))}
    catch
      {:return, val} -> val
    end
  end
  def kullback_leibler_divergence(y_true, y_pred) do
    try do
      if _len(y_true) != _len(y_pred) do
        raise("Input arrays must have the same length.")
      end
      total = 0.0
      i = 0
      while_fun_23 = fn while_fun_23, i, total ->
        if i < _len(y_true) do
          total = total + Enum.at(y_true, i) * ln(Enum.at(y_true, i) / Enum.at(y_pred, i))
          i = i + 1
          while_fun_23.(while_fun_23, i, total)
        else
          {i, total}
        end
      end
      {i, total} = try do
          while_fun_23.(while_fun_23, i, total)
        catch
          {:break, {i, total}} -> {i, total}
        end

      throw {:return, total}
    catch
      {:return, val} -> val
    end
  end
  def main() do
    try do
      y_true_bc = [0.0, 1.0, 1.0, 0.0, 1.0]
      y_pred_bc = [0.2, 0.7, 0.9, 0.3, 0.8]
      IO.puts(Kernel.inspect(binary_cross_entropy(y_true_bc, y_pred_bc, 0.000000000000001)))
      IO.puts(Kernel.inspect(binary_focal_cross_entropy(y_true_bc, y_pred_bc, 2.0, 0.25, 0.000000000000001)))
      y_true_cce = [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]
      y_pred_cce = [[0.9, 0.1, 0.0], [0.2, 0.7, 0.1], [0.0, 0.1, 0.9]]
      IO.puts(Kernel.inspect(categorical_cross_entropy(y_true_cce, y_pred_cce, 0.000000000000001)))
      alpha = [0.6, 0.2, 0.7]
      IO.puts(Kernel.inspect(categorical_focal_cross_entropy(y_true_cce, y_pred_cce, alpha, 2.0, 0.000000000000001)))
      y_true_hinge = [-1.0, 1.0, 1.0, -1.0, 1.0]
      y_pred_hinge = [-4.0, -0.3, 0.7, 5.0, 10.0]
      IO.puts(Kernel.inspect(hinge_loss(y_true_hinge, y_pred_hinge)))
      y_true_huber = [0.9, 10.0, 2.0, 1.0, 5.2]
      y_pred_huber = [0.8, 2.1, 2.9, 4.2, 5.2]
      IO.puts(Kernel.inspect(huber_loss(y_true_huber, y_pred_huber, 1.0)))
      IO.puts(Kernel.inspect(mean_squared_error(y_true_huber, y_pred_huber)))
      IO.puts(Kernel.inspect(mean_absolute_error(y_true_huber, y_pred_huber)))
      IO.puts(Kernel.inspect(mean_squared_logarithmic_error(y_true_huber, y_pred_huber)))
      y_true_mape = [10.0, 20.0, 30.0, 40.0]
      y_pred_mape = [12.0, 18.0, 33.0, 45.0]
      IO.puts(Kernel.inspect(mean_absolute_percentage_error(y_true_mape, y_pred_mape, 0.000000000000001)))
      y_true_perp = [[1, 4], [2, 3]]
      y_pred_perp = [[[0.28, 0.19, 0.21, 0.15, 0.17], [0.24, 0.19, 0.09, 0.18, 0.3]], [[0.03, 0.26, 0.21, 0.18, 0.32], [0.28, 0.1, 0.33, 0.15, 0.14]]]
      IO.puts(Kernel.inspect(perplexity_loss(y_true_perp, y_pred_perp, 0.0000001)))
      y_true_smooth = [3.0, 5.0, 2.0, 7.0]
      y_pred_smooth = [2.9, 4.8, 2.1, 7.2]
      IO.puts(Kernel.inspect(smooth_l1_loss(y_true_smooth, y_pred_smooth, 1.0)))
      y_true_kl = [0.2, 0.3, 0.5]
      y_pred_kl = [0.3, 0.3, 0.4]
      IO.puts(Kernel.inspect(kullback_leibler_divergence(y_true_kl, y_pred_kl)))
    catch
      {:return, val} -> val
    end
  end
  def bench_main() do
    :erlang.garbage_collect()
    mem_start = _mem()
    t_start = _bench_now()
    main()
    mem_end = _mem()
    duration_us = max(_bench_now() - t_start, 1)
    :erlang.garbage_collect()
    mem_diff = abs(mem_end - mem_start)
    IO.puts("{\n  \"duration_us\": #{duration_us},\n  \"memory_bytes\": #{mem_diff},\n  \"name\": \"main\"\n}")
  end
end
Main.bench_main()
