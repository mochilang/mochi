/*
Back Propagation Neural Network (BPNN)

This program implements a simple fully connected neural network trained
with the back propagation algorithm.  Each layer contains a weight matrix
and bias vector.  During the forward pass, data flows through the layers
as z = W x - b followed by a sigmoid activation.  The backward pass
computes gradients of the mean squared error loss and updates parameters
using gradient descent.

The implementation mirrors the reference Python version from
TheAlgorithms/Python using pure Mochi code without external libraries.
Matrices and vectors are represented with lists and all operations such
as matrix multiplication, outer product and element-wise functions are
implemented explicitly.  Random numbers are generated with a simple
linear congruential generator so results are deterministic.  Complexity is
O(rounds * samples * parameters).
*/

var seed = 1

fun rand(): int {
  seed = (seed * 1103515245 + 12345) % 2147483648
  return seed
}

fun random(): float {
  return (1.0 * rand()) / 2147483648.0
}

fun expApprox(x: float): float {
  var y: float = x
  var is_neg: bool = false
  if x < 0.0 {
    is_neg = true
    y = -x
  }
  var term: float = 1.0
  var sum: float = 1.0
  var n: int = 1
  while n < 30 {
    term = term * y / (n as float)
    sum = sum + term
    n = n + 1
  }
  if is_neg { return 1.0 / sum }
  return sum
}

fun sigmoid(z: float): float {
  return 1.0 / (1.0 + expApprox(-z))
}

fun sigmoid_vec(v: list<float>): list<float> {
  var res: list<float> = []
  var i = 0
  while i < len(v) {
    res = append(res, sigmoid(v[i]))
    i = i + 1
  }
  return res
}

fun sigmoid_derivative(out: list<float>): list<float> {
  var res: list<float> = []
  var i = 0
  while i < len(out) {
    let val = out[i]
    res = append(res, val * (1.0 - val))
    i = i + 1
  }
  return res
}

type Layer {
  units: int,
  weight: list<list<float>>,
  bias: list<float>,
  output: list<float>,
  xdata: list<float>,
  learn_rate: float
}

fun random_vector(n: int): list<float> {
  var v: list<float> = []
  var i = 0
  while i < n {
    v = append(v, random() - 0.5)
    i = i + 1
  }
  return v
}

fun random_matrix(r: int, c: int): list<list<float>> {
  var m: list<list<float>> = []
  var i = 0
  while i < r {
    m = append(m, random_vector(c))
    i = i + 1
  }
  return m
}

fun matvec(mat: list<list<float>>, vec: list<float>): list<float> {
  var res: list<float> = []
  var i = 0
  while i < len(mat) {
    var s = 0.0
    var j = 0
    while j < len(vec) {
      s = s + mat[i][j] * vec[j]
      j = j + 1
    }
    res = append(res, s)
    i = i + 1
  }
  return res
}

fun matTvec(mat: list<list<float>>, vec: list<float>): list<float> {
  var cols = len(mat[0])
  var res: list<float> = []
  var j = 0
  while j < cols {
    var s = 0.0
    var i = 0
    while i < len(mat) {
      s = s + mat[i][j] * vec[i]
      i = i + 1
    }
    res = append(res, s)
    j = j + 1
  }
  return res
}

fun vec_sub(a: list<float>, b: list<float>): list<float> {
  var res: list<float> = []
  var i = 0
  while i < len(a) {
    res = append(res, a[i] - b[i])
    i = i + 1
  }
  return res
}

fun vec_mul(a: list<float>, b: list<float>): list<float> {
  var res: list<float> = []
  var i = 0
  while i < len(a) {
    res = append(res, a[i] * b[i])
    i = i + 1
  }
  return res
}

fun vec_scalar_mul(v: list<float>, s: float): list<float> {
  var res: list<float> = []
  var i = 0
  while i < len(v) {
    res = append(res, v[i] * s)
    i = i + 1
  }
  return res
}

fun outer(a: list<float>, b: list<float>): list<list<float>> {
  var res: list<list<float>> = []
  var i = 0
  while i < len(a) {
    var row: list<float> = []
    var j = 0
    while j < len(b) {
      row = append(row, a[i] * b[j])
      j = j + 1
    }
    res = append(res, row)
    i = i + 1
  }
  return res
}

fun mat_scalar_mul(mat: list<list<float>>, s: float): list<list<float>> {
  var res: list<list<float>> = []
  var i = 0
  while i < len(mat) {
    var row: list<float> = []
    var j = 0
    while j < len(mat[i]) {
      row = append(row, mat[i][j] * s)
      j = j + 1
    }
    res = append(res, row)
    i = i + 1
  }
  return res
}

fun mat_sub(a: list<list<float>>, b: list<list<float>>): list<list<float>> {
  var res: list<list<float>> = []
  var i = 0
  while i < len(a) {
    var row: list<float> = []
    var j = 0
    while j < len(a[i]) {
      row = append(row, a[i][j] - b[i][j])
      j = j + 1
    }
    res = append(res, row)
    i = i + 1
  }
  return res
}

fun init_layer(units: int, back_units: int, lr: float): Layer {
  return Layer {
    units: units,
    weight: random_matrix(units, back_units),
    bias: random_vector(units),
    output: [],
    xdata: [],
    learn_rate: lr
  }
}

fun forward(layers: list<Layer>, x: list<float>): list<Layer> {
  var data = x
  var i = 0
  while i < len(layers) {
    var layer = layers[i]
    layer.xdata = data
    if i == 0 {
      layer.output = data
    } else {
      let z = vec_sub(matvec(layer.weight, data), layer.bias)
      layer.output = sigmoid_vec(z)
      data = layer.output
    }
    layers[i] = layer
    i = i + 1
  }
  return layers
}

fun backward(layers: list<Layer>, grad: list<float>): list<Layer> {
  var g = grad
  var i = len(layers) - 1
  while i > 0 {
    var layer = layers[i]
    let deriv = sigmoid_derivative(layer.output)
    let delta = vec_mul(g, deriv)
    let grad_w = outer(delta, layer.xdata)
    layer.weight = mat_sub(layer.weight, mat_scalar_mul(grad_w, layer.learn_rate))
    layer.bias = vec_sub(layer.bias, vec_scalar_mul(delta, layer.learn_rate))
    g = matTvec(layer.weight, delta)
    layers[i] = layer
    i = i - 1
  }
  return layers
}

fun calc_loss(y: list<float>, yhat: list<float>): float {
  var s = 0.0
  var i = 0
  while i < len(y) {
    let d = y[i] - yhat[i]
    s = s + d * d
    i = i + 1
  }
  return s
}

fun calc_gradient(y: list<float>, yhat: list<float>): list<float> {
  var g: list<float> = []
  var i = 0
  while i < len(y) {
    g = append(g, 2.0 * (yhat[i] - y[i]))
    i = i + 1
  }
  return g
}

fun train(layers: list<Layer>, xdata: list<list<float>>, ydata: list<list<float>>, rounds: int, acc: float): float {
  var r = 0
  while r < rounds {
    var i = 0
    while i < len(xdata) {
      layers = forward(layers, xdata[i])
      let out = layers[len(layers)-1].output
      let grad = calc_gradient(ydata[i], out)
      layers = backward(layers, grad)
      i = i + 1
    }
    r = r + 1
  }
  return 0.0
}


type Data {
  x: list<list<float>>,
  y: list<list<float>>
}

fun create_data(): Data {
  var x: list<list<float>> = []
  var i = 0
  while i < 10 {
    x = append(x, random_vector(10))
    i = i + 1
  }
  let y: list<list<float>> = [
    [0.8, 0.4],
    [0.4, 0.3],
    [0.34, 0.45],
    [0.67, 0.32],
    [0.88, 0.67],
    [0.78, 0.77],
    [0.55, 0.66],
    [0.55, 0.43],
    [0.54, 0.1],
    [0.1, 0.5],
  ]
  return Data { x: x, y: y }
}

fun main() {
  let data = create_data()
  let x = data.x
  let y = data.y
  var layers: list<Layer> = []
  layers = append(layers, init_layer(10, 0, 0.3))
  layers = append(layers, init_layer(20, 10, 0.3))
  layers = append(layers, init_layer(30, 20, 0.3))
  layers = append(layers, init_layer(2, 30, 0.3))
  let final_mse = train(layers, x, y, 100, 0.01)
  print(final_mse)
}

main()
